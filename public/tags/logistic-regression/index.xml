<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Logistic Regression | FLH Website</title>
    <link>http://localhost:1313/tags/logistic-regression/</link>
      <atom:link href="http://localhost:1313/tags/logistic-regression/index.xml" rel="self" type="application/rss+xml" />
    <description>Logistic Regression</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Fri, 27 Oct 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu_7c4bcaa7031a3f50.png</url>
      <title>Logistic Regression</title>
      <link>http://localhost:1313/tags/logistic-regression/</link>
    </image>
    
    <item>
      <title>🖩 Logistic regression with matrices</title>
      <link>http://localhost:1313/post/logreg_matrix/</link>
      <pubDate>Fri, 27 Oct 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/logreg_matrix/</guid>
      <description>


&lt;details class=&#34;print:hidden xl:hidden&#34; open&gt;
  &lt;summary&gt;Table of Contents&lt;/summary&gt;
  &lt;div class=&#34;text-sm&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;&lt;/nav&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;p&gt;Logistic regression is a modeling technique that has attracted a lot of attention, especially from folks interested in classification, machine learning, and prediction using binary outcomes. One of the neat things about using R is that users can revisit commonly used procedures and figure out how they work.&lt;/p&gt;
&lt;p&gt;What follows are some logistic regression notes (this is not on interpreting results). Even though I’ve written about how other &lt;a href=&#34;../applied-example-for-alternatives-to-logistic-regression/&#34;&gt;alternatives&lt;/a&gt; might be simpler than logistic regression or that there are challenges when comparing coefficients across &lt;a href=&#34;../comparing-coefficients-across-logistic-regression-models/&#34;&gt;models&lt;/a&gt;, it is interesting to see how the procedure works.&lt;/p&gt;
&lt;p&gt;I show a few things:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How to use some matrices for getting logistic regression results (in terms of point estimates and standard errors);&lt;/li&gt;
&lt;li&gt;How to compute cluster robust standard errors too;&lt;/li&gt;
&lt;li&gt;How to manually run iteratively weighted least squares to get the same results from scratch.&lt;/li&gt;
&lt;li&gt;I added a section later on to do this using &lt;a href=&#34;../logistic-regression-fisher-scoring/&#34;&gt;Fisher Scoring&lt;/a&gt; as well.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;first-create-a-clustered-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;First, create a (clustered) dataset&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234) #clustered example:: just for sim
pp &amp;lt;- function(x) exp(x) / (1 + exp(x))
NG &amp;lt;- 20 #number of groups
GS &amp;lt;- 15 #group size
total &amp;lt;- NG * GS
tr &amp;lt;- rep(sample(c(0,1), NG, 1), each = GS)
w1 &amp;lt;- rep(rnorm(NG), each = GS)
e2 &amp;lt;- rep(rnorm(NG, 0, .5), each = GS)
school &amp;lt;- rep(1:NG, each = GS)
x1 &amp;lt;- rnorm(total)
x2 &amp;lt;- rbinom(total, 1, .5)
ystar &amp;lt;- 1 + tr * .5 + w1 * .3 + x1 * .6 + x2 * -.4 + e2
y &amp;lt;- rbinom(total, 1, pp(ystar))
dat &amp;lt;- data.frame(y, school, tr, w1, x1, x2)
sel &amp;lt;- sample(total, 100) #randomly select data to remove
dat &amp;lt;- dat[-sel, ] #remove to create unbalanced data&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run a logistic regression model predicting &lt;code&gt;y&lt;/code&gt; (&lt;code&gt;tr&lt;/code&gt; and &lt;code&gt;w1&lt;/code&gt; are actually level-2 variables but we’ll ignore the clustering first).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        y            school            tr              w1          
##  Min.   :0.00   Min.   : 1.00   Min.   :0.000   Min.   :-1.44820  
##  1st Qu.:0.00   1st Qu.: 6.00   1st Qu.:0.000   1st Qu.:-0.91120  
##  Median :1.00   Median :11.00   Median :1.000   Median :-0.49069  
##  Mean   :0.65   Mean   :10.71   Mean   :0.725   Mean   :-0.26632  
##  3rd Qu.:1.00   3rd Qu.:16.00   3rd Qu.:1.000   3rd Qu.: 0.08187  
##  Max.   :1.00   Max.   :20.00   Max.   :1.000   Max.   : 2.41584  
##        x1                 x2     
##  Min.   :-2.85576   Min.   :0.0  
##  1st Qu.:-0.63885   1st Qu.:0.0  
##  Median :-0.09820   Median :0.5  
##  Mean   :-0.02651   Mean   :0.5  
##  3rd Qu.: 0.51302   3rd Qu.:1.0  
##  Max.   : 2.91914   Max.   :1.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 200&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1 &amp;lt;- glm(y ~ tr + w1 + x1 + x2,
          data = dat, 
          family = binomial)
summary(m1)$coef #results are in logits&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                Estimate Std. Error     z value    Pr(&amp;gt;|z|)
## (Intercept)  0.23069482  0.3482386  0.66246188 0.507675259
## tr           0.93184838  0.3465455  2.68896434 0.007167408
## w1           0.60952221  0.2296481  2.65415706 0.007950681
## x1           0.57411101  0.1852548  3.09903484 0.001941522
## x2          -0.01320503  0.3218338 -0.04103058 0.967271514&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;extract-matrices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extract matrices&lt;/h2&gt;
&lt;p&gt;The standard formula for the coefficients in linear regression is &lt;span class=&#34;math inline&#34;&gt;\((X&amp;#39;X)^{-1}(X&amp;#39;y)\)&lt;/span&gt;. However, in a generalized linear model, a difference is that &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is now &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; (see computation) and weights are estimated as well. The GzLM formula for the betas is now: &lt;span class=&#34;math inline&#34;&gt;\((X&amp;#39;WX)^{-1}(X&amp;#39;Wz)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The explanation of the matrices is shown at the latter part of this post (when computing this manually) which describes what goes into the computation. We’ll use the &lt;code&gt;m1&lt;/code&gt; object just created and extract elements from it that can be used to cobble together the same results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- model.matrix(m1) #the X matrix
y &amp;lt;- m1$y #the outcome
eta &amp;lt;- X %*% coef(m1) #predicted values
mu &amp;lt;- as.vector(1 / (1 + exp(-eta))) #transformed predicted values
z &amp;lt;- eta + (y - mu) * (1 / (mu * (1 - mu)))
Ws &amp;lt;- diag(weights(m1, &amp;#39;working&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After the matrices are created, can use the formulas to get the same results (compare below):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;### point estimates (similar)
solve(t(X) %*% Ws %*% X) %*% 
  t(X) %*% Ws %*% z&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    [,1]
## (Intercept)  0.23069529
## tr           0.93184794
## w1           0.60952156
## x1           0.57411052
## x2          -0.01320591&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(m1) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)          tr          w1          x1          x2 
##  0.23069482  0.93184838  0.60952221  0.57411101 -0.01320503&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For standard errors, this is just the square root of the diagonal of &lt;span class=&#34;math inline&#34;&gt;\((X&amp;#39;WX)^{-1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(diag(solve(t(X) %*% Ws %*% X)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)          tr          w1          x1          x2 
##   0.3482386   0.3465455   0.2296481   0.1852548   0.3218338&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(diag(vcov(m1))) #same&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)          tr          w1          x1          x2 
##   0.3482386   0.3465455   0.2296481   0.1852548   0.3218338&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;we-can-also-compute-cluster-robust-standard-errors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;We can also compute cluster robust standard errors&lt;/h2&gt;
&lt;p&gt;I’ve written about &lt;a href=&#34;https://francish.netlify.app/post/note-on-robust-standard-errors/&#34;&gt;robust SEs&lt;/a&gt; in another post but in this case, the residuals are different as seen in the syntax. I saw in this &lt;a href=&#34;https://stats.stackexchange.com/questions/283801/how-are-robust-standard-errors-calculated-in-the-case-of-logistic-regression&#34;&gt;post&lt;/a&gt; that you can extract the weighted residuals shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- model.matrix(m1) * sqrt(weights(m1, &amp;quot;working&amp;quot;))
u &amp;lt;- residuals(m1, &amp;quot;working&amp;quot;)  * sqrt(weights(m1, &amp;quot;working&amp;quot;)) #residuals
&lt;p&gt;cdata &amp;lt;- data.frame(cluster = dat$school, r = u)
(m &amp;lt;- length(table(cdata$cluster))) #number of clusters&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] 20&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- nobs(m1) #number of observations
k &amp;lt;- m1$rank
dfa &amp;lt;- (m/(m-1)) * ((n-1)/(n-k)) #degrees of freedom adjustment
gs &amp;lt;- names(table(cdata$cluster)) #cluster names
u &amp;lt;- matrix(NA, nrow = m, ncol = k) #clusters x rank:: creating a new u matrix

for(i in 1:m){
  u[i,] &amp;lt;- t(cdata$r[cdata$cluster == gs[i]]) %*% X[dat$school == gs[i], 1:k]
} 

br &amp;lt;- solve(t(X) %*% X) #the bread
mt &amp;lt;- t(u) %*% u #the meat
(clvc &amp;lt;- (br %*% mt %*% br * dfa)) #cluster robust VCOV manually computed&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              (Intercept)           tr           w1           x1           x2
## (Intercept)  0.161369126 -0.137857215  0.017878858 -0.001318135 -0.054932212
## tr          -0.137857215  0.184883609 -0.004615105  0.001738463 -0.004407959
## w1           0.017878858 -0.004615105  0.053945127  0.021829090 -0.012319716
## x1          -0.001318135  0.001738463  0.021829090  0.051276709  0.005750240
## x2          -0.054932212 -0.004407959 -0.012319716  0.005750240  0.129468425&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(sandwich) #to &amp;#39;automatically&amp;#39; compute
vcovCL(m1, cluster = dat$school, type = &amp;#39;HC1&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              (Intercept)           tr           w1           x1           x2
## (Intercept)  0.161369126 -0.137857215  0.017878858 -0.001318135 -0.054932212
## tr          -0.137857215  0.184883609 -0.004615105  0.001738463 -0.004407959
## w1           0.017878858 -0.004615105  0.053945127  0.021829090 -0.012319716
## x1          -0.001318135  0.001738463  0.021829090  0.051276709  0.005750240
## x2          -0.054932212 -0.004407959 -0.012319716  0.005750240  0.129468425&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all.equal(clvc, vcovCL(m1, cluster = dat$school, type = &amp;#39;HC1&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;manually-performing-a-logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Manually performing a logistic regression&lt;/h2&gt;
&lt;p&gt;A GLM requires several quantities (in this case, shown are the ones for a binomial family using a logit link– this will differ based on both the link and the family)&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the link function: &lt;span class=&#34;math inline&#34;&gt;\(g(\mu)=log(\frac{\mu}{1 - \mu})\)&lt;/span&gt; known as the log odds&lt;/li&gt;
&lt;li&gt;the inverse link: &lt;span class=&#34;math inline&#34;&gt;\(g^{-1}(\mu) =\frac{1}{1 + exp(-\eta)}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the variance function: &lt;span class=&#34;math inline&#34;&gt;\(V(\mu) = \mu(1 - \mu)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the first derivative: &lt;span class=&#34;math inline&#34;&gt;\(g&amp;#39;(\mu) = \frac{1}{\mu(1 - \mu)}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The generic GLM algorithm uses the following computations (substitute the values from above):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(diag[\frac{1}{V(\mu)g&amp;#39;(\mu)^2}]\)&lt;/span&gt; : weights&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(z = \eta + (y - \mu)g&amp;#39;(\mu)\)&lt;/span&gt; : working response&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\eta = X\beta\)&lt;/span&gt; : linear prediction&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu = g^{-1}(\eta)\)&lt;/span&gt; : the fitted values&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- model.matrix(~tr + w1 + x1 + x2, data = dat) #just to quickly get this
#this is not the same as the weighted one
tol &amp;lt;- 1e-6 #tolerance: if change is low, then stop
y &amp;lt;- dat$y
mu &amp;lt;- (y + mean(y))/2 #for convergence
#eta &amp;lt;- log(mu) ## Poisson link
eta &amp;lt;- log(mu / (1 - mu)) #logit
&lt;p&gt;dev &amp;lt;- 0 #deviance
delta.dev &amp;lt;- 1
i &amp;lt;- 1 #iteration number&lt;/p&gt;
&lt;p&gt;while(i &amp;lt; 50 &amp;amp; abs(delta.dev) &amp;gt; tol) { #repeat until deviance change is minimal
tmp &amp;lt;- mu * (1 - mu) #since I use this over and over again
W &amp;lt;- diag(1 / (tmp * (1/tmp)^2))
z &amp;lt;- eta + (y - mu) * (1 / tmp)
b &amp;lt;- solve(t(X) %&lt;em&gt;% W %&lt;/em&gt;% X) %&lt;em&gt;% t(X) %&lt;/em&gt;% W %&lt;em&gt;% z #these are the cofficients
eta &amp;lt;- X %&lt;/em&gt;% b #update eta
mu &amp;lt;- as.vector( 1 / (1 + exp(-eta)))
dev0 &amp;lt;- dev
#dev &amp;lt;- -2 * sum(y * log(y/mu) + (1 - y) * (1 -log(y/mu)), na.rm = T)
dev &amp;lt;- -2 * sum((y * log(mu)) + ((1 - y) * log(1 - mu))) #deviance
delta.dev &amp;lt;- dev - dev0 #assess change in deviance for logistic reg
cat(i, dev, &amp;quot;::&amp;quot;)
i &amp;lt;- i + 1 #increment by 1
}&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 1 234.3943 ::2 233.4394 ::3 233.4389 ::4 233.4389 ::&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The solution was reached after five iterations. Note: the deviance (which is -2 log likelihood) is the same using our manual method and the built-in function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;deviance(m1) #using the results from glm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 233.4389&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dev #manually computed&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 233.4389&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logLik(m1) * -2 #showing this is -2 x log likelihood&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;log Lik.&amp;#39; 233.4389 (df=5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To compute the standard errors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;serror &amp;lt;- sqrt(diag(solve(t(X) %*% W %*% X)))
# compare results
data.frame(B.manual = as.numeric(b), se.manual = serror)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                B.manual se.manual
## (Intercept)  0.23069482 0.3482383
## tr           0.93184838 0.3465451
## w1           0.60952221 0.2296474
## x1           0.57411101 0.1852545
## x2          -0.01320503 0.3218335&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(m1)$coef[,1:2] #using the glm function&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                Estimate Std. Error
## (Intercept)  0.23069482  0.3482386
## tr           0.93184838  0.3465455
## w1           0.60952221  0.2296481
## x1           0.57411101  0.1852548
## x2          -0.01320503  0.3218338&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Standard errors are the same (to the sixth decimal place) as those estimated using the &lt;code&gt;glm&lt;/code&gt; function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;heres-a-final-short-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Here’s a final short example&lt;/h2&gt;
&lt;p&gt;Just using the small, built-in &lt;code&gt;mtcars&lt;/code&gt; dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(mtcars)
table(mtcars$am) #the car transmission; just a toy outcome&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  0  1 
## 19 13&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m2 &amp;lt;- glm(am ~ mpg + wt, family = binomial, data = mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Computing this ‘manually’ using extracted matrices (object is now &lt;code&gt;m2&lt;/code&gt; for model 2):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- model.matrix(m2)
eta &amp;lt;- X %*% coef(m2)
mu &amp;lt;- as.vector(1 / (1 + exp(-eta)))
zz &amp;lt;- eta + (mtcars$am - mu) * (1 / (mu * (1 - mu)))
Ws &amp;lt;- diag(weights(m2, &amp;#39;working&amp;#39;))
estimate &amp;lt;- solve(t(X) %*% Ws %*% X) %*% t(X) %*% Ws %*% zz #est
se &amp;lt;- sqrt(diag(solve(t(X) %*% Ws %*% X))) #SE
z &amp;lt;- estimate / se
pv &amp;lt;- 2 * pt(-abs(z), df = Inf) #get the p values&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Put it all together and compare results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data.frame(estimate, se, z, pv)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               estimate        se         z         pv
## (Intercept) 25.8865522 12.193529  2.122975 0.03375598
## mpg         -0.3241639  0.239499 -1.353508 0.17589325
## wt          -6.4161717  2.546606 -2.519500 0.01175218&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(m2)$coef #based on GLM function&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               Estimate Std. Error   z value   Pr(&amp;gt;|z|)
## (Intercept) 25.8865540  12.193529  2.122975 0.03375597
## mpg         -0.3241639   0.239499 -1.353509 0.17589322
## wt          -6.4161721   2.546606 -2.519500 0.01175217&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Based on the notes from Dr. Wolfgang Wiedermann’s GLM course&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Logistic regression from scratch (Newton Raphson and Fisher Scoring)</title>
      <link>http://localhost:1313/post/logistic-regression-fisher-scoring/</link>
      <pubDate>Tue, 15 Feb 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/logistic-regression-fisher-scoring/</guid>
      <description>&lt;h2&gt;&lt;/h2&gt;
&lt;p&gt;In an earlier &lt;a href=&#34;../logistic-regression-using-matrices/&#34;&gt;post&lt;/a&gt;, I had shown this using iteratively reweighted least squares (IRLS). This is just an alternative method using Newton Raphson and the Fisher scoring algorithm. For further details, you can look here as &lt;a href=&#34;http://www.jtrive.com/estimating-logistic-regression-coefficents-from-scratch-r-version.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;well&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;library&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;MLMusingR&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;suspend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;m1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;glm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sus&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;~&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;male&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gpa&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;frpl&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;fight&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;frmp.c&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pminor.c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;suspend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          &lt;span class=&#34;n&#34;&gt;family&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;binomial&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;### extracting raw components&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;dat&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;model.frame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;fml&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;formula&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;model.matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fml&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;model.response&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#39;numeric&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;ncol&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#pp &amp;lt;- function(x) 1 / (1 + exp(-x)) #convert logit to pred prob&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;beta_0&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;rep&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#initialize with zeroes&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;eta&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%*%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;beta_0&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#predicted logit&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;mu&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;plogis&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;eta&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#same as pp but built-in; convert logit to pred prob&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;vr&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mu&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#variance&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;wts&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;diag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;as.vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#create weight matrix&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;d2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%*%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;wts&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%*%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#the Fisher information matrix&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;u1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%*%&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#the score function&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;iter&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;50&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#max number of iterations&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;tol&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1e-8&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#tolerance&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;### now iterate&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kr&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;kr&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;iter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;){&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;#beta_1 &amp;lt;- beta_0 + solve(d2) %*% u1 #update beta&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;beta_1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;beta_0&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;chol2inv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;chol&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%*%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;u1&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#update beta avoiding inversion&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;c1&#34;&gt;#print(beta_1) #show estimates&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;kr&#34;&gt;if&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;any&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;abs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;beta_1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;beta_0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;beta_0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tol&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;kr&#34;&gt;break&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#stop loop if no change&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;eta&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%*%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;beta_1&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#predicted logit&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;mu&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;plogis&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;eta&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#this is just pp(eta)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;vr&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mu&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#variance&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;wts&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;diag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;as.vector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#putting in a weight matrix&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;d2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%*%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;wts&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%*%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#information matrix&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;u1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%*%&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#score function&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;beta_0&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;beta_1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nf&#34;&gt;cat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;::&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#show iteration number&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;1 ::2 ::3 ::4 ::5 ::6 ::7 ::
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;### compare results&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;se&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;sqrt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;diag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;solve&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;d2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#get standard error, inverse of information matrix&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;z&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;beta_1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;se&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;p.val&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;pt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;abs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;z&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;Inf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;data.frame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;beta&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;beta_1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;se&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;se&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;z&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;z&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;format&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;p.val&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                     beta       se      z        p
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;(Intercept)     -1.592202 0.269404 -5.910 3.42e-09
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;male             0.324897 0.099384  3.269 1.08e-03
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;gpa             -0.795479 0.084849 -9.375 6.90e-21
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;frpl            -0.562734 0.318874 -1.765 7.76e-02
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;fight            2.078100 0.098472 21.103 7.40e-99
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;frmp.c           0.003004 0.003189  0.942 3.46e-01
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pminor.c        -0.002236 0.002302 -0.971 3.31e-01
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;gpa:frpl         0.387256 0.109169  3.547 3.89e-04
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;frmp.c:pminor.c  0.000124 0.000107  1.167 2.43e-01
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;#### to compare&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;m2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;update&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;epsilon&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1e-16&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#change tolerance so results match&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;summary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;coef&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                 Estimate Std. Error z value Pr(&amp;gt;|z|)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;(Intercept)     -1.592202   0.269404  -5.910 3.42e-09
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;male             0.324897   0.099384   3.269 1.08e-03
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;gpa             -0.795479   0.084849  -9.375 6.90e-21
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;frpl            -0.562734   0.318874  -1.765 7.76e-02
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;fight            2.078100   0.098472  21.103 7.40e-99
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;frmp.c           0.003004   0.003189   0.942 3.46e-01
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pminor.c        -0.002236   0.002302  -0.971 3.31e-01
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;gpa:frpl         0.387256   0.109169   3.547 3.89e-04
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;frmp.c:pminor.c  0.000124   0.000107   1.167 2.43e-01
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;## equal deviances&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;m&#34;&gt;-2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;[1] 3331
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;deviance&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;[1] 3331
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;an-alternative-using-maximum-likelihood-estimation-mle&#34;&gt;An alternative using maximum likelihood estimation (MLE)&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;m3&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;glm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sus&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;~&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;male&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gpa&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;frpl&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;gpa&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;frpl&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;suspend&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          &lt;span class=&#34;n&#34;&gt;family&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;binomial&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;model.matrix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;## this is the function to maximize&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;mle&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;kr&#34;&gt;function&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;){&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;pp&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;plogis&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%*%&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nf&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;dbinom&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;log&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;TRUE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;ncol&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#number of variables&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;sta&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;rep&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#initial values&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;out&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;optim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;par&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sta&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mle&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#starting values and function to optimize&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;control&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fnscale&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;-1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#-1 to maximize&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;reltol&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1e-15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#tolerance&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;maxit&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;hessian&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;TRUE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#so it doesn&amp;#39;t keep iterating&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;out&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;par&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#coefficients, difference due to tolerance?&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;[1] -0.371  0.673 -0.988 -0.543  0.406
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;coef&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#using built in function&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;(Intercept)        male         gpa        frpl    gpa:frpl 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     -0.371       0.673      -0.988      -0.543       0.406 
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;out&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;value&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#the log likelihood&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;[1] -1908
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;logLik&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;#39;log Lik.&amp;#39; -1908 (df=5)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;sqrt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;diag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;solve&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;out&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;hessian&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;#standard errors&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;[1] 0.2386 0.0931 0.0777 0.2941 0.1010
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-r&#34; data-lang=&#34;r&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nf&#34;&gt;sqrt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;diag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;vcov&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;m3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;(Intercept)        male         gpa        frpl    gpa:frpl 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     0.2386      0.0931      0.0777      0.2941      0.1010 
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&amp;ndash; END&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alternatives to logistic regression models in experimental studies</title>
      <link>http://localhost:1313/publication/journal-article/2022/huang-alternatives-2022/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/2022/huang-alternatives-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Alternatives to logistic regression models when analyzing cluster randomized trials with binary outcomes</title>
      <link>http://localhost:1313/publication/journal-article/alternatives/huang-alternatives-2021/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/alternatives/huang-alternatives-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>🎉 Comparing coefficients across logistic regression models</title>
      <link>http://localhost:1313/post/compare/</link>
      <pubDate>Sun, 28 Jun 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/compare/</guid>
      <description>&lt;p&gt;&lt;strong&gt;ROUGH NOTES&lt;/strong&gt;: &lt;span style=&#34;color: red;&#34;&gt;&lt;em&gt;[let me know if you spot any errors– there might be a couple!]&lt;/em&gt;&lt;/span&gt; Often, in randomized control trial where individuals are randomly assigned to treatment and control conditions, covariates are included to improve precision by reducing error and improving statistical power. However, when binary outcomes are used (e.g., patient recovers or not), there are several additional concerns that have gone unnoticed by many applied researchers.&lt;/p&gt;
&lt;p&gt;Take a simulated example where our true model data generating process is (to keep things simple, the intercept is zero and the parameters are both set to 1):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[log(\frac{p}{1 - p}) = 1 + 1 \times (tr) + 1 \times (x2)\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(summarytools)
library(stargazer)
&lt;p&gt;pp &amp;lt;- function(x) exp(x) / (1 + exp(x)) #convert logit to prob
set.seed(2468)
ns &amp;lt;- 10000
tr &amp;lt;- rbinom(ns, 1, .50) #50% treat / 50% control
x2 &amp;lt;- rnorm(ns, 0 , 2) #uncorrelated x2
ystar &amp;lt;- 1 + 1 * tr + 1 * x2 #all get a unit weight to keep it simple
y &amp;lt;- rbinom(ns, 1, pp(ystar))
ctable(y, tr, prop = &#39;c&#39;)&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Cross-Tabulation, Column Proportions  
## y * tr  
## 
## ------- ---- --------------- --------------- ----------------
##           tr               0               1            Total
##       y                                                      
##       0        1756 ( 35.0%)   1165 ( 23.3%)    2921 ( 29.2%)
##       1        3254 ( 65.0%)   3825 ( 76.7%)    7079 ( 70.8%)
##   Total        5010 (100.0%)   4990 (100.0%)   10000 (100.0%)
## ------- ---- --------------- --------------- ----------------&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In our example, think about y = 1 as recovered from illness and y = 0 as did not recover from illness. In the treatment group, &lt;em&gt;77%&lt;/em&gt; recovered vs. &lt;em&gt;65%&lt;/em&gt;. This is a difference of around 12 percentage points or a higher rate of recovery by a factor of 1.18 (77/65).&lt;/p&gt;
&lt;p&gt;Note that the two predictor variables are not correlated with each other:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- data.frame(y, tr, x2)
round(cor(df), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        y     tr     x2
## y  1.000  0.129  0.569
## tr 0.129  1.000 -0.017
## x2 0.569 -0.017  1.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When running a logistic regression model, notice how the coefficient changes even if &lt;code&gt;x2&lt;/code&gt; is not correlated with &lt;code&gt;tr&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reduced &amp;lt;- glm(y ~ tr, family = binomial)
full &amp;lt;- update(reduced, . ~ . + x2)
stargazer(reduced, full, star.cutoffs = c(.05, .01, .001),
          no.space = T, type = &amp;#39;text&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ================================================
##                        Dependent variable:      
##                   ------------------------------
##                                 y               
##                         (1)            (2)      
## ------------------------------------------------
## tr                   0.572***        1.005***   
##                       (0.045)        (0.059)    
## x2                                   1.040***   
##                                      (0.023)    
## Constant             0.617***        1.019***   
##                       (0.030)        (0.041)    
## ------------------------------------------------
## Observations          10,000          10,000    
## Log Likelihood      -5,956.977      -3,883.476  
## Akaike Inf. Crit.   11,917.950      7,772.952   
## ================================================
## Note:              *p&amp;lt;0.05; **p&amp;lt;0.01; ***p&amp;lt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The true effect should be around 1.0 (these are in logit units). In this basic example, there are several things to notice:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The coefficient for &lt;code&gt;tr&lt;/code&gt; is &lt;strong&gt;underestimated&lt;/strong&gt; in the first model (remember, I specified that the value should be 1).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The coefficients for &lt;code&gt;tr&lt;/code&gt; (treatment status) change with the inclusion of &lt;code&gt;x2&lt;/code&gt;. &lt;code&gt;x2&lt;/code&gt; is not your typical confounder which is associated both with y and the treatment status.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The standard error increases in the 2nd model– it does not decrease as one might expect.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As a result, this makes a few things problematic– even in the presence of randomization.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;We might conclude that &lt;code&gt;x2&lt;/code&gt; is a suppressor variable– but it is not!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This makes comparing results across models even within the same sample problematic. Results can change as long as variables predict the outcome.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will never have all predictors of interest in a model (w/c is why we don’t get an &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; of 1.00).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another question that might be raised is if model 1 is really incorrect. We know that the coefficient is lower than 1 but then by looking at the crosstabs, this actually matches what was shown earlier:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pp(.617) #control:: 65%&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6495359&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pp(.617 + .572) #treatment:: 77%&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7665622&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we take the model 2 coefficients (and holding &lt;code&gt;x2&lt;/code&gt; at its average of zero), the coefficients are correct (controlling for &lt;code&gt;x2&lt;/code&gt;) but then this is not reflected in the crosstabs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pp(1) #control:: 73%&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7310586&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pp(1 + 1) #treatment: 88%&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8807971&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;why-does-that-happen-with-lrms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why does that happen with LRMs?&lt;/h2&gt;
&lt;p&gt;In a standard OLS model, we have the intercept, slope, and the residual.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = \beta_0 + \beta_1  x_1 + e\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The residuals have a mean of 0 and variance, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_e\)&lt;/span&gt;. The residual variance depends on the model and students of OLS regression know that if a predictor is added to the model, the residual variance decreases (even if it is a useless variable; w/c is why we at times use adjusted R2 vs R2).&lt;/p&gt;
&lt;p&gt;However, in a logistic regression model, the variance of the error term is fixed to &lt;span class=&#34;math inline&#34;&gt;\(\frac{\pi^2}{3}\)&lt;/span&gt; or 3.29, which is the variance of a logistic distribution. Adding predictors to the model does not change the residual variance. As a result, the added variability is absorbed in the other parts of the model (at times referred to as an issue or rescaling or unobserved heterogeneity).&lt;/p&gt;
&lt;p&gt;Over three decades ago, Winship and Mare (1984, p. 517) stated:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Adding new independent variables to an equation alters the variance of Y and thus the remaining coefficients in the model, even if the new independent variables are uncorrelated with the original independent variables.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Mood (2010) more recently indicated (based off what WM wrote) that what is actually being estimated (&lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt;) in the first model is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[b_1 \approx \beta_1 \times \frac{\sqrt{3.29}}{\sqrt{3.29 + \beta^2_2var(x_2)}} = 1.0 \times \frac{\sqrt{3.29}}{\sqrt{3.29 + 1^2 (4) }}\]&lt;/span&gt;
As long as the denominator is greater than &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{3.29}\)&lt;/span&gt;, the estimated coefficient with the missing &lt;span class=&#34;math inline&#34;&gt;\(b_2\)&lt;/span&gt; will be underestimated. The underestimation depends on effect and variability of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The reason why coefficients may change when variables are added to the model may be due to:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;real confounding (the control variable is correlated with BOTH the treatment and the outcome) and/or&lt;/li&gt;
&lt;li&gt;rescaling.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the full model (with both &lt;code&gt;tr&lt;/code&gt; and &lt;code&gt;x2&lt;/code&gt;), the variability of the latent y variable (which causes y) is: &lt;span class=&#34;math inline&#34;&gt;\(1 * .25 + 1 * 4 + 3.29 = 7.54\)&lt;/span&gt;. The .25 is the variability of the treatment variable which is .5 * (1 - .5). In the reduced model (w/c omits &lt;code&gt;x2&lt;/code&gt;), the variability is: &lt;span class=&#34;math inline&#34;&gt;\(1 * .25 + 3.29 = 3.54\)&lt;/span&gt;. The variability– even though the same sample is being used– differs between models. &lt;strong&gt;This is an approximation? Does not result in the latent y var exactly.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-then-do-we-compare-results-across-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How then do we compare results across models?&lt;/h2&gt;
&lt;p&gt;If we stick with logistic regression, we have a few options.&lt;/p&gt;
&lt;div id=&#34;y-standardization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Y standardization&lt;/h3&gt;
&lt;p&gt;Y standardization has been around for a while (Winship &amp;amp; Mare, 1984). Involves taking the standard deviation of the latent y and using this as a scaling factor to create standardized coefficients for the predictors. Here’s a small function &lt;code&gt;ystd&lt;/code&gt; that will compute the y standardized variables to be used with the model coefficients. Basically, this takes the variance of the predicted logits from the model estimated, adds the constant of 3.29 which results in the total variance of the outcome. Then take the square root of the variance which is the standard deviation. Divide the coefficients by the SD of y* (y* is referred to as the latent y; above which some threshold, y = 1, if else, y = 0).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ystd &amp;lt;- function(x){
  evar &amp;lt;- (pi ^ 2) / 3 #3.29 #constant
  vr &amp;lt;- var(predict(x)) + evar #variance of predicted logits + evar
  sdy &amp;lt;- sqrt(vr) #getting the sd
  coef(x) / sdy
}
&lt;p&gt;ystd(reduced)&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)          tr 
##   0.3359345   0.3115061&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ystd(full)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)          tr          x2 
##   0.3628906   0.3578878   0.3701445&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The resulting coefficients can be interpreted as the usual standardized beta– for a one standard deviation increase in the predictor, results in a std coef change in the outcome (i.e., .31 and .36). However, for binary predictors, that doesn’t make much sense to me (e.g., a SD coefficient change in gender, race, etc.) since you have a unit change or nothing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;karlson-holm-breen-khb-method&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Karlson, Holm, &amp;amp; Breen (khb) method&lt;/h3&gt;
&lt;p&gt;Again, this involves comparing the full and reduced models. Two ways to think about it. We need to estimate the reduced model but include &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; &lt;em&gt;without really including it&lt;/em&gt;. A way to do that is to get the residuals of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; regressed on treatment (can just use OLS) and then include that residual as a predictor (don’t interpret it) in the model. The output below (.93) is much closer to 1.0.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- resid(lm(x2 ~ tr))
reduced2 &amp;lt;- glm(y ~ tr + res, family = binomial)
stargazer(reduced, reduced2, full,
          star.cutoffs = c(.05, .01, .001),
          no.space = T, type = &amp;#39;text&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ==================================================
##                         Dependent variable:       
##                   --------------------------------
##                                  y                
##                      (1)        (2)        (3)    
## --------------------------------------------------
## tr                 0.572***   0.933***   1.005*** 
##                    (0.045)    (0.058)    (0.059)  
## res                           1.040***            
##                               (0.023)             
## x2                                       1.040*** 
##                                          (0.023)  
## Constant           0.617***   1.034***   1.019*** 
##                    (0.030)    (0.041)    (0.041)  
## --------------------------------------------------
## Observations        10,000     10,000     10,000  
## Log Likelihood    -5,956.977 -3,883.476 -3,883.476
## Akaike Inf. Crit. 11,917.950 7,772.952  7,772.952 
## ==================================================
## Note:                *p&amp;lt;0.05; **p&amp;lt;0.01; ***p&amp;lt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to think about it is to get the predicted logits (the estimated y*) from the full model and then just use that as the outcome in a linear regression (since the predicted logits are not anymore ones and zeroes but are continuous). [I am not sure about the standard errors in these models, just the point estimates/coefficients: &lt;code&gt;reduced2&lt;/code&gt; is fine though].&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predy &amp;lt;- predict(full) #gets logits, do not use fitted which returns prob
reduced3 &amp;lt;- glm(predy ~ tr)
stargazer(reduced, reduced2, reduced3, full,
          star.cutoffs = c(.05, .01, .001),
          no.space = T, type = &amp;#39;text&amp;#39;,
          omit.stat = c(&amp;#39;f&amp;#39;, &amp;#39;ll&amp;#39;, &amp;#39;ser&amp;#39;, &amp;#39;rsq&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ===========================================================
##                              Dependent variable:           
##                   -----------------------------------------
##                            y             predy        y    
##                         logistic         normal   logistic 
##                      (1)        (2)       (3)        (4)   
## -----------------------------------------------------------
## tr                 0.572***  0.933***   0.933***  1.005*** 
##                    (0.045)    (0.058)   (0.042)    (0.059) 
## res                          1.040***                      
##                               (0.023)                      
## x2                                                1.040*** 
##                                                    (0.023) 
## Constant           0.617***  1.034***   1.034***  1.019*** 
##                    (0.030)    (0.041)   (0.030)    (0.041) 
## -----------------------------------------------------------
## Observations        10,000    10,000     10,000    10,000  
## Akaike Inf. Crit. 11,917.950 7,772.952 43,161.080 7,772.952
## ===========================================================
## Note:                         *p&amp;lt;0.05; **p&amp;lt;0.01; ***p&amp;lt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;use-the-khb-package&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Use the khb package&lt;/h4&gt;
&lt;p&gt;Another way is to use the &lt;code&gt;khb&lt;/code&gt; package. Compare results with what was computed earlier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(khb)
compareModels(reduced, full, method = &amp;#39;naive&amp;#39;) #no adjustments&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Reduced  Full    perc   
## (Intercept) 0.61685  1.0194  -65.253
## tr          0.57199  1.0053  -75.756
## x2                   1.0397         
##                                     
## Pseudo R2   0.023544 0.49965        
## Dev.        11914    7767           
## Null        12080    12080          
## Chisq       166.47   4313.5         
## Sig         ***      ***            
## Dl          1        2              
## BIC         11923    7785.4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compareModels(reduced, full, method = &amp;#39;ystand&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Reduced  Full    perc    
## (Intercept) 0.33593  0.36289  -8.0242
## tr          0.31151  0.35789 -14.8895
## x2                   0.37014         
##                                      
## Pseudo R2   0.023544 0.49965         
## Dev.        11914    7767            
## Null        12080    12080           
## Chisq       166.47   4313.5          
## Sig         ***      ***             
## Dl          1        2               
## BIC         11923    7785.4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compareModels(reduced, full, method = &amp;#39;khb&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Reduced Full    perc   
## (Intercept) 1.03378 1.0194   1.3950
## tr          0.93291 1.0053  -7.7599
## Resid(x2)   1.03973                
## x2                  1.0397         
##                                    
## Pseudo R2   0.49965 0.49965        
## Dev.        7767    7767           
## Null        12080   12080          
## Chisq       4313.5  4313.5         
## Sig         ***     ***            
## Dl          2       2              
## BIC         7785.4  7785.4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k &amp;lt;- khb(reduced, full) #to compare and get SEs
print(k)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## KHB method
## Model type: glm lm (logit) 
## Variables of interest: tr
## Z variables (mediators): x2
## 
## Summary of confounding
##       Ratio Percentage Rescaling
## tr  0.92799   -7.75989     1.631
## ------------------------------------------------------------------
## tr :
##          Estimate Std. Error z value Pr(&amp;gt;|z|)    
## Reduced  0.932912   0.058321 15.9963  &amp;lt; 2e-16 ***
## Full     1.005305   0.058673 17.1340  &amp;lt; 2e-16 ***
## Diff    -0.072393   0.041898 -1.7278  0.08402 .&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;kb&lt;/code&gt; results indicate that the difference between the full and reduced model shown are not statistically significant (p = .08; i.e., not different from zero).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;use-alternative-procedures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Use alternative procedures&lt;/h2&gt;
&lt;p&gt;NOTE: if we use alternative procedures such as a linear probability model (LPM) or a modified Poisson regression (I’m not adjusting for overdisperson in the example, just focusing on the regression coefficients), these are not subject to the same issues. In particular, the LPM works as we would expect where the coefficient for &lt;code&gt;tr&lt;/code&gt; is relatively unchanged and the standard errors decrease. I’ve also shown this in a prior study (Huang, 2019).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reduced.ols &amp;lt;- glm(y ~ tr, family = gaussian) #normal
full.ols &amp;lt;- update(reduced.ols, . ~ . + x2)
reduced.poisson &amp;lt;- glm(y ~ tr, family = poisson) #normal
full.poisson &amp;lt;- update(reduced.poisson, . ~ . + x2)
stargazer(reduced.ols, full.ols, 
          reduced.poisson, full.poisson,
          star.cutoffs = c(.05, .01, .001),
          no.space = T, type = &amp;#39;text&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## =============================================================
##                               Dependent variable:            
##                   -------------------------------------------
##                                        y                     
##                          normal                Poisson       
##                      (1)        (2)        (3)        (4)    
## -------------------------------------------------------------
## tr                 0.117***   0.126***   0.166***   0.180*** 
##                    (0.009)    (0.007)    (0.024)    (0.024)  
## x2                            0.129***              0.182*** 
##                               (0.002)               (0.006)  
## Constant           0.650***   0.648***  -0.432***  -0.502*** 
##                    (0.006)    (0.005)    (0.018)    (0.018)  
## -------------------------------------------------------------
## Observations        10,000     10,000     10,000     10,000  
## Log Likelihood    -6,226.338 -4,209.938 -9,500.248 -9,023.055
## Akaike Inf. Crit. 12,456.670 8,425.875  19,004.500 18,052.110
## =============================================================
## Note:                           *p&amp;lt;0.05; **p&amp;lt;0.01; ***p&amp;lt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you compare the results above to the crosstabs shown at the start, we find that the treatment group is around 12 percentage points higher than the control (shown using OLS) and the the rate of recovery of the treatment group is higher by a factor of ~1.2 (&lt;code&gt;exp(.18)&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Huang, F. (2019). Alternatives to logistic regression models with binary outcomes. Journal of Experimental Education. doi: 10.1080/00220973.2019.1699769&lt;/p&gt;
&lt;p&gt;Karlson, KB, Holm, A, &amp;amp; Breen, R (2012). Comparing regression coefficients between same-sample nested mModels using logit and probit: A new method. Sociological Methodology, 42(1), pp 286-313.&lt;/p&gt;
&lt;p&gt;Mood, C. (2010). Logistic regression: Why we cannot do what we think we can do, and what we can do about it. European Sociological Review, 26, 67-82. &lt;a href=&#34;https://doi.org/10.1093/esr/jcp006&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1093/esr/jcp006&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Studer, M. (2014). khb: Comparing nonlinear regression models. R package version 0.1.&lt;/p&gt;
&lt;p&gt;Winship, C., &amp;amp; Mare, R. D. (1984). Regression models with ordinal variables. American Sociological Review, 49, 512-525.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>🎉 Applied example for alternatives to logistic regression</title>
      <link>http://localhost:1313/post/applied/</link>
      <pubDate>Sun, 27 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/applied/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Logistic regression is often used to analyze experiments with binary
outcomes (e.g., pass vs fail) and binary predictors (e.g., treatment vs
control). Although appropriate, there are other possible models that can
be run that may provide easier to interpret results.&lt;/p&gt;
&lt;p&gt;In addition, some of these models may be quicker to run. Some may say
that this point is moot given the availability of computing power today
but if you’ve ever tried to run a hierarchical generalized linear model
with a logit link function and a binary outcome, you know that when
using R (using &lt;code&gt;glmer&lt;/code&gt; or &lt;code&gt;nlme&lt;/code&gt;) this may take quite a long time (and
cross your fingers that you don’t have convergence issues).&lt;/p&gt;
&lt;p&gt;The following code replicates the example (see the manuscript for
details) in the
&lt;a href=&#34;https://www.tandfonline.com/eprint/YS723ZYEIB2CPWKBEMPZ/full?target=10.1080/00220973.2019.1699769&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;article&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Huang, F. (2019). Alternatives to logistic regression models with
binary outcomes. Journal of Experimental Education. doi:
10.1080/00220973.2019.1699769&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Data are based on the article of Huang and Cornell (2015). Using an
online survey, investigators tested for the presence of the
question-order effect. Based on a random number generated when the
students took the survey, they were placed in either the treatment (n =
1037) or control (n = 963) condition. Students in the treatment
condition were asked four specific types bullying questions (i.e.,
verbal, physical, social, cyber) and then were asked a general bullying
question (“I have been bullied in the past year”). Students in the
control condition were asked the general bullying question first and
then the specific bullying questions. We hypothesized that students who
were asked the specific bullying questions first would report overall
higher bullying vs the control group.&lt;/p&gt;
&lt;h2 id=&#34;1-examining-cross-tabs&#34;&gt;1. Examining cross tabs&lt;/h2&gt;
&lt;p&gt;Load in the required packages:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(dplyr) #just for the pipe, %&amp;gt;%
library(logbin) #to run a log binomial model with a function
library(summarytools) #for nicer crosstabs
library(jtools) #for easier exp, confints, and adjusted standard errors
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Load and examine the data frame:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#specify the location on website
dat &amp;lt;- url(&amp;quot;https://github.com/flh3/pubdata/raw/refs/heads/main/JXE_Alt_Logistic/jxe.rdata&amp;quot;) 
load(dat) #load in data from the website
summary(jxe) #name of the data.frame

     abully           tord            female          gl     
 Min.   :0.000   Min.   :0.0000   Min.   :0.0000   9th :446  
 1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0000   10th:530  
 Median :0.000   Median :1.0000   Median :1.0000   11th:517  
 Mean   :0.126   Mean   :0.5185   Mean   :0.5005   12th:507  
 3rd Qu.:0.000   3rd Qu.:1.0000   3rd Qu.:1.0000             
 Max.   :1.000   Max.   :1.0000   Max.   :1.0000             
 race    
 w:1278  
 b: 287  
 h: 170  
 a:  79  
 o: 186  
         

head(jxe)

     abully tord female   gl race
6910      0    0      1 10th    w
8394      0    0      0  9th    w
7293      0    1      1  9th    w
8491      0    1      0 10th    w
4374      1    1      0 10th    w
1594      0    1      1 10th    b

dim(jxe)

[1] 2000    5
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Review some crosstabs. All computations are based on the table below.
&lt;code&gt;tord&lt;/code&gt; is the treatment variable. &lt;code&gt;abully&lt;/code&gt; indicates if the respondent
had been bullied.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(janitor)
jxe |&amp;gt; tabyl(abully, tord) |&amp;gt;
  adorn_percentages(denominator = &#39;col&#39;) |&amp;gt;
  adorn_pct_formatting(digits = 2) |&amp;gt; 
  adorn_ns(position = &#39;front&#39;) 

 abully            0            1
      0 863 (89.62%) 885 (85.34%)
      1 100 (10.38%) 152 (14.66%)

(14.66 / 85.34) / (10.38 / 89.62) #OR = 1.48 

[1] 1.483163

14.66 - 10.38 #risk difference: 4.28

[1] 4.28

14.66 / 10.38 #risk ratio: 1.41

[1] 1.412331
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;2-models-without-covariates&#34;&gt;2. Models without covariates&lt;/h2&gt;
&lt;p&gt;For comparability, see how the results above map on to the first set of
regressions without covariates:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tab6.log1 &amp;lt;- glm(abully ~ tord, data = jxe, family = binomial)
summ(tab6.log1, exp = T)$coef %&amp;gt;% round(3)

##             exp(Est.)  2.5% 97.5%  z val.     p
## (Intercept)     0.116 0.094 0.143 -20.404 0.000
## tord            1.482 1.132 1.940   2.865 0.004

tab6.lpm1 &amp;lt;- glm(abully ~ tord, data = jxe) #risk difference
summ(tab6.lpm1, confint = T, robust = &#39;HC3&#39;)$coef %&amp;gt;% round(3)

##              Est.  2.5% 97.5% t val.     p
## (Intercept) 0.104 0.085 0.123 10.553 0.000
## tord        0.043 0.014 0.072  2.896 0.004

tab6.poi1 &amp;lt;- glm(abully ~ tord, data = jxe, family = poisson) #risk ratio
summ(tab6.poi1, robust = &#39;HC3&#39;)$coef %&amp;gt;% round(3)

##               Est.  S.E.  z val.     p
## (Intercept) -2.265 0.095 -23.900 0.000
## tord         0.345 0.121   2.852 0.004
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to run a log-binomial model&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;logb &amp;lt;- update(tab6.lpm1, family = binomial(link = &#39;log&#39;))
#logb &amp;lt;- logbin(abully ~ tord, data = jxe)
summ(logb, exp = T, digits = 3, robust = &#39;HC3&#39;)$coef %&amp;gt;% round(3)

            exp(Est.)  2.5% 97.5%  z val.     p
(Intercept)     0.104 0.086 0.125 -23.900 0.000
tord            1.412 1.114 1.789   2.852 0.004
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;3-models-with-covariates&#34;&gt;3. Models with covariates&lt;/h2&gt;
&lt;p&gt;Compare these to the results in Table 6 in the article with the
covariates:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tab6.log2 &amp;lt;- glm(abully ~ tord + female + gl + race, data = jxe, family = binomial)
summ(tab6.log2, exp = T)$coef %&amp;gt;% round(3)

            exp(Est.)  2.5% 97.5%  z val.     p
(Intercept)     0.149 0.105 0.211 -10.706 0.000
tord            1.487 1.134 1.951   2.866 0.004
female          1.184 0.906 1.547   1.239 0.215
gl10th          0.749 0.525 1.068  -1.597 0.110
gl11th          0.633 0.438 0.914  -2.439 0.015
gl12th          0.515 0.348 0.761  -3.333 0.001
raceb           0.646 0.411 1.015  -1.895 0.058
raceh           1.227 0.780 1.929   0.885 0.376
racea           1.512 0.826 2.769   1.340 0.180
raceo           1.165 0.747 1.815   0.674 0.501

tab6.lpm2 &amp;lt;- update(tab6.log2, family = gaussian)
summ(tab6.lpm2, confint = T, robust = &#39;HC3&#39;)$coef %&amp;gt;% round(3)

              Est.   2.5%  97.5% t val.     p
(Intercept)  0.138  0.098  0.178  6.707 0.000
tord         0.042  0.013  0.071  2.873 0.004
female       0.018 -0.011  0.047  1.231 0.218
gl10th      -0.038 -0.083  0.008 -1.631 0.103
gl11th      -0.055 -0.100 -0.011 -2.440 0.015
gl12th      -0.074 -0.117 -0.030 -3.331 0.001
raceb       -0.040 -0.077 -0.003 -2.109 0.035
raceh        0.024 -0.033  0.081  0.825 0.409
racea        0.051 -0.035  0.138  1.161 0.246
raceo        0.017 -0.037  0.071  0.625 0.532

tab6.poi2 &amp;lt;- update(tab6.log2, family = poisson)
summ(tab6.poi2, exp = T, robust = &#39;HC3&#39;)$coef %&amp;gt;% round(3)

            exp(Est.)  2.5% 97.5%  z val.     p
(Intercept)     0.128 0.096 0.172 -13.815 0.000
tord            1.411 1.114 1.787   2.852 0.004
female          1.157 0.917 1.458   1.232 0.218
gl10th          0.785 0.581 1.060  -1.581 0.114
gl11th          0.678 0.494 0.929  -2.421 0.015
gl12th          0.563 0.401 0.793  -3.295 0.001
raceb           0.678 0.449 1.024  -1.845 0.065
raceh           1.190 0.811 1.746   0.888 0.374
racea           1.417 0.858 2.339   1.363 0.173
raceo           1.140 0.781 1.664   0.678 0.498

tab6.logbin &amp;lt;- update(tab6.log2, family = binomial(link = &#39;log&#39;))
## using logbinomial, adding `em` to speed up the estimation
## tab6.logbin &amp;lt;- logbin(abully ~ tord + female + gl + race, 
##              data = jxe, method = &#39;em&#39;)
summ(tab6.logbin, exp = T, confint = T, robust = &#39;HC3&#39;)$coef %&amp;gt;% round(3)

            exp(Est.)  2.5% 97.5%  z val.     p
(Intercept)     0.128 0.096 0.171 -13.869 0.000
tord            1.412 1.115 1.788   2.866 0.004
female          1.165 0.924 1.468   1.291 0.197
gl10th          0.780 0.578 1.052  -1.628 0.104
gl11th          0.672 0.491 0.921  -2.470 0.013
gl12th          0.565 0.402 0.794  -3.288 0.001
raceb           0.674 0.446 1.019  -1.872 0.061
raceh           1.187 0.809 1.741   0.876 0.381
racea           1.422 0.863 2.342   1.382 0.167
raceo           1.142 0.782 1.666   0.687 0.492
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The pattern of results are similar. The Poisson, log-binomial, and
linear probability models however may provide results that are easier to
understand (especially if communicating results to a lay audience who do
not understand odds ratios).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Huang, F., &amp;amp; Cornell, D. (2015). Order and definitional effects in
bullying surveys: Results from an experimental study. &lt;em&gt;Psychological
Assessment, 27,&lt;/em&gt; 1484-1493. doi: &lt;a href=&#34;http://dx.doi.org/10.1037/pas0000149&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://dx.doi.org/10.1037/pas0000149&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;– END&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
