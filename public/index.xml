<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Francis Huang | FLH Website</title>
    <link>http://localhost:1313/</link>
      <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <description>Francis Huang</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu_258291280f5c9c98.png</url>
      <title>Francis Huang</title>
      <link>http://localhost:1313/</link>
    </image>
    
    <item>
      <title>Youth Violence Project @ UVA</title>
      <link>http://localhost:1313/project/uva/</link>
      <pubDate>Sat, 01 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/uva/</guid>
      <description>&lt;p&gt;Our team of faculty and graduate students conducts research on effective methods and policies for youth violence prevention and school safety.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>🎉 Job postings 2024-2025</title>
      <link>http://localhost:1313/post/jobs-2025/</link>
      <pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/jobs-2025/</guid>
      <description>&lt;p&gt;&lt;strong&gt;List of Research, Statistics, and Evaluation job postings (that I&amp;rsquo;ve seen) as of Feb 2025.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Postings for (2024-2025):&lt;/p&gt;
&lt;p&gt;As of 2025.01&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Post-doctoral Research Associate- Educational Measurement and Statistics- &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=179041668&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Tennessee&lt;/a&gt;, Knoxville, Knoxville, TN&lt;/li&gt;
&lt;li&gt;Learning Sciences Faculty Cluster Hires (Open-Rank and Tenure-Track; Educational Data Science and Analytics, AI and Education, and Advanced Quantitative Research Methods, Measurement and/or Evaluation; &lt;strong&gt;5&lt;/strong&gt; positions) &lt;a href=&#34;https://apply.interfolio.com/162168&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Clemson University&lt;/a&gt; in Clemson, SC&lt;/li&gt;
&lt;li&gt;Assistant Director for Education Research, The &lt;a href=&#34;https://uic.csod.com/ux/ats/careersite/1/home/requisition/13915?c=uic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Illinois at Chicago&lt;/a&gt; in Chicago, IL&lt;/li&gt;
&lt;li&gt;Assistant/Associate Professor of Research Design, Methods, and Analysis, &lt;a href=&#34;https://jobs.wm.edu/postings/63482&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;William &amp;amp; Mary&lt;/a&gt; in Williamsburg, VA&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As of 2024.12&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assistant Professor in Quantitative Methods in Educational Research, &lt;a href=&#34;https://recruit.ap.ucsb.edu/JPF02883&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UC Santa Barbara&lt;/a&gt; (NOTE: may also be at the Associate level)&lt;/li&gt;
&lt;li&gt;Assistant/Associate Professor, Quantitative and/or Mixed Methods Applied Research, &lt;a href=&#34;https://appstate.peopleadmin.com/postings/48926&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Appalachian State University&lt;/a&gt;, North Carolina, United States&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As of 2024.11&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assistant Professor in Quantitative Methods in Educational Research, &lt;a href=&#34;https://recruit.ap.ucsb.edu/JPF02883&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of California&lt;/a&gt; Santa Barbara&lt;/li&gt;
&lt;li&gt;Tenure Eligible (probationary) Open Rank for the Department of Emergency Medicine as a Research Methodologist, Professor (Open Rank) - R106021
&lt;a href=&#34;https://uofl.wd1.myworkdayjobs.com/en-US/UofLCareerSite/job/Health-Sciences-Center/Professor--Open-Rank-_R106021?jobFamilyGroup=122eabeb58371000c79b8793f92f0000?source=francish&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Louisville&lt;/a&gt; in Louisville, KY&lt;/li&gt;
&lt;li&gt;Assistant or Associate Professor of Critical Quantitative Research &lt;a href=&#34;https://aprecruit.ucr.edu/JPF02027&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of California&lt;/a&gt;, Riverside, CA&lt;/li&gt;
&lt;li&gt;Assistant Professor of Psychology - Quantitative Analysis, &lt;a href=&#34;https://jobs.oakland.edu/postings/32847&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Oakland University&lt;/a&gt;, Department of Psychology, Michigan&lt;/li&gt;
&lt;li&gt;Assistant/Associate Professor, Educational Research, &lt;a href=&#34;https://usm.csod.com/ats/careersite/JobDetails.aspx?id=4410&amp;amp;site=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Southern Mississippi&lt;/a&gt;, Hattiesburg, MS&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As of 2024.10&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assistant Professor in the SDSU College of Education focused on Quantitative Research Methods, &lt;a href=&#34;https://apply.interfolio.com/156881&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;San Diego State University&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Assistant Professor of Quantitative Methods and Assessment in Education, &lt;a href=&#34;https://uni.wd5.myworkdayjobs.com/en-US/UNI/job/Main-Campus/Assistant-Professor-of-Quantitative-Methods-and-Assessment-in-Education_JR283&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Northern Iowa&lt;/a&gt;, Cedar Falls, IA&lt;/li&gt;
&lt;li&gt;Assistant Professor-Educational Research, Measurement, and Analysis, &lt;a href=&#34;https://www.auemployment.com/postings/49210&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Auburn University&lt;/a&gt;, Auburn, AL&lt;/li&gt;
&lt;li&gt;Assistant Professor - Psychology, &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178937871&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Arkansas&lt;/a&gt; at Little Rock, AR&lt;/li&gt;
&lt;li&gt;School Of Education-Tenure-track Assistant Professor- Educational Psychology And Research; Department Human Studies, &lt;a href=&#34;https://uab.peopleadmin.com/postings/23082&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Alabama at Birmingham&lt;/a&gt;, Birmingham, AL&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As of 2024.09&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assistant/Associate Professor Of Quantitative Methodology, &lt;a href=&#34;https://facultycareers.gsu.edu/postings/4858&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Georgia State University&lt;/a&gt; Atlanta, GA&lt;/li&gt;
&lt;li&gt;Assistant Professor (Quant Psych), &lt;a href=&#34;https://unc.peopleadmin.com/postings/288570&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of North Carolina&lt;/a&gt; at Chapel Hill&lt;/li&gt;
&lt;li&gt;Open-Rank Tenure-Track Faculty Position in Quantitative Methods, &lt;a href=&#34;https://usccareers.usc.edu/job/los-angeles/open-rank-tenure-track-faculty-position-in-quantitative-methods-rossier-school-of-education/1209/70221450528&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rossier School of Education&lt;/a&gt;, Los Angeles, California&lt;/li&gt;
&lt;li&gt;Tenured Associate or Full Professor in Evaluation Studies (# 364671), &lt;a href=&#34;https://hr.myu.umn.edu/psc/hrprd/EMPLOYEE/HRMS/c/HRS_HRAM_FL.HRS_CG_SEARCH_FL.GBL?Page=HRS_APP_JBPST_FL&amp;amp;Action=U&amp;amp;SiteId=1&amp;amp;FOCUS=Applicant&amp;amp;JobOpeningId=364671&amp;amp;PostingSeq=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Minnesota&lt;/a&gt;, Twin Cities, MN&lt;/li&gt;
&lt;li&gt;Assistant/Associate Professor - QME Educational Statistics, &lt;a href=&#34;https://hr.myu.umn.edu/psc/hrprd/EMPLOYEE/HRMS/c/HRS_HRAM_FL.HRS_CG_SEARCH_FL.GBL?Page=HRS_APP_JBPST_FL&amp;amp;Action=U&amp;amp;SiteId=1&amp;amp;FOCUS=Applicant&amp;amp;JobOpeningId=364621&amp;amp;PostingSeq=1&amp;amp;&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Minnesota&lt;/a&gt; Twin Cities, MN&lt;/li&gt;
&lt;li&gt;Assistant Professor of Quantitative Psychology, &lt;a href=&#34;https://careers.pageuppeople.com/873/lb/en-us/job/541543/assistant-professor-of-quantitative-psychology&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;California State University&lt;/a&gt;, Long Beach&lt;/li&gt;
&lt;li&gt;Assistant Professor, Quantitative Psychology (Open Area), &lt;a href=&#34;https://webapps.cmc.edu/jobs/faculty/faculty_opening_detail.php?PostingID=17040&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Claremont McKenna College&lt;/a&gt;, California&lt;/li&gt;
&lt;li&gt;Assistant/Associate Professor of Evaluation in Education, &lt;a href=&#34;https://recruit.apo.ucla.edu/JPF09709&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of California Los Angeles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Associate/Full Professor, Tenured, Quantitative Methods, Educational Psychology, &lt;a href=&#34;https://apply.interfolio.com/153027&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Baylor University&lt;/a&gt;, Waco, TX&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As of 2024.08&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assistant Professor, Psychology (Quantitative) Tenure-track, &lt;a href=&#34;https://jobs.chronicle.com/job/37701200/assistant-professor-psychology-quantitative-tenure-track/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ohio University&lt;/a&gt;, Athens, Ohio&lt;/li&gt;
&lt;li&gt;Assistant Professor of Quantitative Psychology, &lt;a href=&#34;https://jobs.chronicle.com/job/37695060/assistant-professor-of-quantitative-psychology/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;California State University&lt;/a&gt;, Long Beach&lt;/li&gt;
&lt;li&gt;Research Assistant Professor, &lt;a href=&#34;https://employment.unl.edu/postings/92950&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nebraska Academy for Methodology, Analytics &amp;amp; Psychometrics&lt;/a&gt;, Nebraska Center for Research on Children, Youth, Families and Schools, College of Education and Human Sciences, University of Nebraska-Lincoln&lt;/li&gt;
&lt;li&gt;Assistant Professor, Tenure-Track, &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178892364&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Educational Data Science&lt;/a&gt; - Counseling, School, and Educational Psychology, University at Buffalo, Buffalo, NY&lt;/li&gt;
&lt;li&gt;Lecturer, &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178889719&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multivariate Statistics&lt;/a&gt;, University of California Santa Barbara, Santa Barbara, CA&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As of 2024.07&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Assistant Prof, &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178848717&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Educational Studies&lt;/a&gt;, University of Cincinnati, Cincinnati, OH&lt;/li&gt;
&lt;li&gt;Assistant Professor in Mixed Methods, Qualitative and Quantitative Tenure-Track, &lt;a href=&#34;https://jobs.chronicle.com/job/37678495/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;St. John&amp;rsquo;s University&lt;/a&gt;, New York&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;mdash;END&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Behavioral threat assessment and equity in exclusionary school discipline</title>
      <link>http://localhost:1313/publication/journal-article/2025/cornell-behavioral-2025/</link>
      <pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/2025/cornell-behavioral-2025/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cluster-robust standard errors with three-level data</title>
      <link>http://localhost:1313/publication/journal-article/2025/huang-cluster-robust-2025/</link>
      <pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/2025/huang-cluster-robust-2025/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Referral rates for school threat assessment</title>
      <link>http://localhost:1313/publication/journal-article/2025/cornell-referral-2025/</link>
      <pubDate>Wed, 01 Jan 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/2025/cornell-referral-2025/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Restorative justice conferences in elementary and secondary schools</title>
      <link>http://localhost:1313/publication/journal-article/test2/gregory-restorative-2025/</link>
      <pubDate>Wed, 01 Jan 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test2/gregory-restorative-2025/</guid>
      <description>&lt;p&gt;Add the &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; for the publication here using Markdown formatting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Advancing diversity, equity, and inclusion in school psychology science and scholarship: Changing training and practice in the field of school psychology</title>
      <link>http://localhost:1313/publication/journal-article/test3/jimerson-advancing-2024/</link>
      <pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test3/jimerson-advancing-2024/</guid>
      <description>&lt;p&gt;Add the &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; for the publication here using Markdown formatting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Potential moderation across racial groups in perceptions of authoritative school climate and peer victimization and student engagement</title>
      <link>http://localhost:1313/publication/journal-article/test3/chuang-potential-2022/</link>
      <pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test3/chuang-potential-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The seasonality of school climate</title>
      <link>http://localhost:1313/publication/journal-article/test3/huang-seasonality-2023/</link>
      <pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test3/huang-seasonality-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>School bullying and gender minority youth: Victimization experiences and perceived prevalence</title>
      <link>http://localhost:1313/publication/journal-article/test3/smith-school-2024/</link>
      <pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test3/smith-school-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MO Prison Transformation Project</title>
      <link>http://localhost:1313/project/av/</link>
      <pubDate>Sat, 26 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/av/</guid>
      <description>&lt;p&gt;This project uses a randomized controlled trial to study the impact of improving living and working conditions in adult correctional facilities in Missouri.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Having a trusted adult in school: Concurrent and predictive relations with internalizing problems across development</title>
      <link>http://localhost:1313/publication/journal-article/test3/reinke-having-2024/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test3/reinke-having-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Positive student-teacher relationships and exclusionary discipline practices</title>
      <link>http://localhost:1313/publication/journal-article/test3/eddy-positive-2024/</link>
      <pubDate>Thu, 01 Aug 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test3/eddy-positive-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Steadfast implementation of restorative practices during the Covid-19 pandemic: A case study of consultancy and leadership in an elementary school</title>
      <link>http://localhost:1313/publication/journal-article/test2/gregory-steadfast-2023/</link>
      <pubDate>Mon, 01 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test2/gregory-steadfast-2023/</guid>
      <description>&lt;p&gt;Add the &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; for the publication here using Markdown formatting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Projects</title>
      <link>http://localhost:1313/projects/</link>
      <pubDate>Sun, 19 May 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/projects/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Replication studies using secondary and nonexperimental datasets</title>
      <link>http://localhost:1313/event/example/</link>
      <pubDate>Mon, 13 May 2024 13:00:00 +0000</pubDate>
      <guid>http://localhost:1313/event/example/</guid>
      <description>&lt;h1&gt;&lt;/h1&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;&lt;p&gt;Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.&lt;/p&gt;
&lt;h1&gt;&lt;/h1&gt;&lt;/span&gt;
&lt;/div&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;lecture&#34; srcset=&#34;
               /event/example/lecture_hu_72803af5273a7270.webp 400w,
               /event/example/lecture_hu_f445ed1fac9e8a49.webp 760w,
               /event/example/lecture_hu_80229ba9a13b0c49.webp 1200w&#34;
               src=&#34;http://localhost:1313/event/example/lecture_hu_72803af5273a7270.webp&#34;
               width=&#34;608&#34;
               height=&#34;398&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multilevel Modeling with Large-Scale International Databases Using HLM (Philadelphia)</title>
      <link>http://localhost:1313/event/mlm_ph/</link>
      <pubDate>Wed, 10 Apr 2024 13:00:00 +0000</pubDate>
      <guid>http://localhost:1313/event/mlm_ph/</guid>
      <description>&lt;p&gt;















&lt;figure  id=&#34;figure-our-team-from-mu-air-and-iea&#34;&gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;lecture&#34; srcset=&#34;
               /event/mlm_ph/lecture_hu_993d7e4e9432b042.webp 400w,
               /event/mlm_ph/lecture_hu_391cd18ed58d733c.webp 760w,
               /event/mlm_ph/lecture_hu_243d7193e5c88939.webp 1200w&#34;
               src=&#34;http://localhost:1313/event/mlm_ph/lecture_hu_993d7e4e9432b042.webp&#34;
               width=&#34;760&#34;
               height=&#34;570&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Our team from MU, AIR and IEA.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Replication studies using secondary or nonexperimental datasets</title>
      <link>http://localhost:1313/publication/journal-article/test2/huang-replication-2024/</link>
      <pubDate>Mon, 01 Apr 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test2/huang-replication-2024/</guid>
      <description>&lt;p&gt;We present a framework of different types of replication studies with nonexperimental or secondary data and provide examples in the context of school psychology.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Management (using R)</title>
      <link>http://localhost:1313/teaching/dm/</link>
      <pubDate>Sun, 03 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/teaching/dm/</guid>
      <description>&lt;p&gt;Good data management is a prerequisite for successful research, needed for reproducibility of results, and essential when collaborating with others.&lt;/p&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;&lt;a href=&#34;https://dm2023.netlify.app/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://dm2023.netlify.app/&lt;/a&gt;&lt;/span&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Monte Carlo Simulation</title>
      <link>http://localhost:1313/teaching/mcsim/</link>
      <pubDate>Sun, 03 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/teaching/mcsim/</guid>
      <description>&lt;p&gt;How to conduct Monte Carlo Simulation studies.&lt;/p&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;&lt;a href=&#34;https://mumcs.netlify.app/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://mumcs.netlify.app/&lt;/a&gt;&lt;/span&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Multilevel Modeling</title>
      <link>http://localhost:1313/teaching/mlm/</link>
      <pubDate>Sun, 03 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/teaching/mlm/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://mlmusingr.netlify.app/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://mlmusingr.netlify.app/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Program Evaluation</title>
      <link>http://localhost:1313/teaching/eval/</link>
      <pubDate>Sun, 03 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/teaching/eval/</guid>
      <description>&lt;p&gt;Evaluating the quantifiable impact of social programs is a key task that policy makers, governments, and program funders perform. In education and the social sciences, a fundamental question asked is “How do we know our policy or program works?” Topics to be covered will include the analysis of experimental data, clustered data (e.g., multilevel models, fixed effect models- as most data in education involve nested data), the use of difference in difference models, regression discontinuity designs, and power analysis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quantitative Foundations</title>
      <link>http://localhost:1313/teaching/quantfound/</link>
      <pubDate>Sun, 03 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/teaching/quantfound/</guid>
      <description>&lt;p&gt;This course is designed to provide students the fundamental and necessary quantitative methods in educational research. Topics include one-way and factorial analysis of variance (ANOVA), analysis of covariance (ANCOVA), multiple regression, regression diagnostics, basic mediation analysis, and statistical power analysis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using plausible values when fitting multilevel models with large-scale assessment data using R</title>
      <link>http://localhost:1313/publication/journal-article/test3/huang-using-2024/</link>
      <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test3/huang-using-2024/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/f/f3/Open_Access_PLoS.svg&#34; alt=&#34;logo&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

Article is open access. The &lt;code&gt;mixPV&lt;/code&gt; function can now be accessed by installing the &lt;code&gt;MLMusingR&lt;/code&gt; package.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>🎉 Job postings 2023-2024</title>
      <link>http://localhost:1313/post/jobs-2024/</link>
      <pubDate>Tue, 27 Feb 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/jobs-2024/</guid>
      <description>&lt;p&gt;List of Research, Statistics, and Evaluation job postings (that I&amp;rsquo;ve seen) as of April 2024.&lt;/p&gt;
&lt;p&gt;Postings for (2023-2024):&lt;/p&gt;
&lt;p&gt;As of 2024.04&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Visiting Assistant Professor- &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178747312&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Educational Research&lt;/a&gt;, University of Southern Mississippi, Hattiesburg, MS&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As of 2024.03&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Assistant Professor in Evaluation (tenure track). Louisville, KY. &lt;a href=&#34;https://uofl.wd1.myworkdayjobs.com/UofLCareerSite/job/Belknap-Campus/Assistant-Professor--Tenure-Track----Human-Resource-Organizational-Development-Program---LEAD-Department---College-of-Education-and-Human-Development_R103760&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Louisville&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Postdoc, Research and Evaluation Methodology. Gainesville, FL. &lt;a href=&#34;https://explore.jobs.ufl.edu/en-us/job/530453/postdoctoral-scholar-research-and-evaluation-methodology&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Florida&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As of 2023.12&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Associate/Full Professor of Quantitative Research Methods, &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178628192&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Texas at Arlington&lt;/a&gt; in Arlington, TX&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As of 2023.11&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Tenure-Track Assistant Professor position in Education Statistics and Data Science, The &lt;a href=&#34;https://careers.udel.edu/cw/en-us/job/500590/tenuretrack-assistant-professor-in-education-statistics-and-data-science&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Delaware&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Assistant Professor of Quantitative Research Methods, Tenure-Track, &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178618815&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chapman University&lt;/a&gt;, Orange, CA&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Assistant/Associate Professor - Educational Research, Measurement, and Analysis, &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178616183&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Auburn University&lt;/a&gt;, Auburn, AL&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Assistant/Associate Professor in Quantitative Research Methodology, &lt;a href=&#34;https://jobs.chronicle.com/job/37562644/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Oklahoma State University&lt;/a&gt;, Stillwater, Oklahoma&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Assistant/Associate/Full Professor, Methodologist, &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178600024&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of North Florida&lt;/a&gt; in Jacksonville, FL&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Assistant Professor of Psychology (Quantitative Methods), &lt;a href=&#34;https://jobs.chronicle.com/job/37556414/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kennesaw State University&lt;/a&gt;, Georgia&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Data &amp;amp; Policy Fellow - MDHHS MIHP, &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178593591&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Michigan&lt;/a&gt; in Ann Arbor, MI&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As of 2023.10&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Associate/Full &amp;amp; Assistant/Associate Professors (Two Tenure Track Positions) - Educational Statistics, Measurement, and Evaluation &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178589470&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rutgers, The State University of New Jersey&lt;/a&gt; in New Brunswick, NJ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Assistant Professor of Psychology - Quantitative Psychology with Educational Outcomes, &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178588158&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Oklahoma&lt;/a&gt;, Norman, OK&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Visiting Assistant Professor/Visiting Instructor (non-tenure track), Data Science and Learning Analytics - Counseling, School, and Educational Psychology, &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178588923&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University at Buffalo in Buffalo&lt;/a&gt;, NY&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Faculty Position (tenure track)- Educational Statistics and Research, &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178586125&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gardner-Webb University&lt;/a&gt;, Boiling Springs, NC&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Full Professor, Measurement &amp;amp; Evaluation in Schools, &lt;a href=&#34;https://jobs.chronicle.com/job/37549792/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Binghamton University College of Community and Public Affairs&lt;/a&gt;, Binghamton, New York&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Assistant Professor in Educational Statistics (SRM), &lt;a href=&#34;https://jobs.chronicle.com/job/37547101/assistant-professor-in-educational-statistics-srm-/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of California Los Angeles&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Clinical Assistant Professor of Research and Evaluation Methodology, &lt;a href=&#34;https://explore.jobs.ufl.edu/en-us/job/528895/clinical-assistant-professor-of-research-and-evaluation-methodology&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Florida&lt;/a&gt;, Gainesville, FL, REMS program&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Assistant Professor, &lt;a href=&#34;https://aprecruit.ucmerced.edu/JPF01611&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of California, Merced&lt;/a&gt; - Psychological Sciences&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Teaching Assistant Professor, &lt;a href=&#34;https://okstate.csod.com/ats/careersite/JobDetails.aspx?id=16464&amp;amp;site=8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Oklahoma State University&lt;/a&gt; -  Research, Evaluation, Measurement and Statistics&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As of 2023.09&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Open Rank, Tenure-Eligible Position - Quantitative Research Methods &amp;amp; Education Policy, &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178543263&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Kentucky&lt;/a&gt;, Lexington, KY&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Assistant Professor in Quantitative Methods and Computational Psychology, &lt;a href=&#34;https://usccareers.usc.edu/job/los-angeles/assistant-professor-in-quantitative-methods-and-computational-psychology/1209/54627205408&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Southern California&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Assistant Professor of Quantitative Methodology (educational data mining and learning analytics), &lt;a href=&#34;https://www.ugajobsearch.com/postings/341715&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Georgia&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Two&lt;/strong&gt; positions, Quantitative Methods in Education, University of Minnesota. 1) Applied measurement &lt;a href=&#34;https://hr.myu.umn.edu/jobs/ext/357308&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Assoc or Full&lt;/a&gt;. 2) Educational statistics &lt;a href=&#34;https://hr.myu.umn.edu/jobs/ext/357169&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Open rank&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Assistant/Associate Professor of Evaluation &amp;amp; Education, &lt;a href=&#34;https://recruit.apo.ucla.edu/JPF08711&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UCLA&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Assistant Professor, Educational Research/ Higher Education Administration, &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178538367&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The University of Southern Mississippi&lt;/a&gt;, Hattiesburg, MS&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Assistant Professor of Education and Applied Quantitative Methods &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178524105&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Maine&lt;/a&gt;, Orono, ME&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lecturer III- Quantitative Psychology, &lt;a href=&#34;https://apply.interfolio.com/128156&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Michigan&lt;/a&gt;-Ann Arbor&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As of 2023.08&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;QME (Quantitative Methods in Education Program) Educational Statistics (Open Rank), &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178513717&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Minnesota&lt;/a&gt;, Twin Cities, MN&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Assistant Professor in Educational Research, &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178516382&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towson University&lt;/a&gt;, Towson, MD&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Assistant or Associate Professor in Educational Data Science, &lt;a href=&#34;https://apply.interfolio.com/129607&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The University of Tennessee - Knoxville&lt;/a&gt;: Knoxville Academic Units: College of Education, Health, and Human Sciences: Theory &amp;amp; Practice in Teacher Education&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Assistant Professor of Psychology (Quantitative) Tenure Track, &lt;a href=&#34;https://www.ohiouniversityjobs.com/postings/47152&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ohio University&lt;/a&gt;, Athens, Ohio&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tenure-Track Assistant or Associate Professor-School of Education and Urban Studies- &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178512470&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantitative Methodologist&lt;/a&gt;, Morgan State University&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;College of Education, Tenure-Track Assistant Professor in &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178508518&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantitative Methods&lt;/a&gt; University of Texas at Austin, Austin, TX&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Postdoctoral Associate: Quantitative Education Policy Research, &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178495367&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Center for Antiracist Research&lt;/a&gt;, Boston University&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Quantitative Methods Doctoral Coursework Instructor, &lt;a href=&#34;https://careers.liberty.edu/?job_posting=R0002097&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Liberty University&lt;/a&gt;, Lynchburg, VA&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;mdash;END&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adolescent exposure to restorative practices and their perceptions of support, structure, and bullying in the school climate</title>
      <link>http://localhost:1313/publication/journal-article/test3/gregory-adolescent-2024/</link>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test3/gregory-adolescent-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Investigating the use of robust standard errors to account for two-way clustering in cross-classified data structures</title>
      <link>http://localhost:1313/publication/journal-article/2025/stemmler-investigating-2024/</link>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/2025/stemmler-investigating-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>🎉 Simulating two level data</title>
      <link>http://localhost:1313/post/sim2data/</link>
      <pubDate>Fri, 27 Oct 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/sim2data/</guid>
      <description>


&lt;details class=&#34;print:hidden xl:hidden&#34; open&gt;
  &lt;summary&gt;Table of Contents&lt;/summary&gt;
  &lt;div class=&#34;text-sm&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;&lt;/nav&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;p&gt;Researchers may want to simulate a two-level model (i.e., a hierarchical linear model, a random effects model, etc.). The following code illustrates how to generate the data and compares analytic techniques using MLM and OLS.&lt;/p&gt;
&lt;div id=&#34;simulate-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Simulate the data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234) #for reproducability
nG &amp;lt;- 20 #number of groups
nJ &amp;lt;- 30 #cluster size
W1 &amp;lt;- 2 #level 2 coeff
X1 &amp;lt;- 3 #level 1 coeff
&lt;p&gt;tmp2 &amp;lt;- rnorm(nG) #generate 20 random numbers, m = 0, sd = 1
l2 &amp;lt;- rep(tmp2, each = nJ) #all units in l2 have the same value
group &amp;lt;- gl(nG, k = nJ) #creating cluster variable
tmp2 &amp;lt;- rnorm(nG) #error term for level 2
err2 &amp;lt;- rep(tmp2, each = nJ) #all units in l2 have the same value&lt;/p&gt;
&lt;p&gt;l1 &amp;lt;- rnorm(nG * nJ) #total sample size is nG * nJ
err1 &amp;lt;- rnorm(nG * nJ) #level 1&lt;/p&gt;
&lt;p&gt;#putting it all together
y &amp;lt;- W1 * l2 + X1 * l1 + err2 + err1
dat &amp;lt;- data.frame(y, group, l2, err2,l1, err1)&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;To vary the intraclass correlation (ICC or &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;), users must specify what the variance of the error terms should be (while taking into account the variance of the variables). There is a difference between the &lt;em&gt;unconditional&lt;/em&gt; vs &lt;em&gt;conditional&lt;/em&gt; ICC (often, in education, we want to know the unconditional first). Use covariance algebra to figure this out.&lt;/p&gt;
&lt;p&gt;For example, for two variables (X and Y):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Var(X + Y) = Var(X) + Var(Y) + 2cov(X, Y)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In our case, the variables are not related with each other so the last part is 0.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The level 2 variance (due to l2) should be 4 (in our case, it is 3.912).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The level 1 variance (due to l1) should be 9 (in our case, it is 9.285).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The errors both have 1 as the variance. So, in our models, the overall variance of y should be: (4 + 1) + (9 + 1) = 15. For the one simulated run above, the variance of y is 14.341. This is close. The theoretical unconditional ICC should be: 5/15 or .33. In our example, the standard errors turned out to be larger.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analyze-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Analyze the data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4) #to run multilevel models
library(jtools) #to get nicer output
mlm0 &amp;lt;- lmer(y ~ (1|group), data = dat) #unconditional
summ(mlm0) #shows the ICC, close&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## MODEL INFO:
## Observations: 600
## Dependent Variable: y
## Type: Mixed effects linear regression 
## 
## MODEL FIT:
## AIC = 3166.12, BIC = 3179.31
## Pseudo-R² (fixed effects) = 0.00
## Pseudo-R² (total) = 0.28 
## 
## FIXED EFFECTS:
##              Est. S.E. t val.    p  
## (Intercept) -1.08 0.47  -2.28 0.03 *
## 
## p values calculated using Kenward-Roger d.f. = 19 
## 
## RANDOM EFFECTS:
##     Group   Parameter Std. Dev.
##     group (Intercept)      2.02
##  Residual                  3.23
## 
## Grouping variables:
##  Group # groups  ICC
##  group       20 0.28&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mlm1 &amp;lt;- lmer(y ~ l2 + l1  + (1|group), data = dat)
ols1 &amp;lt;- lm(y ~ l2 + l1, data = dat)
&lt;p&gt;#export_summs(mlm1, ols1, model.names = c(&#39;MLM&#39;, &#39;OLS&#39;))&lt;/p&gt;
&lt;p&gt;stargazer::stargazer(mlm1, ols1, type = &#39;text&#39;, no.space = T, star.cutoffs = c(.05,.01,.001))&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## ============================================================
##                               Dependent variable:           
##                     ----------------------------------------
##                                        y                    
##                        linear                OLS            
##                     mixed-effects                           
##                          (1)                 (2)            
## ------------------------------------------------------------
## l2                    1.778***             1.777***         
##                        (0.173)             (0.049)          
## l1                    3.024***             3.047***         
##                        (0.039)             (0.048)          
## Constant              -0.663***           -0.663***         
##                        (0.176)             (0.050)          
## ------------------------------------------------------------
## Observations             600                 600            
## R2                                          0.901           
## Adjusted R2                                 0.900           
## Log Likelihood        -861.194                              
## Akaike Inf. Crit.     1,732.389                             
## Bayesian Inf. Crit.   1,754.373                             
## Residual Std. Error                    1.195 (df = 597)     
## F Statistic                       2,708.287*** (df = 2; 597)
## ============================================================
## Note:                          *p&amp;lt;0.05; **p&amp;lt;0.01; ***p&amp;lt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can compare the results using MLM vs. OLS. The standard error for the level 2 variable is much smaller using the OLS model. Here we see why we can get Type I errors so easily (in this case, both are statistically significant though). The coefficients are similar to each other because the variables were generated to both be uncorrelated with each other.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-items-of-interest&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Other items of interest&lt;/h2&gt;
&lt;p&gt;To see how these results may differ, readers can check out:&lt;/p&gt;
&lt;p&gt;Huang, F. (2018). Multilevel modeling and ordinary least squares: How comparable are they? &lt;em&gt;Journal of Experimental Education, 86&lt;/em&gt;, 265-281. &lt;a href=&#34;https://doi.org/10.1080/00220973.2016.1277339&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1080/00220973.2016.1277339&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.tandfonline.com/eprint/WHmbzEjIhPidHtbt7IRk/full&#34; class=&#34;uri&#34;&gt;http://www.tandfonline.com/eprint/WHmbzEjIhPidHtbt7IRk/full&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For more info comparing the two approaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://faculty.missouri.edu/huangf/data/pubdata/jxe/online%20appendix%20A%20formatted.pdf&#34;&gt;http://faculty.missouri.edu/huangf/data/pubdata/jxe/online%20appendix%20A%20formatted.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://faculty.missouri.edu/huangf/data/pubdata/jxe/&#34; class=&#34;uri&#34;&gt;http://faculty.missouri.edu/huangf/data/pubdata/jxe/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NOTE: a simpler way to get corrected level 2 standard errors is to use cluster robust standard errors. However, take note, that adjusted standard errors may often still be underestimated when the number of clusters is low (e.g., &amp;lt; 50).&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;jtools::summ&lt;/code&gt; function makes getting cluster robust standard errors easier! Without having to run other functions before hand.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summ(ols1, cluster = &amp;#39;group&amp;#39;, robust = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## MODEL INFO:
## Observations: 600
## Dependent Variable: y
## Type: OLS linear regression 
## 
## MODEL FIT:
## F(2,597) = 2708.29, p = 0.00
## R² = 0.90
## Adj. R² = 0.90 
## 
## Standard errors: Cluster-robust, type = HC3
##              Est. S.E. t val.    p    
## (Intercept) -0.66 0.20  -3.25 0.00  **
## l2           1.78 0.25   7.23 0.00 ***
## l1           3.05 0.05  65.98 0.00 ***&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See:&lt;/p&gt;
&lt;p&gt;Huang, F. (2016). Alternatives to multilevel modeling for the analysis of clustered data. &lt;em&gt;Journal of Experimental Education, 84&lt;/em&gt;, 175-196. doi: 10.1080/00220973.2014.952397&lt;/p&gt;
&lt;p&gt;– END&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>🖩 Logistic regression with matrices</title>
      <link>http://localhost:1313/post/test3/</link>
      <pubDate>Fri, 27 Oct 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/test3/</guid>
      <description>


&lt;details class=&#34;print:hidden xl:hidden&#34; open&gt;
  &lt;summary&gt;Table of Contents&lt;/summary&gt;
  &lt;div class=&#34;text-sm&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;&lt;/nav&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;p&gt;Logistic regression is a modeling technique that has attracted a lot of attention, especially from folks interested in classification, machine learning, and prediction using binary outcomes. One of the neat things about using R is that users can revisit commonly used procedures and figure out how they work.&lt;/p&gt;
&lt;p&gt;What follows are some logistic regression notes (this is not on interpreting results). Even though I’ve written about how other &lt;a href=&#34;../applied-example-for-alternatives-to-logistic-regression/&#34;&gt;alternatives&lt;/a&gt; might be simpler than logistic regression or that there are challenges when comparing coefficients across &lt;a href=&#34;../comparing-coefficients-across-logistic-regression-models/&#34;&gt;models&lt;/a&gt;, it is interesting to see how the procedure works.&lt;/p&gt;
&lt;p&gt;I show a few things:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How to use some matrices for getting logistic regression results (in terms of point estimates and standard errors);&lt;/li&gt;
&lt;li&gt;How to compute cluster robust standard errors too;&lt;/li&gt;
&lt;li&gt;How to manually run iteratively weighted least squares to get the same results from scratch.&lt;/li&gt;
&lt;li&gt;I added a section later on to do this using &lt;a href=&#34;../logistic-regression-fisher-scoring/&#34;&gt;Fisher Scoring&lt;/a&gt; as well.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;first-create-a-clustered-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;First, create a (clustered) dataset&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234) #clustered example:: just for sim
pp &amp;lt;- function(x) exp(x) / (1 + exp(x))
NG &amp;lt;- 20 #number of groups
GS &amp;lt;- 15 #group size
total &amp;lt;- NG * GS
tr &amp;lt;- rep(sample(c(0,1), NG, 1), each = GS)
w1 &amp;lt;- rep(rnorm(NG), each = GS)
e2 &amp;lt;- rep(rnorm(NG, 0, .5), each = GS)
school &amp;lt;- rep(1:NG, each = GS)
x1 &amp;lt;- rnorm(total)
x2 &amp;lt;- rbinom(total, 1, .5)
ystar &amp;lt;- 1 + tr * .5 + w1 * .3 + x1 * .6 + x2 * -.4 + e2
y &amp;lt;- rbinom(total, 1, pp(ystar))
dat &amp;lt;- data.frame(y, school, tr, w1, x1, x2)
sel &amp;lt;- sample(total, 100) #randomly select data to remove
dat &amp;lt;- dat[-sel, ] #remove to create unbalanced data&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run a logistic regression model predicting &lt;code&gt;y&lt;/code&gt; (&lt;code&gt;tr&lt;/code&gt; and &lt;code&gt;w1&lt;/code&gt; are actually level-2 variables but we’ll ignore the clustering first).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        y            school            tr              w1          
##  Min.   :0.00   Min.   : 1.00   Min.   :0.000   Min.   :-1.44820  
##  1st Qu.:0.00   1st Qu.: 6.00   1st Qu.:0.000   1st Qu.:-0.91120  
##  Median :1.00   Median :11.00   Median :1.000   Median :-0.49069  
##  Mean   :0.65   Mean   :10.71   Mean   :0.725   Mean   :-0.26632  
##  3rd Qu.:1.00   3rd Qu.:16.00   3rd Qu.:1.000   3rd Qu.: 0.08187  
##  Max.   :1.00   Max.   :20.00   Max.   :1.000   Max.   : 2.41584  
##        x1                 x2     
##  Min.   :-2.85576   Min.   :0.0  
##  1st Qu.:-0.63885   1st Qu.:0.0  
##  Median :-0.09820   Median :0.5  
##  Mean   :-0.02651   Mean   :0.5  
##  3rd Qu.: 0.51302   3rd Qu.:1.0  
##  Max.   : 2.91914   Max.   :1.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 200&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1 &amp;lt;- glm(y ~ tr + w1 + x1 + x2,
          data = dat, 
          family = binomial)
summary(m1)$coef #results are in logits&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                Estimate Std. Error     z value    Pr(&amp;gt;|z|)
## (Intercept)  0.23069482  0.3482386  0.66246188 0.507675259
## tr           0.93184838  0.3465455  2.68896434 0.007167408
## w1           0.60952221  0.2296481  2.65415706 0.007950681
## x1           0.57411101  0.1852548  3.09903484 0.001941522
## x2          -0.01320503  0.3218338 -0.04103058 0.967271514&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;extract-matrices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Extract matrices&lt;/h2&gt;
&lt;p&gt;The standard formula for the coefficients in linear regression is &lt;span class=&#34;math inline&#34;&gt;\((X&amp;#39;X)^{-1}(X&amp;#39;y)\)&lt;/span&gt;. However, in a generalized linear model, a difference is that &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is now &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; (see computation) and weights are estimated as well. The GzLM formula for the betas is now: &lt;span class=&#34;math inline&#34;&gt;\((X&amp;#39;WX)^{-1}(X&amp;#39;Wz)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The explanation of the matrices is shown at the latter part of this post (when computing this manually) which describes what goes into the computation. We’ll use the &lt;code&gt;m1&lt;/code&gt; object just created and extract elements from it that can be used to cobble together the same results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- model.matrix(m1) #the X matrix
y &amp;lt;- m1$y #the outcome
eta &amp;lt;- X %*% coef(m1) #predicted values
mu &amp;lt;- as.vector(1 / (1 + exp(-eta))) #transformed predicted values
z &amp;lt;- eta + (y - mu) * (1 / (mu * (1 - mu)))
Ws &amp;lt;- diag(weights(m1, &amp;#39;working&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After the matrices are created, can use the formulas to get the same results (compare below):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;### point estimates (similar)
solve(t(X) %*% Ws %*% X) %*% 
  t(X) %*% Ws %*% z&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    [,1]
## (Intercept)  0.23069529
## tr           0.93184794
## w1           0.60952156
## x1           0.57411052
## x2          -0.01320591&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(m1) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)          tr          w1          x1          x2 
##  0.23069482  0.93184838  0.60952221  0.57411101 -0.01320503&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For standard errors, this is just the square root of the diagonal of &lt;span class=&#34;math inline&#34;&gt;\((X&amp;#39;WX)^{-1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(diag(solve(t(X) %*% Ws %*% X)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)          tr          w1          x1          x2 
##   0.3482386   0.3465455   0.2296481   0.1852548   0.3218338&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(diag(vcov(m1))) #same&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)          tr          w1          x1          x2 
##   0.3482386   0.3465455   0.2296481   0.1852548   0.3218338&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;we-can-also-compute-cluster-robust-standard-errors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;We can also compute cluster robust standard errors&lt;/h2&gt;
&lt;p&gt;I’ve written about &lt;a href=&#34;https://francish.netlify.app/post/note-on-robust-standard-errors/&#34;&gt;robust SEs&lt;/a&gt; in another post but in this case, the residuals are different as seen in the syntax. I saw in this &lt;a href=&#34;https://stats.stackexchange.com/questions/283801/how-are-robust-standard-errors-calculated-in-the-case-of-logistic-regression&#34;&gt;post&lt;/a&gt; that you can extract the weighted residuals shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- model.matrix(m1) * sqrt(weights(m1, &amp;quot;working&amp;quot;))
u &amp;lt;- residuals(m1, &amp;quot;working&amp;quot;)  * sqrt(weights(m1, &amp;quot;working&amp;quot;)) #residuals
&lt;p&gt;cdata &amp;lt;- data.frame(cluster = dat$school, r = u)
(m &amp;lt;- length(table(cdata$cluster))) #number of clusters&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] 20&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- nobs(m1) #number of observations
k &amp;lt;- m1$rank
dfa &amp;lt;- (m/(m-1)) * ((n-1)/(n-k)) #degrees of freedom adjustment
gs &amp;lt;- names(table(cdata$cluster)) #cluster names
u &amp;lt;- matrix(NA, nrow = m, ncol = k) #clusters x rank:: creating a new u matrix

for(i in 1:m){
  u[i,] &amp;lt;- t(cdata$r[cdata$cluster == gs[i]]) %*% X[dat$school == gs[i], 1:k]
} 

br &amp;lt;- solve(t(X) %*% X) #the bread
mt &amp;lt;- t(u) %*% u #the meat
(clvc &amp;lt;- (br %*% mt %*% br * dfa)) #cluster robust VCOV manually computed&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              (Intercept)           tr           w1           x1           x2
## (Intercept)  0.161369126 -0.137857215  0.017878858 -0.001318135 -0.054932212
## tr          -0.137857215  0.184883609 -0.004615105  0.001738463 -0.004407959
## w1           0.017878858 -0.004615105  0.053945127  0.021829090 -0.012319716
## x1          -0.001318135  0.001738463  0.021829090  0.051276709  0.005750240
## x2          -0.054932212 -0.004407959 -0.012319716  0.005750240  0.129468425&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(sandwich) #to &amp;#39;automatically&amp;#39; compute
vcovCL(m1, cluster = dat$school, type = &amp;#39;HC1&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              (Intercept)           tr           w1           x1           x2
## (Intercept)  0.161369126 -0.137857215  0.017878858 -0.001318135 -0.054932212
## tr          -0.137857215  0.184883609 -0.004615105  0.001738463 -0.004407959
## w1           0.017878858 -0.004615105  0.053945127  0.021829090 -0.012319716
## x1          -0.001318135  0.001738463  0.021829090  0.051276709  0.005750240
## x2          -0.054932212 -0.004407959 -0.012319716  0.005750240  0.129468425&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all.equal(clvc, vcovCL(m1, cluster = dat$school, type = &amp;#39;HC1&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;manually-performing-a-logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Manually performing a logistic regression&lt;/h2&gt;
&lt;p&gt;A GLM requires several quantities (in this case, shown are the ones for a binomial family using a logit link– this will differ based on both the link and the family)&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the link function: &lt;span class=&#34;math inline&#34;&gt;\(g(\mu)=log(\frac{\mu}{1 - \mu})\)&lt;/span&gt; known as the log odds&lt;/li&gt;
&lt;li&gt;the inverse link: &lt;span class=&#34;math inline&#34;&gt;\(g^{-1}(\mu) =\frac{1}{1 + exp(-\eta)}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the variance function: &lt;span class=&#34;math inline&#34;&gt;\(V(\mu) = \mu(1 - \mu)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the first derivative: &lt;span class=&#34;math inline&#34;&gt;\(g&amp;#39;(\mu) = \frac{1}{\mu(1 - \mu)}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The generic GLM algorithm uses the following computations (substitute the values from above):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(diag[\frac{1}{V(\mu)g&amp;#39;(\mu)^2}]\)&lt;/span&gt; : weights&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(z = \eta + (y - \mu)g&amp;#39;(\mu)\)&lt;/span&gt; : working response&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\eta = X\beta\)&lt;/span&gt; : linear prediction&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu = g^{-1}(\eta)\)&lt;/span&gt; : the fitted values&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- model.matrix(~tr + w1 + x1 + x2, data = dat) #just to quickly get this
#this is not the same as the weighted one
tol &amp;lt;- 1e-6 #tolerance: if change is low, then stop
y &amp;lt;- dat$y
mu &amp;lt;- (y + mean(y))/2 #for convergence
#eta &amp;lt;- log(mu) ## Poisson link
eta &amp;lt;- log(mu / (1 - mu)) #logit
&lt;p&gt;dev &amp;lt;- 0 #deviance
delta.dev &amp;lt;- 1
i &amp;lt;- 1 #iteration number&lt;/p&gt;
&lt;p&gt;while(i &amp;lt; 50 &amp;amp; abs(delta.dev) &amp;gt; tol) { #repeat until deviance change is minimal
tmp &amp;lt;- mu * (1 - mu) #since I use this over and over again
W &amp;lt;- diag(1 / (tmp * (1/tmp)^2))
z &amp;lt;- eta + (y - mu) * (1 / tmp)
b &amp;lt;- solve(t(X) %&lt;em&gt;% W %&lt;/em&gt;% X) %&lt;em&gt;% t(X) %&lt;/em&gt;% W %&lt;em&gt;% z #these are the cofficients
eta &amp;lt;- X %&lt;/em&gt;% b #update eta
mu &amp;lt;- as.vector( 1 / (1 + exp(-eta)))
dev0 &amp;lt;- dev
#dev &amp;lt;- -2 * sum(y * log(y/mu) + (1 - y) * (1 -log(y/mu)), na.rm = T)
dev &amp;lt;- -2 * sum((y * log(mu)) + ((1 - y) * log(1 - mu))) #deviance
delta.dev &amp;lt;- dev - dev0 #assess change in deviance for logistic reg
cat(i, dev, &amp;quot;::&amp;quot;)
i &amp;lt;- i + 1 #increment by 1
}&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 1 234.3943 ::2 233.4394 ::3 233.4389 ::4 233.4389 ::&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The solution was reached after five iterations. Note: the deviance (which is -2 log likelihood) is the same using our manual method and the built-in function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;deviance(m1) #using the results from glm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 233.4389&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dev #manually computed&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 233.4389&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logLik(m1) * -2 #showing this is -2 x log likelihood&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;log Lik.&amp;#39; 233.4389 (df=5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To compute the standard errors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;serror &amp;lt;- sqrt(diag(solve(t(X) %*% W %*% X)))
# compare results
data.frame(B.manual = as.numeric(b), se.manual = serror)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                B.manual se.manual
## (Intercept)  0.23069482 0.3482383
## tr           0.93184838 0.3465451
## w1           0.60952221 0.2296474
## x1           0.57411101 0.1852545
## x2          -0.01320503 0.3218335&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(m1)$coef[,1:2] #using the glm function&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                Estimate Std. Error
## (Intercept)  0.23069482  0.3482386
## tr           0.93184838  0.3465455
## w1           0.60952221  0.2296481
## x1           0.57411101  0.1852548
## x2          -0.01320503  0.3218338&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Standard errors are the same (to the sixth decimal place) as those estimated using the &lt;code&gt;glm&lt;/code&gt; function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;heres-a-final-short-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Here’s a final short example&lt;/h2&gt;
&lt;p&gt;Just using the small, built-in &lt;code&gt;mtcars&lt;/code&gt; dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(mtcars)
table(mtcars$am) #the car transmission; just a toy outcome&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  0  1 
## 19 13&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m2 &amp;lt;- glm(am ~ mpg + wt, family = binomial, data = mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Computing this ‘manually’ using extracted matrices (object is now &lt;code&gt;m2&lt;/code&gt; for model 2):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- model.matrix(m2)
eta &amp;lt;- X %*% coef(m2)
mu &amp;lt;- as.vector(1 / (1 + exp(-eta)))
zz &amp;lt;- eta + (mtcars$am - mu) * (1 / (mu * (1 - mu)))
Ws &amp;lt;- diag(weights(m2, &amp;#39;working&amp;#39;))
estimate &amp;lt;- solve(t(X) %*% Ws %*% X) %*% t(X) %*% Ws %*% zz #est
se &amp;lt;- sqrt(diag(solve(t(X) %*% Ws %*% X))) #SE
z &amp;lt;- estimate / se
pv &amp;lt;- 2 * pt(-abs(z), df = Inf) #get the p values&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Put it all together and compare results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data.frame(estimate, se, z, pv)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               estimate        se         z         pv
## (Intercept) 25.8865522 12.193529  2.122975 0.03375598
## mpg         -0.3241639  0.239499 -1.353508 0.17589325
## wt          -6.4161717  2.546606 -2.519500 0.01175218&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(m2)$coef #based on GLM function&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               Estimate Std. Error   z value   Pr(&amp;gt;|z|)
## (Intercept) 25.8865540  12.193529  2.122975 0.03375597
## mpg         -0.3241639   0.239499 -1.353509 0.17589322
## wt          -6.4161721   2.546606 -2.519500 0.01175217&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Based on the notes from Dr. Wolfgang Wiedermann’s GLM course&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Experience</title>
      <link>http://localhost:1313/experience/</link>
      <pubDate>Tue, 24 Oct 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/experience/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The impact of restorative practices on the use of out-of-school suspensions: Results from a cluster randomized controlled trial</title>
      <link>http://localhost:1313/publication/journal-article/test3/huang-impact-2023/</link>
      <pubDate>Sat, 01 Jul 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test3/huang-impact-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The relationship between teacher stress and job satisfaction as moderated by coping</title>
      <link>http://localhost:1313/publication/journal-article/test3/woods-relationship-2023/</link>
      <pubDate>Sat, 01 Jul 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test3/woods-relationship-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Usability and social consequences of the early identification system as a universal screener for social, emotional, and behavioral risks.</title>
      <link>http://localhost:1313/publication/journal-article/test3/herman-usability-2023/</link>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test3/herman-usability-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Missouri Prevention Science Institute</title>
      <link>http://localhost:1313/project/mpsi/</link>
      <pubDate>Fri, 03 Mar 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/mpsi/</guid>
      <description>&lt;p&gt;The Missouri Prevention Science Institute (MPSI) brings community members and researchers together to help schools and families apply techniques that promote social and academic success. Through community outreach, the institute&amp;rsquo;s staff provides parent training and teacher consultation services&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>🎉 Job postings 2022-2023</title>
      <link>http://localhost:1313/post/jobs-2023/</link>
      <pubDate>Mon, 27 Feb 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/jobs-2023/</guid>
      <description>&lt;p&gt;&lt;strong&gt;List of Research, Statistics, and Evaluation job postings (that I&amp;rsquo;ve seen) as of Feb 2023.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Postings for (2022-2023):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Visiting Assistant Professor - REMS &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178224809&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Oklahoma State University&lt;/a&gt; in Stillwater, OK&lt;/li&gt;
&lt;li&gt;Assistant Professor in Applied Measurement &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178226483&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Florida State University&lt;/a&gt; in Tallahassee, FL&lt;/li&gt;
&lt;li&gt;Assistant Professor - Quantitative Methods, &lt;a href=&#34;https://jobs.chronicle.com/job/37353227/assistant-professor-quantitative-methods&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of South Florida&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Assistant Professor, Educational Research, &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178167908&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Southern Mississippi&lt;/a&gt;, Hattiesburg, MS&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Two Tenure Track Faculty Position in Statistics, Measurement, and Evaluation in Education Assistant Professor Statistics, Measurement, and Evaluation in Education at the &lt;a href=&#34;https://francish.netlify.app/post/jobs3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Missouri&lt;/a&gt; (Job ID 43395)&lt;/strong&gt; $\color{red}{\text{NEW!}}$&lt;/li&gt;
&lt;li&gt;Assistant or Associate Professor of Education &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178138744&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;West Texas A&amp;amp;M University&lt;/a&gt;, Canyon, TX&lt;/li&gt;
&lt;li&gt;Open Rank Faculty Position in Quantitative Research Methods and Teacher Education, August 2023 &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178138037&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chapman University&lt;/a&gt;, Orange, CA&lt;/li&gt;
&lt;li&gt;Research Methods, Measurement, And Evaluation (RMME) Assistant Professor. &lt;a href=&#34;https://jobs.hr.uconn.edu/cw/en-us/job/496834/research-methods-measurement-and-evaluation-rmme-assistant-professor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Connecticut&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Research Methods, Measurement, And Evaluation (RMME) Visiting Assistant Professor. &lt;a href=&#34;https://jobs.hr.uconn.edu/cw/en-us/job/496833/research-methods-measurement-and-evaluation-rmme-visiting-assistant-professor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Connecticut&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Research Faculty I in College of Education (Methodologist) &lt;a href=&#34;https://jobs.chronicle.com/job/37309200/research-faculty-i-in-college-of-education-methodologist-/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Florida State University&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tenure-Track Assistant or Associate Professor-School of Education and Urban Studies- Quantitative Methodologist &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178112582&amp;amp;utm_source=09_23_22&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Morgan State University&lt;/a&gt; Baltimore, MD&lt;/li&gt;
&lt;li&gt;Assistant or Associate Professor (Quantitative Psychology) in the &lt;a href=&#34;https://recruit.apo.ucla.edu/JPF07723&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UCLA Department of Psychology&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Assistant/Associate/Full Professor, Evaluation, Statistics, and Methodology, Fall 2023. &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178066179&amp;amp;&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Tennessee, Knoxville&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Petrone Professorship in Educational Data Science (Associate or Full Professor). &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=178064863&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Oregon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2022.08.04. Assistant Professor, Quantitative Methods. &lt;a href=&#34;https://jobs.chronicle.com/job/37293518/assistant-professor-quantitative-methods/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Princeton University&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Assistant Professor of Education with Expertise in Quantitative Methodologies in Service of Latina/o/x Communities. &lt;a href=&#34;https://apol-recruit.ucsd.edu/JPF03251&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UC San Diego&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;2022.06.21. Visiting Assistant Professor (in Education Policy and Equity, with a focus on teaching quantitative methods and applied research). &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=177976393&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Saint Louis University&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Visiting Assistant Teaching Professor. Research, Evaluation, Statistics and Assessment. &lt;a href=&#34;https://www.higheredjobs.com/details.cfm?JobCode=177976509&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Southern Mississippi&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;2022.05.28. Assistant, Associate or Full Professor - Ph.D. Program in Educational Psychology, New York, New York. &lt;a href=&#34;https://cuny.jobs/jobs/?q=24339&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;City University of New York.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;mdash;END&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Accounting for heteroskedasticity resulting from between-group differences in multilevel models</title>
      <link>http://localhost:1313/publication/journal-article/test3/huang-accounting-2023/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test3/huang-accounting-2023/</guid>
      <description>&lt;p&gt;Robust standard errors for multilevel models.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Advanced categorical data analysis in prevention science</title>
      <link>http://localhost:1313/publication/journal-article/test3/wiedermann-advanced-2023/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test3/wiedermann-advanced-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>In-school and out-of-school suspension: Behavioral and psychological outcomes in a predominately black sample of middle school students</title>
      <link>http://localhost:1313/publication/journal-article/test3/cohen-school-2023/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test3/cohen-school-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Practical multilevel modeling using R</title>
      <link>http://localhost:1313/publication/journal-article/test2/huang-practical-2023/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test2/huang-practical-2023/</guid>
      <description>&lt;p&gt;Add the &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; for the publication here using Markdown formatting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Teacher Perceptions of School Resource Officers and Associations with School Safety</title>
      <link>http://localhost:1313/publication/journal-article/test3/maeng-teacher-2023/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test3/maeng-teacher-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Using robust standard errors for the analysis of binary outcomes with a small number of clusters</title>
      <link>http://localhost:1313/publication/journal-article/test3/huang-using-2023/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test3/huang-using-2023/</guid>
      <description>&lt;p&gt;CR2 plug in for SPSS can be downloaded from: &lt;a href=&#34;https://github.com/flh3/CR2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/flh3/CR2&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Interaction of socioeconomic status and class relations on reading</title>
      <link>http://localhost:1313/publication/journal-article/2022/olsen-interaction-2022/</link>
      <pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/2022/olsen-interaction-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Student perceptions of school resource officers and threat reporting</title>
      <link>http://localhost:1313/publication/journal-article/test2/crichlow-ball-student-2022/</link>
      <pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test2/crichlow-ball-student-2022/</guid>
      <description>&lt;p&gt;Add the &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; for the publication here using Markdown formatting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Examining the validity of the Early Identification System – Student Version for screening in an elementary school sample</title>
      <link>http://localhost:1313/publication/journal-article/2022/reinke-examining-2022/</link>
      <pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/2022/reinke-examining-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Alternatives to logistic regression models in experimental studies</title>
      <link>http://localhost:1313/publication/journal-article/2022/huang-alternatives-2022/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/2022/huang-alternatives-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Analyzing cross-sectionally clustered data using generalized estimating equations</title>
      <link>http://localhost:1313/publication/journal-article/2022/huang-analyzing-2022/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/2022/huang-analyzing-2022/</guid>
      <description>&lt;p&gt;As of 2024.10.03, the most read article on JEBS (for the last 6 months)!&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;jebs_most_read.png&#34; srcset=&#34;
               /publication/journal-article/2022/huang-analyzing-2022/jebs_most_read_hu_59359045a62af85a.webp 400w,
               /publication/journal-article/2022/huang-analyzing-2022/jebs_most_read_hu_c2fa98b20b25e821.webp 760w,
               /publication/journal-article/2022/huang-analyzing-2022/jebs_most_read_hu_bc691bc4ace80636.webp 1200w&#34;
               src=&#34;http://localhost:1313/publication/journal-article/2022/huang-analyzing-2022/jebs_most_read_hu_59359045a62af85a.webp&#34;
               width=&#34;760&#34;
               height=&#34;536&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In the original paper draft, I had a section which showed how much more widely used mixed models (i.e., MLMs, HLMs) were compared to GEEs but was asked to remove that. I thought the usage was interesting so I am including it here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In psychology, mixed model studies are much more popular than studies using GEEs by a ratio of 15:1 (Bauer &amp;amp; Sterba, 2011).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Citations in JEBS:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the Journal of Educational and Behavioral Statistics (JEBS): one article on how to use multilevel models by Singer (1998) has over 3,300 citations (as of 2020.06.11, Google Scholar)&lt;/li&gt;
&lt;li&gt;In the same journal, Ghisletta &amp;amp; Sini (2004) provided an introduction to GEEs. This article has 329 citations. GS wrote (p. 431):&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although GEEs are widely applied in biological, pharmacological, and closely related disciplines, their application in educational and social sciences remains relatively scarce.&lt;/p&gt;
&lt;p&gt;There is a difference of 6 years but the Singer article has been cited over &lt;strong&gt;10&lt;/strong&gt; times more! If using average citations per year, &lt;strong&gt;7.5&lt;/strong&gt; times more.&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;Ghisletta, P., &amp;amp; Spini, D. (2004). An introduction to generalized estimating equations and an application to assess selectivity effects in a longitudinal study on very old individuals. Journal of Educational and Behavioral Statistics, 29(4), 421-437.&lt;/p&gt;
&lt;p&gt;Singer, J. D. (1998). Using SAS PROC MIXED to fit multilevel models, hierarchical models, and individual growth models. Journal of Educational and Behavioral Statistics, 23(4), 323-355.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Evaluation of a bibliotherapy-based stress intervention for teachers</title>
      <link>http://localhost:1313/publication/journal-article/2022/eddy-evaluation-2022/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/2022/eddy-evaluation-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Using cluster-robust standard errors when analyzing group-randomized trials with few clusters</title>
      <link>http://localhost:1313/publication/journal-article/2022/huang-using-2022/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/2022/huang-using-2022/</guid>
      <description>&lt;p&gt;The SPSS version can be accessed here: &lt;a href=&#34;https://github.com/flh3/CR2/tree/master/SPSS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/flh3/CR2/tree/master/SPSS&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Individual and school predictors of teacher stress, coping, and wellness during the COVID-19 pandemic</title>
      <link>http://localhost:1313/publication/journal-article/2022/herman-individual-2021/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/2022/herman-individual-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The association between student socioeconomic status and student–teacher relationships on math achievement</title>
      <link>http://localhost:1313/publication/journal-article/2022/olsen-association-2021/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/2022/olsen-association-2021/</guid>
      <description>&lt;p&gt;Add the &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; for the publication here using Markdown formatting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Does worrying matter? Priming and attitudes towards mask wearing in a midwestern state</title>
      <link>http://localhost:1313/publication/journal-article/2022/huang-does-2021/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/2022/huang-does-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>High school graduation outcomes of student threat assessment</title>
      <link>http://localhost:1313/publication/journal-article/2022/stohlman-high-2021/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/2022/stohlman-high-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Advancing diversity, equity, and inclusion in school psychology: Be the change</title>
      <link>http://localhost:1313/publication/journal-article/test3/jimerson-advancing-2021/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test3/jimerson-advancing-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An investigation of the psychometric properties of the Early Identification System–student report in a middle school sample</title>
      <link>http://localhost:1313/publication/journal-article/2022/herman-investigation-2021/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/2022/herman-investigation-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Authoritative school climate and out-of-school suspensions: Results from a nationally-representative survey of 10th grade students</title>
      <link>http://localhost:1313/publication/journal-article/2020/huang-authoritative-2021/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/2020/huang-authoritative-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Authoritative school climate and out-of-school suspensions: Results from a nationally-representative survey of 10th grade students</title>
      <link>http://localhost:1313/publication/journal-article/psf/huang-authoritative-2021/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/psf/huang-authoritative-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Confirmatory factor structure and predictive validity of the Early Identification System—Student Report in a community sample of high school students</title>
      <link>http://localhost:1313/publication/journal-article/2020/thompson-confirmatory-2021/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/2020/thompson-confirmatory-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Teacher support for zero tolerance is associated with higher suspension rates and lower feelings of safety</title>
      <link>http://localhost:1313/publication/journal-article/test2/huang-teacher-2021/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test2/huang-teacher-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>🎉 Using maximum likelihood</title>
      <link>http://localhost:1313/post/ml/</link>
      <pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/ml/</guid>
      <description>&lt;script src=&#34;http://localhost:1313/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;em&gt;Notes to self.&lt;/em&gt; Often, discussions related to maximum likelihood refer to (A) estimating parameters likely to have come from some (B) probability distribution (PD) by (C) maximizing a likelihood function (from Wikipedia).&lt;/p&gt;
&lt;p&gt;In many cases, (A) is the unknown which we want to figure out (e.g., means, variances, regression coefficients). We make an assumption about the PD where the data come from and determine the likelihood or the log likelihood of observing the datum. Get the product of all the likelihoods (multiplicative) or the sum of the log likelihoods (additive) and you will get the total likelihood.&lt;/p&gt;
&lt;p&gt;An example will help:&lt;/p&gt;
&lt;div id=&#34;simulate-some-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulate some data&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
x &amp;lt;- rnorm(10, mean = 110, sd = 3) #make up some data
x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 108.3186 109.3095 114.6761 110.2115 110.3879 115.1452 111.3827 106.2048
##  [9] 107.9394 108.6630&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 110.2239&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.861352&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have a vector with 10 elements. We can easily get the mean of this by just typing &lt;code&gt;mean(x)&lt;/code&gt; but this is for illustrative purposes.&lt;/p&gt;
&lt;p&gt;We create a likelihood function &lt;code&gt;llike&lt;/code&gt; based on a normal distribution using:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(x; \mu, \sigma^2) = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x - \mu}{\sigma})^2}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This take the sums of the log likelihood of whatever data is in x with a specified mean (&lt;code&gt;mu&lt;/code&gt;) and standard deviation &lt;code&gt;sigma&lt;/code&gt;– the later two we “don’t” know. There is also the built-in function of &lt;code&gt;dnorm&lt;/code&gt; which we can use to check if we are getting the same results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;llike &amp;lt;- function(x, mu, sigma){
  sum(log((1 / (sigma * sqrt(2 * pi))) * exp(-.5 * ((x - mu) / sigma)^2)))
}
&lt;p&gt;llike(x, 100, 10) #testing function to see if this works&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] -37.81005&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#just checking if we specify m = 100, sd = 10
sum(dnorm(x, mean = 100, sd = 10, log = T)) #using built-in function&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -37.81005&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get the same result using a specified mean of 100 and an SD of 10.&lt;/p&gt;
&lt;p&gt;We can take a series of means (&lt;code&gt;mu&lt;/code&gt; below) and see where the sum of the ll is at the highest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mu &amp;lt;- seq(100, 120, .1) #set of values to test
ll &amp;lt;- sapply(mu, function(z)
  llike(x, z, 10) #find the log likelihood
)
&lt;p&gt;plot(ll ~ mu) #plot &lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mu[which(ll == max(ll))] #which mu had the highest point&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 110.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on the results, it peaks around 110.&lt;/p&gt;
&lt;p&gt;We can use another function to find the maximum point (optimizing involves either finding the maximum or minimum). A lot of these ideas come from the very useful and accessible book of Hilbe and Robinson (2013) &lt;em&gt;Methods of Statistical Model Estimation&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## another function to optimize
bb &amp;lt;- function(p, x){
  sum(dnorm(x, mean = p[1], sd = p[2], log = T))
}
&lt;h2 id=&#34;finding-the-maximum-likelihood-using-39optim39&#34;&gt;finding the maximum likelihood using &#39;optim&#39;&lt;/h2&gt;
&lt;p&gt;fit &amp;lt;- optim(c(mean = 100, sd = 10), #starting values or guesses
fn = bb, x = x,
control = list(fnscale = -1)) #-1 to maximize
fit$par #contain the mean and the SD&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##       mean         sd 
## 110.224295   2.715141&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 110.2239&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(x) #this is the actual variance&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.861352&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(sum((x - mean(x))^2) / length(x)) #this is the ML variance&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.714517&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# (SS / n), not n - 1
# based on ML
# the variance is under estimated&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-about-using-ml-in-a-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What about using ML in a regression?&lt;/h1&gt;
&lt;p&gt;I have not seen examples showing this. I’ve only seen examples of the first kind shown (e.g., getting the mean).&lt;/p&gt;
&lt;p&gt;First, create some data with two variables (&lt;code&gt;x1&lt;/code&gt; and &lt;code&gt;x2&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## simulate some data
set.seed(2468)
ns &amp;lt;- 1000
x1 &amp;lt;- rnorm(ns)
x2 &amp;lt;- rnorm(ns)
e1 &amp;lt;- rnorm(ns)
y &amp;lt;- 10 + 1 * x1 + .5 * x2 + e1
dat &amp;lt;- data.frame(y = y, x1 = x1, x2 = x2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prepare the formula and some matrices:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fml &amp;lt;- formula(&amp;#39;y ~ x1 + x2&amp;#39;)
df &amp;lt;- model.frame(fml, dat)
y &amp;lt;- model.response(df)
X &amp;lt;- model.matrix(fml, df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create another function to optimize:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## log likelihood for continuous variables
ll.cont &amp;lt;- function(p, X, y){
  len &amp;lt;- length(p)
  betas &amp;lt;- p
  mu &amp;lt;- X %*% betas
  sum((y - mu)^2) #minimize this, sum of squared residuals
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, we want to minimize the sums of squared residuals (error)– that’s what the best fitting line does.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;start &amp;lt;- ncol(X) #how many start values
fit &amp;lt;- optim(p = rep(1, start), #repeat 1 
      fn = ll.cont, 
      X = X, y = y) #this minimizes the SSR
fit$par #compare with results below &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 9.9952812 1.0151535 0.5098497&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## values are for the intercept / Bx1 / Bx2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compare to using our standard &lt;code&gt;lm&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t1 &amp;lt;- lm(y ~ x1 + x2, data = df)
summary(t1)$coef&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Estimate Std. Error   t value      Pr(&amp;gt;|t|)
## (Intercept) 9.9950936 0.03144700 317.83937  0.000000e+00
## x1          1.0153582 0.03116854  32.57638 4.166280e-159
## x2          0.5096043 0.03093340  16.47424  4.091831e-54&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead of minimizing the error, what if we want to maximize the likelihoods again?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ll.cont.mx &amp;lt;- function(p, X, y){
  len &amp;lt;- length(p)
  betas &amp;lt;- p[-len]
  mu &amp;lt;- X %*% betas
  sds &amp;lt;- p[len]
  sum(dnorm(y, mean = mu, sd = sds, log = T)) #maximize this
  #sum((y - mu)^2) #minimize this
}
&lt;p&gt;start &amp;lt;- ncol(X) + 1 #add one w/c is the variance
fit &amp;lt;- optim(p = rep(1, start), #starting values
fn = ll.cont.mx, #function to use
X = X, y = y, #pass this to our
hessian = T, #to get standard errors
control = list(fnscale = -1, #to maximize
reltol = 1e-10) #change in deviance to converge
)
b &amp;lt;- fit$par[1:3] #the betas
ll &amp;lt;- fit$value #the log likelihood
conv &amp;lt;- fit$convergence #value of zero means convergence
ses &amp;lt;- sqrt(diag(solve(-fit$hessian)))[1:3] #get the SE from the inverse of the Hessian matrix
df &amp;lt;- data.frame(b, ses, t = b/ses)    &lt;br&gt;
rownames(df) &amp;lt;- colnames(X)
df #putting it all together   &lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##                     b        ses         t
## (Intercept) 9.9951172 0.03139866 318.32944
## x1          1.0153670 0.03112063  32.62681
## x2          0.5095777 0.03088585  16.49874&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ll #the log likelihood&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -1411.179&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t1 &amp;lt;- lm(y ~ x1 + x2, data = df)
summary(t1)$coef&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Estimate Std. Error   t value      Pr(&amp;gt;|t|)
## (Intercept) 9.9950936 0.03144700 317.83937  0.000000e+00
## x1          1.0153582 0.03116854  32.57638 4.166280e-159
## x2          0.5096043 0.03093340  16.47424  4.091831e-54&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logLik(t1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;log Lik.&amp;#39; -1411.179 (df=4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similar coefficients, standard errors, and the same log likelihood.&lt;/p&gt;
&lt;p&gt;— END&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Student threat assessment as an alternative to exclusionary discipline</title>
      <link>http://localhost:1313/publication/journal-article/test2/maeng-student-2020/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test2/maeng-student-2020/</guid>
      <description>&lt;p&gt;Add the &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; for the publication here using Markdown formatting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The role of the perceptions of school climate and teacher victimization by students</title>
      <link>http://localhost:1313/publication/journal-article/jiv/huang-role-2017/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/jiv/huang-role-2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>🎉 Comparing coefficients across logistic regression models</title>
      <link>http://localhost:1313/post/compare/</link>
      <pubDate>Sun, 28 Jun 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/compare/</guid>
      <description>&lt;p&gt;&lt;strong&gt;ROUGH NOTES&lt;/strong&gt;: &lt;span style=&#34;color: red;&#34;&gt;&lt;em&gt;[let me know if you spot any errors– there might be a couple!]&lt;/em&gt;&lt;/span&gt; Often, in randomized control trial where individuals are randomly assigned to treatment and control conditions, covariates are included to improve precision by reducing error and improving statistical power. However, when binary outcomes are used (e.g., patient recovers or not), there are several additional concerns that have gone unnoticed by many applied researchers.&lt;/p&gt;
&lt;p&gt;Take a simulated example where our true model data generating process is (to keep things simple, the intercept is zero and the parameters are both set to 1):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[log(\frac{p}{1 - p}) = 1 + 1 \times (tr) + 1 \times (x2)\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(summarytools)
library(stargazer)
&lt;p&gt;pp &amp;lt;- function(x) exp(x) / (1 + exp(x)) #convert logit to prob
set.seed(2468)
ns &amp;lt;- 10000
tr &amp;lt;- rbinom(ns, 1, .50) #50% treat / 50% control
x2 &amp;lt;- rnorm(ns, 0 , 2) #uncorrelated x2
ystar &amp;lt;- 1 + 1 * tr + 1 * x2 #all get a unit weight to keep it simple
y &amp;lt;- rbinom(ns, 1, pp(ystar))
ctable(y, tr, prop = &#39;c&#39;)&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Cross-Tabulation, Column Proportions  
## y * tr  
## 
## ------- ---- --------------- --------------- ----------------
##           tr               0               1            Total
##       y                                                      
##       0        1756 ( 35.0%)   1165 ( 23.3%)    2921 ( 29.2%)
##       1        3254 ( 65.0%)   3825 ( 76.7%)    7079 ( 70.8%)
##   Total        5010 (100.0%)   4990 (100.0%)   10000 (100.0%)
## ------- ---- --------------- --------------- ----------------&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In our example, think about y = 1 as recovered from illness and y = 0 as did not recover from illness. In the treatment group, &lt;em&gt;77%&lt;/em&gt; recovered vs. &lt;em&gt;65%&lt;/em&gt;. This is a difference of around 12 percentage points or a higher rate of recovery by a factor of 1.18 (77/65).&lt;/p&gt;
&lt;p&gt;Note that the two predictor variables are not correlated with each other:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- data.frame(y, tr, x2)
round(cor(df), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        y     tr     x2
## y  1.000  0.129  0.569
## tr 0.129  1.000 -0.017
## x2 0.569 -0.017  1.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When running a logistic regression model, notice how the coefficient changes even if &lt;code&gt;x2&lt;/code&gt; is not correlated with &lt;code&gt;tr&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reduced &amp;lt;- glm(y ~ tr, family = binomial)
full &amp;lt;- update(reduced, . ~ . + x2)
stargazer(reduced, full, star.cutoffs = c(.05, .01, .001),
          no.space = T, type = &amp;#39;text&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ================================================
##                        Dependent variable:      
##                   ------------------------------
##                                 y               
##                         (1)            (2)      
## ------------------------------------------------
## tr                   0.572***        1.005***   
##                       (0.045)        (0.059)    
## x2                                   1.040***   
##                                      (0.023)    
## Constant             0.617***        1.019***   
##                       (0.030)        (0.041)    
## ------------------------------------------------
## Observations          10,000          10,000    
## Log Likelihood      -5,956.977      -3,883.476  
## Akaike Inf. Crit.   11,917.950      7,772.952   
## ================================================
## Note:              *p&amp;lt;0.05; **p&amp;lt;0.01; ***p&amp;lt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The true effect should be around 1.0 (these are in logit units). In this basic example, there are several things to notice:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The coefficient for &lt;code&gt;tr&lt;/code&gt; is &lt;strong&gt;underestimated&lt;/strong&gt; in the first model (remember, I specified that the value should be 1).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The coefficients for &lt;code&gt;tr&lt;/code&gt; (treatment status) change with the inclusion of &lt;code&gt;x2&lt;/code&gt;. &lt;code&gt;x2&lt;/code&gt; is not your typical confounder which is associated both with y and the treatment status.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The standard error increases in the 2nd model– it does not decrease as one might expect.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As a result, this makes a few things problematic– even in the presence of randomization.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;We might conclude that &lt;code&gt;x2&lt;/code&gt; is a suppressor variable– but it is not!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This makes comparing results across models even within the same sample problematic. Results can change as long as variables predict the outcome.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We will never have all predictors of interest in a model (w/c is why we don’t get an &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; of 1.00).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another question that might be raised is if model 1 is really incorrect. We know that the coefficient is lower than 1 but then by looking at the crosstabs, this actually matches what was shown earlier:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pp(.617) #control:: 65%&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6495359&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pp(.617 + .572) #treatment:: 77%&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7665622&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we take the model 2 coefficients (and holding &lt;code&gt;x2&lt;/code&gt; at its average of zero), the coefficients are correct (controlling for &lt;code&gt;x2&lt;/code&gt;) but then this is not reflected in the crosstabs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pp(1) #control:: 73%&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7310586&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pp(1 + 1) #treatment: 88%&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8807971&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;why-does-that-happen-with-lrms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why does that happen with LRMs?&lt;/h2&gt;
&lt;p&gt;In a standard OLS model, we have the intercept, slope, and the residual.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = \beta_0 + \beta_1  x_1 + e\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The residuals have a mean of 0 and variance, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_e\)&lt;/span&gt;. The residual variance depends on the model and students of OLS regression know that if a predictor is added to the model, the residual variance decreases (even if it is a useless variable; w/c is why we at times use adjusted R2 vs R2).&lt;/p&gt;
&lt;p&gt;However, in a logistic regression model, the variance of the error term is fixed to &lt;span class=&#34;math inline&#34;&gt;\(\frac{\pi^2}{3}\)&lt;/span&gt; or 3.29, which is the variance of a logistic distribution. Adding predictors to the model does not change the residual variance. As a result, the added variability is absorbed in the other parts of the model (at times referred to as an issue or rescaling or unobserved heterogeneity).&lt;/p&gt;
&lt;p&gt;Over three decades ago, Winship and Mare (1984, p. 517) stated:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Adding new independent variables to an equation alters the variance of Y and thus the remaining coefficients in the model, even if the new independent variables are uncorrelated with the original independent variables.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Mood (2010) more recently indicated (based off what WM wrote) that what is actually being estimated (&lt;span class=&#34;math inline&#34;&gt;\(b_1\)&lt;/span&gt;) in the first model is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[b_1 \approx \beta_1 \times \frac{\sqrt{3.29}}{\sqrt{3.29 + \beta^2_2var(x_2)}} = 1.0 \times \frac{\sqrt{3.29}}{\sqrt{3.29 + 1^2 (4) }}\]&lt;/span&gt;
As long as the denominator is greater than &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{3.29}\)&lt;/span&gt;, the estimated coefficient with the missing &lt;span class=&#34;math inline&#34;&gt;\(b_2\)&lt;/span&gt; will be underestimated. The underestimation depends on effect and variability of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The reason why coefficients may change when variables are added to the model may be due to:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;real confounding (the control variable is correlated with BOTH the treatment and the outcome) and/or&lt;/li&gt;
&lt;li&gt;rescaling.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the full model (with both &lt;code&gt;tr&lt;/code&gt; and &lt;code&gt;x2&lt;/code&gt;), the variability of the latent y variable (which causes y) is: &lt;span class=&#34;math inline&#34;&gt;\(1 * .25 + 1 * 4 + 3.29 = 7.54\)&lt;/span&gt;. The .25 is the variability of the treatment variable which is .5 * (1 - .5). In the reduced model (w/c omits &lt;code&gt;x2&lt;/code&gt;), the variability is: &lt;span class=&#34;math inline&#34;&gt;\(1 * .25 + 3.29 = 3.54\)&lt;/span&gt;. The variability– even though the same sample is being used– differs between models. &lt;strong&gt;This is an approximation? Does not result in the latent y var exactly.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-then-do-we-compare-results-across-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How then do we compare results across models?&lt;/h2&gt;
&lt;p&gt;If we stick with logistic regression, we have a few options.&lt;/p&gt;
&lt;div id=&#34;y-standardization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Y standardization&lt;/h3&gt;
&lt;p&gt;Y standardization has been around for a while (Winship &amp;amp; Mare, 1984). Involves taking the standard deviation of the latent y and using this as a scaling factor to create standardized coefficients for the predictors. Here’s a small function &lt;code&gt;ystd&lt;/code&gt; that will compute the y standardized variables to be used with the model coefficients. Basically, this takes the variance of the predicted logits from the model estimated, adds the constant of 3.29 which results in the total variance of the outcome. Then take the square root of the variance which is the standard deviation. Divide the coefficients by the SD of y* (y* is referred to as the latent y; above which some threshold, y = 1, if else, y = 0).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ystd &amp;lt;- function(x){
  evar &amp;lt;- (pi ^ 2) / 3 #3.29 #constant
  vr &amp;lt;- var(predict(x)) + evar #variance of predicted logits + evar
  sdy &amp;lt;- sqrt(vr) #getting the sd
  coef(x) / sdy
}
&lt;p&gt;ystd(reduced)&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)          tr 
##   0.3359345   0.3115061&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ystd(full)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)          tr          x2 
##   0.3628906   0.3578878   0.3701445&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The resulting coefficients can be interpreted as the usual standardized beta– for a one standard deviation increase in the predictor, results in a std coef change in the outcome (i.e., .31 and .36). However, for binary predictors, that doesn’t make much sense to me (e.g., a SD coefficient change in gender, race, etc.) since you have a unit change or nothing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;karlson-holm-breen-khb-method&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Karlson, Holm, &amp;amp; Breen (khb) method&lt;/h3&gt;
&lt;p&gt;Again, this involves comparing the full and reduced models. Two ways to think about it. We need to estimate the reduced model but include &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; &lt;em&gt;without really including it&lt;/em&gt;. A way to do that is to get the residuals of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; regressed on treatment (can just use OLS) and then include that residual as a predictor (don’t interpret it) in the model. The output below (.93) is much closer to 1.0.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- resid(lm(x2 ~ tr))
reduced2 &amp;lt;- glm(y ~ tr + res, family = binomial)
stargazer(reduced, reduced2, full,
          star.cutoffs = c(.05, .01, .001),
          no.space = T, type = &amp;#39;text&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ==================================================
##                         Dependent variable:       
##                   --------------------------------
##                                  y                
##                      (1)        (2)        (3)    
## --------------------------------------------------
## tr                 0.572***   0.933***   1.005*** 
##                    (0.045)    (0.058)    (0.059)  
## res                           1.040***            
##                               (0.023)             
## x2                                       1.040*** 
##                                          (0.023)  
## Constant           0.617***   1.034***   1.019*** 
##                    (0.030)    (0.041)    (0.041)  
## --------------------------------------------------
## Observations        10,000     10,000     10,000  
## Log Likelihood    -5,956.977 -3,883.476 -3,883.476
## Akaike Inf. Crit. 11,917.950 7,772.952  7,772.952 
## ==================================================
## Note:                *p&amp;lt;0.05; **p&amp;lt;0.01; ***p&amp;lt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to think about it is to get the predicted logits (the estimated y*) from the full model and then just use that as the outcome in a linear regression (since the predicted logits are not anymore ones and zeroes but are continuous). [I am not sure about the standard errors in these models, just the point estimates/coefficients: &lt;code&gt;reduced2&lt;/code&gt; is fine though].&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predy &amp;lt;- predict(full) #gets logits, do not use fitted which returns prob
reduced3 &amp;lt;- glm(predy ~ tr)
stargazer(reduced, reduced2, reduced3, full,
          star.cutoffs = c(.05, .01, .001),
          no.space = T, type = &amp;#39;text&amp;#39;,
          omit.stat = c(&amp;#39;f&amp;#39;, &amp;#39;ll&amp;#39;, &amp;#39;ser&amp;#39;, &amp;#39;rsq&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ===========================================================
##                              Dependent variable:           
##                   -----------------------------------------
##                            y             predy        y    
##                         logistic         normal   logistic 
##                      (1)        (2)       (3)        (4)   
## -----------------------------------------------------------
## tr                 0.572***  0.933***   0.933***  1.005*** 
##                    (0.045)    (0.058)   (0.042)    (0.059) 
## res                          1.040***                      
##                               (0.023)                      
## x2                                                1.040*** 
##                                                    (0.023) 
## Constant           0.617***  1.034***   1.034***  1.019*** 
##                    (0.030)    (0.041)   (0.030)    (0.041) 
## -----------------------------------------------------------
## Observations        10,000    10,000     10,000    10,000  
## Akaike Inf. Crit. 11,917.950 7,772.952 43,161.080 7,772.952
## ===========================================================
## Note:                         *p&amp;lt;0.05; **p&amp;lt;0.01; ***p&amp;lt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;use-the-khb-package&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Use the khb package&lt;/h4&gt;
&lt;p&gt;Another way is to use the &lt;code&gt;khb&lt;/code&gt; package. Compare results with what was computed earlier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(khb)
compareModels(reduced, full, method = &amp;#39;naive&amp;#39;) #no adjustments&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Reduced  Full    perc   
## (Intercept) 0.61685  1.0194  -65.253
## tr          0.57199  1.0053  -75.756
## x2                   1.0397         
##                                     
## Pseudo R2   0.023544 0.49965        
## Dev.        11914    7767           
## Null        12080    12080          
## Chisq       166.47   4313.5         
## Sig         ***      ***            
## Dl          1        2              
## BIC         11923    7785.4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compareModels(reduced, full, method = &amp;#39;ystand&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Reduced  Full    perc    
## (Intercept) 0.33593  0.36289  -8.0242
## tr          0.31151  0.35789 -14.8895
## x2                   0.37014         
##                                      
## Pseudo R2   0.023544 0.49965         
## Dev.        11914    7767            
## Null        12080    12080           
## Chisq       166.47   4313.5          
## Sig         ***      ***             
## Dl          1        2               
## BIC         11923    7785.4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compareModels(reduced, full, method = &amp;#39;khb&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Reduced Full    perc   
## (Intercept) 1.03378 1.0194   1.3950
## tr          0.93291 1.0053  -7.7599
## Resid(x2)   1.03973                
## x2                  1.0397         
##                                    
## Pseudo R2   0.49965 0.49965        
## Dev.        7767    7767           
## Null        12080   12080          
## Chisq       4313.5  4313.5         
## Sig         ***     ***            
## Dl          2       2              
## BIC         7785.4  7785.4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k &amp;lt;- khb(reduced, full) #to compare and get SEs
print(k)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## KHB method
## Model type: glm lm (logit) 
## Variables of interest: tr
## Z variables (mediators): x2
## 
## Summary of confounding
##       Ratio Percentage Rescaling
## tr  0.92799   -7.75989     1.631
## ------------------------------------------------------------------
## tr :
##          Estimate Std. Error z value Pr(&amp;gt;|z|)    
## Reduced  0.932912   0.058321 15.9963  &amp;lt; 2e-16 ***
## Full     1.005305   0.058673 17.1340  &amp;lt; 2e-16 ***
## Diff    -0.072393   0.041898 -1.7278  0.08402 .&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;kb&lt;/code&gt; results indicate that the difference between the full and reduced model shown are not statistically significant (p = .08; i.e., not different from zero).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;use-alternative-procedures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Use alternative procedures&lt;/h2&gt;
&lt;p&gt;NOTE: if we use alternative procedures such as a linear probability model (LPM) or a modified Poisson regression (I’m not adjusting for overdisperson in the example, just focusing on the regression coefficients), these are not subject to the same issues. In particular, the LPM works as we would expect where the coefficient for &lt;code&gt;tr&lt;/code&gt; is relatively unchanged and the standard errors decrease. I’ve also shown this in a prior study (Huang, 2019).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reduced.ols &amp;lt;- glm(y ~ tr, family = gaussian) #normal
full.ols &amp;lt;- update(reduced.ols, . ~ . + x2)
reduced.poisson &amp;lt;- glm(y ~ tr, family = poisson) #normal
full.poisson &amp;lt;- update(reduced.poisson, . ~ . + x2)
stargazer(reduced.ols, full.ols, 
          reduced.poisson, full.poisson,
          star.cutoffs = c(.05, .01, .001),
          no.space = T, type = &amp;#39;text&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## =============================================================
##                               Dependent variable:            
##                   -------------------------------------------
##                                        y                     
##                          normal                Poisson       
##                      (1)        (2)        (3)        (4)    
## -------------------------------------------------------------
## tr                 0.117***   0.126***   0.166***   0.180*** 
##                    (0.009)    (0.007)    (0.024)    (0.024)  
## x2                            0.129***              0.182*** 
##                               (0.002)               (0.006)  
## Constant           0.650***   0.648***  -0.432***  -0.502*** 
##                    (0.006)    (0.005)    (0.018)    (0.018)  
## -------------------------------------------------------------
## Observations        10,000     10,000     10,000     10,000  
## Log Likelihood    -6,226.338 -4,209.938 -9,500.248 -9,023.055
## Akaike Inf. Crit. 12,456.670 8,425.875  19,004.500 18,052.110
## =============================================================
## Note:                           *p&amp;lt;0.05; **p&amp;lt;0.01; ***p&amp;lt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you compare the results above to the crosstabs shown at the start, we find that the treatment group is around 12 percentage points higher than the control (shown using OLS) and the the rate of recovery of the treatment group is higher by a factor of ~1.2 (&lt;code&gt;exp(.18)&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Huang, F. (2019). Alternatives to logistic regression models with binary outcomes. Journal of Experimental Education. doi: 10.1080/00220973.2019.1699769&lt;/p&gt;
&lt;p&gt;Karlson, KB, Holm, A, &amp;amp; Breen, R (2012). Comparing regression coefficients between same-sample nested mModels using logit and probit: A new method. Sociological Methodology, 42(1), pp 286-313.&lt;/p&gt;
&lt;p&gt;Mood, C. (2010). Logistic regression: Why we cannot do what we think we can do, and what we can do about it. European Sociological Review, 26, 67-82. &lt;a href=&#34;https://doi.org/10.1093/esr/jcp006&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1093/esr/jcp006&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Studer, M. (2014). khb: Comparing nonlinear regression models. R package version 0.1.&lt;/p&gt;
&lt;p&gt;Winship, C., &amp;amp; Mare, R. D. (1984). Regression models with ordinal variables. American Sociological Review, 49, 512-525.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Does teacher emotional exhaustion and efficacy predict student discipline sanctions?</title>
      <link>http://localhost:1313/publication/journal-article/2020/eddy-does-2020/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/2020/eddy-does-2020/</guid>
      <description>&lt;p&gt;[School Psychology Review, article of the year]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Prior problem behaviors do not account for the racial suspension gap</title>
      <link>http://localhost:1313/publication/journal-article/huang-prior-2020/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/huang-prior-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>🎉 Using FIML and MI in R</title>
      <link>http://localhost:1313/post/mdata/</link>
      <pubDate>Thu, 27 Feb 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/mdata/</guid>
      <description>&lt;div id=&#34;using-fiml-in-r-part-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using FIML in R (Part 2)&lt;/h2&gt;
&lt;p&gt;A recurring question that I get asked is how to handle missing data when researchers are interested in performing a multiple regression analysis. There are so many excellent articles, books, and websites that discuss the theory and rationale behind what can be done. Often, what is recommended is to either use full information likelihood (FIML) or multiple imputation (MI). Many excellent articles explain in detail how these work. The purpose though of this post is to show &lt;em&gt;how&lt;/em&gt; (again) to just run these models in &lt;code&gt;R&lt;/code&gt; (the examples I show here are just for single-level data). I had already shown some of this before over &lt;a href=&#34;https://francish.netlify.com/post/01_missing/&#34;&gt;here&lt;/a&gt; though I am adding to those notes to show some comparability with Mplus results. I will use &lt;code&gt;lavaan&lt;/code&gt; for getting FIML results.&lt;/p&gt;
&lt;div id=&#34;read-in-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Read in the data&lt;/h3&gt;
&lt;p&gt;For purposes of comparability, I will just use the &lt;em&gt;High School and Beyond&lt;/em&gt; demo data (n = 200) found on the &lt;a href=&#34;https://stats.idre.ucla.edu/mplus/seminars/mplus-class-notes/analyze/&#34;&gt;UCLA Statistical Computing&lt;/a&gt; website which shows how to use FIML with Mplus. We first read in the &lt;em&gt;complete&lt;/em&gt; data which we can use later when comparing results when using the dataset with missing data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr) #for selecting and using the pipe
hsbnomiss &amp;lt;- read.csv(&amp;#39;https://stats.idre.ucla.edu/wp-content/uploads/2016/02/hsbdemo.dat&amp;#39;, 
    header = F)
#note when data prepared for Mplus, there are no headers
#indicating the variable names 
hsbnomiss2 &amp;lt;- select(hsbnomiss, 2, 6:8) 
#only select columns we want
names(hsbnomiss2) &amp;lt;- c(&amp;#39;female&amp;#39;, &amp;#39;read&amp;#39;, &amp;#39;write&amp;#39;, &amp;#39;math&amp;#39;) 
#name the columns
head(hsbnomiss2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   female read write math
## 1      1   34    35   41
## 2      0   34    33   41
## 3      0   39    39   44
## 4      0   37    37   42
## 5      0   39    31   40
## 6      1   42    36   42&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(hsbnomiss2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    200 obs. of  4 variables:
##  $ female: int  1 0 0 0 0 1 0 0 1 0 ...
##  $ read  : int  34 34 39 37 39 42 31 50 39 34 ...
##  $ write : int  35 33 39 37 31 36 36 31 41 37 ...
##  $ math  : int  41 41 44 42 40 42 46 40 33 46 ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;run-the-regression-complete-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Run the regression (complete data)&lt;/h3&gt;
&lt;p&gt;In this example, we want to predict &lt;code&gt;write&lt;/code&gt; using &lt;code&gt;female&lt;/code&gt;, &lt;code&gt;read&lt;/code&gt;, and &lt;code&gt;math&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nomiss1 &amp;lt;- lm(write ~ female + read + math, data = hsbnomiss2)
summary(nomiss1)$coef %&amp;gt;% round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Estimate Std. Error t value Pr(&amp;gt;|t|)
## (Intercept)   11.896      2.863   4.155        0
## female         5.443      0.935   5.822        0
## read           0.325      0.061   5.355        0
## math           0.397      0.066   5.986        0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also do this using &lt;code&gt;lavaan&lt;/code&gt; and the &lt;code&gt;sem&lt;/code&gt; function. &lt;strong&gt;Note, in this case, the formula I specified is in between quotes.&lt;/strong&gt; &lt;code&gt;lavaan&lt;/code&gt; is often used for cfa and sem where the interrelationships between variables and latent factors are specified. Since this is just a regression with all observed variables, we can specify this in just one line (representing the formula).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lavaan)
nomiss2 &amp;lt;- sem(&amp;#39;write ~ female + read + math&amp;#39;, data = hsbnomiss2)
summary(nomiss2) #lot more information&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-5 ended normally after 17 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                          4
##                                                       
##   Number of observations                           200
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                                 0.000
##   Degrees of freedom                                 0
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard errors                             Standard
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   write ~                                             
##     female            5.443    0.926    5.881    0.000
##     read              0.325    0.060    5.409    0.000
##     math              0.397    0.066    6.047    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .write            42.368    4.237   10.000    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These results provide a benchmark of what the results should be when data are not missing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;read-in-missing-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Read in missing data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hsbwmiss &amp;lt;- read.csv(&amp;#39;https://stats.idre.ucla.edu/wp-content/uploads/2017/04/hsbmis2.dat&amp;#39;, 
    header = F)
#missing data are coded as -9999, recode to NA
hsbwmiss[hsbwmiss == -9999] &amp;lt;- NA
#I know you can do this in dplyr using some command
#but this is quick and basic
hsbwmiss2 &amp;lt;- dplyr::select(hsbwmiss, 2, 8:10)
names(hsbwmiss2) &amp;lt;- c(&amp;#39;female&amp;#39;, &amp;#39;read&amp;#39;, &amp;#39;write&amp;#39;, &amp;#39;math&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can just look at the patterns of missing data quickly too using the &lt;code&gt;mice&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mice)
md.pattern(hsbwmiss2, rotate.names = T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##    write read math female    
## 76     1    1    1      1   0
## 39     1    1    1      0   1
## 34     1    1    0      1   1
## 14     1    1    0      0   2
## 19     1    0    1      1   1
## 9      1    0    1      0   2
## 6      1    0    0      1   2
## 3      1    0    0      0   3
##        0   37   57     65 159&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The missing data patterns show a lot of missing data. We can run a naive regression and compare results to the complete case analysis. Also shows that only 48% of respondents have complete data. NOTE: There is no threshold as to what is considered an unacceptable amount of missing data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wmiss1 &amp;lt;- lm(write ~ female + read + math, data = hsbwmiss2)
library(stargazer) #to show side-by-side output&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Please cite as:&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  R package version 5.2.2. https://CRAN.R-project.org/package=stargazer&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stargazer(nomiss1, wmiss1, 
          star.cutoffs = c(.05, .01, .001), 
  no.space = T, type = &amp;#39;text&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ==================================================================
##                                  Dependent variable:              
##                     ----------------------------------------------
##                                         write                     
##                               (1)                    (2)          
## ------------------------------------------------------------------
## female                     5.443***                7.247***       
##                             (0.935)                (1.620)        
## read                       0.325***                 0.167         
##                             (0.061)                (0.107)        
## math                       0.397***                0.447***       
##                             (0.066)                (0.117)        
## Constant                   11.896***              17.160***       
##                             (2.863)                (4.964)        
## ------------------------------------------------------------------
## Observations                  200                     76          
## R2                           0.526                  0.468         
## Adjusted R2                  0.519                  0.446         
## Residual Std. Error    6.575 (df = 196)        6.966 (df = 72)    
## F Statistic         72.518*** (df = 3; 196) 21.111*** (df = 3; 72)
## ==================================================================
## Note:                                *p&amp;lt;0.05; **p&amp;lt;0.01; ***p&amp;lt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Regressions here show different results with reading not being predictive anymore of writing and the strength of the female coefficient increasing (and SEs are much higher).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;run-the-model-accounting-for-missing-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Run the model accounting for missing data&lt;/h3&gt;
&lt;p&gt;We will now use FIML to account for the missing data. We will again use the &lt;code&gt;sem&lt;/code&gt; function but will some additional options:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wmiss2 &amp;lt;- sem(&amp;#39;write ~ female + read + math&amp;#39;, data = hsbwmiss2, 
  missing = &amp;#39;fiml&amp;#39;, fixed.x = F)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We specify &lt;code&gt;missing = &#34;fiml&#34;&lt;/code&gt; which then uses fiml to account for the missing data. In addition, we include: &lt;code&gt;fixed.x = F&lt;/code&gt;. FIML works by estimating the relationships of the variables with each other and requires estimating the means and variances of the variables. If &lt;code&gt;fixed.x = T&lt;/code&gt; (the default), then the variances and covariances are fixed and are based on the existing sample values and are not estimated. You can specify the means and variances to be estimated in the model but requires more typing.&lt;/p&gt;
&lt;p&gt;View the results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(wmiss2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-5 ended normally after 46 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                         14
##                                                       
##   Number of observations                           200
##   Number of missing patterns                         8
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                                 0.000
##   Degrees of freedom                                 0
## 
## Parameter Estimates:
## 
##   Information                                 Observed
##   Observed information based on                Hessian
##   Standard errors                             Standard
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   write ~                                             
##     female            5.436    1.130    4.809    0.000
##     read              0.298    0.073    4.080    0.000
##     math              0.401    0.078    5.117    0.000
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   female ~~                                           
##     read             -0.255    0.462   -0.551    0.582
##     math              0.080    0.451    0.177    0.860
##   read ~~                                             
##     math             68.122    9.634    7.071    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .write            12.949    3.013    4.298    0.000
##     female            0.591    0.041   14.384    0.000
##     read             51.898    0.776   66.855    0.000
##     math             52.724    0.756   69.714    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .write            41.622    4.744    8.773    0.000
##     female            0.238    0.029    8.321    0.000
##     read            107.510   11.881    9.049    0.000
##     math             95.591   10.940    8.737    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The summary presents more information and shows that the analysis is based on the 200 observations. The intercept here is shown under &lt;code&gt;Intercepts&lt;/code&gt; –&amp;gt; &lt;code&gt;.write&lt;/code&gt;. Results are much closer to the original results with no missing data (shown again below):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;            Estimate Std. Error t value Pr(&amp;gt;|t|)
(Intercept)   11.896      2.863   4.155        0
female         5.443      0.935   5.822        0
read           0.325      0.061   5.355        0
math           0.397      0.066   5.986        0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you just want the coefficients without the summary statistics (but still showing all the variances and covariances indicated by &lt;code&gt;~~&lt;/code&gt; and the means denoted by &lt;code&gt;~&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parameterestimates(wmiss2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       lhs op    rhs     est     se      z pvalue ci.lower ci.upper
## 1   write  ~ female   5.436  1.130  4.809  0.000    3.221    7.651
## 2   write  ~   read   0.298  0.073  4.080  0.000    0.155    0.442
## 3   write  ~   math   0.401  0.078  5.117  0.000    0.247    0.554
## 4   write ~~  write  41.622  4.744  8.773  0.000   32.324   50.920
## 5  female ~~ female   0.238  0.029  8.321  0.000    0.182    0.294
## 6  female ~~   read  -0.255  0.462 -0.551  0.582   -1.160    0.651
## 7  female ~~   math   0.080  0.451  0.177  0.860   -0.804    0.964
## 8    read ~~   read 107.510 11.881  9.049  0.000   84.223  130.796
## 9    read ~~   math  68.122  9.634  7.071  0.000   49.240   87.004
## 10   math ~~   math  95.591 10.940  8.737  0.000   74.148  117.034
## 11  write ~1         12.949  3.013  4.298  0.000    7.044   18.854
## 12 female ~1          0.591  0.041 14.384  0.000    0.511    0.672
## 13   read ~1         51.898  0.776 66.855  0.000   50.377   53.420
## 14   math ~1         52.724  0.756 69.714  0.000   51.242   54.206&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can compare the results with those derived using Mplus. Results are comparable:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MODEL RESULTS
&lt;pre&gt;&lt;code&gt;                                                Two-Tailed
                Estimate       S.E.  Est./S.E.    P-Value
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;WRITE    ON
FEMALE             5.435      1.121      4.847      0.000
READ               0.298      0.072      4.168      0.000
MATH               0.401      0.077      5.236      0.000&lt;/p&gt;
&lt;p&gt;Intercepts
WRITE             12.950      2.951      4.388      0.000&lt;/p&gt;
&lt;p&gt;Residual Variances
WRITE             41.622      4.716      8.825      0.000
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;Try estimating the model using MI (see my previous post). Are results similar to the complete case results?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;imp &amp;lt;- mice(hsbwmiss2, m = 50, seed = 1234)
mi1 &amp;lt;- with(imp, lm(write ~ female + read + math))
round(summary(pool(mi1)), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>MANOVA: A procedure whose time has passed?</title>
      <link>http://localhost:1313/publication/journal-article/2020/huang-manova-2020/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/2020/huang-manova-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The relationship between school disciplinary resolutions with school climate and attitudes toward school</title>
      <link>http://localhost:1313/publication/journal-article/2020/huang-relationship-2020/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/2020/huang-relationship-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>🎉 Applied example for alternatives to logistic regression</title>
      <link>http://localhost:1313/post/applied/</link>
      <pubDate>Sun, 27 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/applied/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Logistic regression is often used to analyze experiments with binary outcomes (e.g., pass vs fail) and binary predictors (e.g., treatment vs control). Although appropriate, there are other possible models that can be run that may provide easier to interpret results.&lt;/p&gt;
&lt;p&gt;In addition, some of these models may be quicker to run. Some may say that this point is moot given the availability of computing power today but if you’ve ever tried to run a hierarchical generalized linear model with a logit link function and a binary outcome, you know that when using R (using &lt;code&gt;glmer&lt;/code&gt; or &lt;code&gt;nlme&lt;/code&gt;) this may take quite a long time (and cross your fingers that you don’t have convergence issues).&lt;/p&gt;
&lt;p&gt;The following code replicates the example (see the manuscript for details) in the &lt;a href=&#34;https://www.tandfonline.com/eprint/YS723ZYEIB2CPWKBEMPZ/full?target=10.1080/00220973.2019.1699769&#34;&gt;article&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Huang, F. (2019). Alternatives to logistic regression models with binary outcomes. Journal of Experimental Education. doi: 10.1080/00220973.2019.1699769&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Data are based on the article of Huang and Cornell (2015). Using an online survey, investigators tested for the presence of the question-order effect. Based on a random number generated when the students took the survey, they were placed in either the treatment (n = 1037) or control (n = 963) condition. Students in the treatment condition were asked four specific types bullying questions (i.e., verbal, physical, social, cyber) and then were asked a general bullying question (“I have been bullied in the past year”). Students in the control condition were asked the general bullying question first and then the specific bullying questions. We hypothesized that students who were asked the specific bullying questions first would report overall higher bullying vs the control group.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;examining-cross-tabs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Examining cross tabs&lt;/h2&gt;
&lt;p&gt;Load in the required packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr) #just for the pipe, %&amp;gt;%
library(logbin) #to run a log binomial model with a function
library(summarytools) #for nicer crosstabs
library(jtools) #for easier exp, confints, and adjusted standard errors&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Load and examine the data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#specify the location on website
dat &amp;lt;- url(&amp;quot;http://faculty.missouri.edu/huangf/data/jxe2019/jxe.rdata&amp;quot;) 
load(dat) #load in data from the website
summary(jxe) #name of the data.frame&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      abully           tord            female          gl      race    
##  Min.   :0.000   Min.   :0.0000   Min.   :0.0000   9th :446   w:1278  
##  1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:0.0000   10th:530   b: 287  
##  Median :0.000   Median :1.0000   Median :1.0000   11th:517   h: 170  
##  Mean   :0.126   Mean   :0.5185   Mean   :0.5005   12th:507   a:  79  
##  3rd Qu.:0.000   3rd Qu.:1.0000   3rd Qu.:1.0000              o: 186  
##  Max.   :1.000   Max.   :1.0000   Max.   :1.0000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(jxe)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      abully tord female   gl race
## 6910      0    0      1 10th    w
## 8394      0    0      0  9th    w
## 7293      0    1      1  9th    w
## 8491      0    1      0 10th    w
## 4374      1    1      0 10th    w
## 1594      0    1      1 10th    b&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(jxe)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2000    5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Review some crosstabs. All computations are based on the table below. &lt;code&gt;tord&lt;/code&gt; is the treatment variable. &lt;code&gt;abully&lt;/code&gt; indicates if the respondent had been bullied.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tt &amp;lt;- ctable(jxe$abully, jxe$tord, prop = &amp;#39;c&amp;#39;)
tt&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Cross-Tabulation, Column Proportions  
## abully * tord  
## Data Frame: jxe  
## 
## -------- ------ -------------- --------------- ---------------
##            tord              0               1           Total
##   abully                                                      
##        0          863 ( 89.6%)    885 ( 85.3%)   1748 ( 87.4%)
##        1          100 ( 10.4%)    152 ( 14.7%)    252 ( 12.6%)
##    Total          963 (100.0%)   1037 (100.0%)   2000 (100.0%)
## -------- ------ -------------- --------------- ---------------&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tt$cross_table #crosstabs&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        tord
## abully     0    1 Total
##   0      863  885  1748
##   1      100  152   252
##   Total  963 1037  2000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tt$proportions #proportions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               0         1 Total
## 0     0.8961578 0.8534233 0.874
## 1     0.1038422 0.1465767 0.126
## Total 1.0000000 1.0000000 1.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(14.66 / 85.34) / (10.38 / 89.62) #OR = 1.48 &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.483163&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;14.66 - 10.38 #risk difference: 4.28&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.28&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;14.66 / 10.38 #risk ratio: 1.41&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.412331&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;models-without-covariates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Models without covariates&lt;/h2&gt;
&lt;p&gt;For comparability, see how the results above map on to the first set of regressions without covariates:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab6.log1 &amp;lt;- glm(abully ~ tord, data = jxe, family = binomial)
summ(tab6.log1, exp = T)$coef %&amp;gt;% round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             exp(Est.)  2.5% 97.5%  z val.     p
## (Intercept)     0.116 0.094 0.143 -20.404 0.000
## tord            1.482 1.132 1.940   2.865 0.004&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab6.lpm1 &amp;lt;- glm(abully ~ tord, data = jxe) #risk difference
summ(tab6.lpm1, confint = T, robust = &amp;#39;HC3&amp;#39;)$coef %&amp;gt;% round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Est.  2.5% 97.5% t val.     p
## (Intercept) 0.104 0.085 0.123 10.553 0.000
## tord        0.043 0.014 0.072  2.896 0.004&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab6.poi1 &amp;lt;- glm(abully ~ tord, data = jxe, family = poisson) #risk ratio
summ(tab6.poi1, robust = &amp;#39;HC3&amp;#39;)$coef %&amp;gt;% round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               Est.  S.E.  z val.     p
## (Intercept) -2.265 0.095 -23.900 0.000
## tord         0.345 0.121   2.852 0.004&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to run a log-binomial model&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logb &amp;lt;- update(tab6.lpm1, family = binomial(link = &amp;#39;log&amp;#39;))
#logb &amp;lt;- logbin(abully ~ tord, data = jxe)
summ(logb, exp = T, digits = 3, robust = &amp;#39;HC3&amp;#39;)$coef %&amp;gt;% round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             exp(Est.)  2.5% 97.5%  z val.     p
## (Intercept)     0.104 0.086 0.125 -23.900 0.000
## tord            1.412 1.114 1.789   2.852 0.004&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;models-with-covariates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Models with covariates&lt;/h2&gt;
&lt;p&gt;Compare these to the results in Table 6 in the article with the covariates:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab6.log2 &amp;lt;- glm(abully ~ tord + female + gl + race, data = jxe, family = binomial)
summ(tab6.log2, exp = T)$coef %&amp;gt;% round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             exp(Est.)  2.5% 97.5%  z val.     p
## (Intercept)     0.149 0.105 0.211 -10.706 0.000
## tord            1.487 1.134 1.951   2.866 0.004
## female          1.184 0.906 1.547   1.239 0.215
## gl10th          0.749 0.525 1.068  -1.597 0.110
## gl11th          0.633 0.438 0.914  -2.439 0.015
## gl12th          0.515 0.348 0.761  -3.333 0.001
## raceb           0.646 0.411 1.015  -1.895 0.058
## raceh           1.227 0.780 1.929   0.885 0.376
## racea           1.512 0.826 2.769   1.340 0.180
## raceo           1.165 0.747 1.815   0.674 0.501&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab6.lpm2 &amp;lt;- update(tab6.log2, family = gaussian)
summ(tab6.lpm2, confint = T, robust = &amp;#39;HC3&amp;#39;)$coef %&amp;gt;% round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               Est.   2.5%  97.5% t val.     p
## (Intercept)  0.138  0.098  0.178  6.707 0.000
## tord         0.042  0.013  0.071  2.873 0.004
## female       0.018 -0.011  0.047  1.231 0.218
## gl10th      -0.038 -0.083  0.008 -1.631 0.103
## gl11th      -0.055 -0.100 -0.011 -2.440 0.015
## gl12th      -0.074 -0.117 -0.030 -3.331 0.001
## raceb       -0.040 -0.077 -0.003 -2.109 0.035
## raceh        0.024 -0.033  0.081  0.825 0.409
## racea        0.051 -0.035  0.138  1.161 0.246
## raceo        0.017 -0.037  0.071  0.625 0.532&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab6.poi2 &amp;lt;- update(tab6.log2, family = poisson)
summ(tab6.poi2, exp = T, robust = &amp;#39;HC3&amp;#39;)$coef %&amp;gt;% round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             exp(Est.)  2.5% 97.5%  z val.     p
## (Intercept)     0.128 0.096 0.172 -13.815 0.000
## tord            1.411 1.114 1.787   2.852 0.004
## female          1.157 0.917 1.458   1.232 0.218
## gl10th          0.785 0.581 1.060  -1.581 0.114
## gl11th          0.678 0.494 0.929  -2.421 0.015
## gl12th          0.563 0.401 0.793  -3.295 0.001
## raceb           0.678 0.449 1.024  -1.845 0.065
## raceh           1.190 0.811 1.746   0.888 0.374
## racea           1.417 0.858 2.339   1.363 0.173
## raceo           1.140 0.781 1.664   0.678 0.498&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab6.logbin &amp;lt;- update(tab6.log2, family = binomial(link = &amp;#39;log&amp;#39;))
## using logbinomial, adding `em` to speed up the estimation
## tab6.logbin &amp;lt;- logbin(abully ~ tord + female + gl + race, 
##              data = jxe, method = &amp;#39;em&amp;#39;)
summ(tab6.logbin, exp = T, confint = T, robust = &amp;#39;HC3&amp;#39;)$coef %&amp;gt;% round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             exp(Est.)  2.5% 97.5%  z val.     p
## (Intercept)     0.128 0.096 0.171 -13.869 0.000
## tord            1.412 1.115 1.788   2.866 0.004
## female          1.165 0.924 1.468   1.291 0.197
## gl10th          0.780 0.578 1.052  -1.628 0.104
## gl11th          0.672 0.491 0.921  -2.470 0.013
## gl12th          0.565 0.402 0.794  -3.288 0.001
## raceb           0.674 0.446 1.019  -1.872 0.061
## raceh           1.187 0.809 1.741   0.876 0.381
## racea           1.422 0.863 2.342   1.382 0.167
## raceo           1.142 0.782 1.666   0.687 0.492&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The pattern of results are similar. The Poisson, log-binomial, and linear probability models however may provide results that are easier to understand (especially if communicating results to a lay audience who do not understand odds ratios).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Huang, F., &amp;amp; Cornell, D. (2015). Order and definitional effects in bullying surveys: Results from an experimental study. &lt;em&gt;Psychological Assessment, 27,&lt;/em&gt; 1484-1493. doi: &lt;a href=&#34;http://dx.doi.org/10.1037/pas0000149&#34; class=&#34;uri&#34;&gt;http://dx.doi.org/10.1037/pas0000149&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;– END&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>🎉 Principal Components Analysis using R</title>
      <link>http://localhost:1313/post/pca_2020/</link>
      <pubDate>Sun, 27 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/pca_2020/</guid>
      <description>&lt;p style=&#34;color:red&#34;&gt;
[Rough notes: Let me know if there are corrections]
&lt;/p&gt;
&lt;p&gt;Principal components analysis (PCA) is a convenient way to reduce high-dimensional data into a smaller number number of ‘components.’ PCA has been referred to as a data reduction/compression technique (i.e., dimensionality reduction). PCA is often used as a means to an end and is not the end in itself. For example, instead of performing a regression with six (and highly correlated) variables, we may be able to compress the data into one or two meaningful components instead and use these in our models instead of the original six variables. Using less variables reduces possible problems associated with multicollinearity. This decreases the problems of redundancy.&lt;/p&gt;
&lt;p&gt;Note that PCA has often been called exploratory factor analysis (EFA) probably due to the fact that many software programs list PCA under the factor analysis heading (e.g., in SPSS, analyze &lt;span class=&#34;math inline&#34;&gt;\(\rightarrow\)&lt;/span&gt; dimension reduction &lt;span class=&#34;math inline&#34;&gt;\(\rightarrow\)&lt;/span&gt; factor; PROC FACTOR in SAS). They are &lt;strong&gt;NOT&lt;/strong&gt; the same. Often though– both procedures will produce similar results (depending on the struture of the dataset) and both can be used to reduce high dimensional data into a smaller number of components or factors. However, the causal structure of both procedures differ. Note the directions of the arrows in both procedures. For a FA, the underlying assumption is that the covariation among observed variables is &lt;em&gt;caused&lt;/em&gt; by a common factor (or factors or latent variables). Both procedures can be used for exploratory analysis. EFA (not PCA) is often used as a precursor to Confirmatory Factor Analysis.&lt;/p&gt;
&lt;div id=&#34;manually-running-a-principal-components-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1. Manually running a principal components analysis&lt;/h3&gt;
&lt;p&gt;The following example uses sample classroom literacy data (n = 120). We are interested in six variables (rhyme awareness, beginning sound awareness, alphabet recognition, letter sound knowledge, spelling, and concept of word) and will remove the first variable from the dataset (gender). The six variables of interest are subtasks from the &lt;strong&gt;Phonological Awareness Literacy Screening (PALS)&lt;/strong&gt; assessment. The raw data is imported and then a correlation matrix is generated (this is using simulated data based on the original correlation matrix). The correlation matrix will then be used to run our PCA.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- read.csv(&amp;quot;http://faculty.missouri.edu/huangf/data/mvnotes/READING120n.csv&amp;quot;)
str(dat) #just checking on our data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    120 obs. of  7 variables:
##  $ GEN     : Factor w/ 2 levels &amp;quot;F&amp;quot;,&amp;quot;M&amp;quot;: 2 1 2 1 1 2 2 2 1 1 ...
##  $ rhyme   : int  10 10 9 5 2 5 8 4 3 9 ...
##  $ Begsnd  : int  10 10 10 10 10 6 5 3 7 10 ...
##  $ ABC     : int  6 22 23 10 4 22 25 26 18 26 ...
##  $ LS      : int  7 19 15 3 0 8 20 16 8 17 ...
##  $ Spelling: int  4 9 5 2 0 17 12 3 3 15 ...
##  $ COW     : int  7 15 6 3 2 6 4 0 0 15 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;options(digits = 3) #so we get cleaner output, to the third digit
dat &amp;lt;- dat[,-1] #removing the first variable which is gender
p &amp;lt;- ncol(dat) #no of variables, I know it&amp;#39;s not good to name your variables
# this way (just don&amp;#39;t do it with t or c for example) but that&amp;#39;s it for now...
R &amp;lt;- cor(dat) #saving the correlation matrix
R #displaying the matrix- note: if you put a parenthesis around your statement, &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          rhyme Begsnd   ABC    LS Spelling   COW
## rhyme    1.000  0.616 0.499 0.677    0.668 0.693
## Begsnd   0.616  1.000 0.285 0.347    0.469 0.469
## ABC      0.499  0.285 1.000 0.796    0.589 0.598
## LS       0.677  0.347 0.796 1.000    0.758 0.749
## Spelling 0.668  0.469 0.589 0.758    1.000 0.767
## COW      0.693  0.469 0.598 0.749    0.767 1.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#it will also print the output as a default. Just showing it this way for clarity.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For some, the first stop is to check if the data can be reduced. This relies on variables being correlated with each other and allows them to be combined. If not (i.e., if the variables were all orthogonal), there would be no way to combine the variables as factors or components. One basic test is Bartlett’s test of sphericity (as it is called in SPSS)– the null hypothesis of the test is that the correlation matrix is an identity matrix– or that the matrix has one’s on the diagonal and zeroes on all the off diagonals. The test statistic follows a chi square distribution and to proceed, we would want to see statistically significant results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Highlight from the function name to the ending bracket and run. After, a new function called bart will now be available. For now, don&amp;#39;t have to worry about how that is created.
bart &amp;lt;- function(dat){ #dat is your raw data
   R &amp;lt;- cor(dat)
   p &amp;lt;- ncol(dat)
   n &amp;lt;- nrow(dat)
   chi2 &amp;lt;- -((n - 1)-((2 * p)+ 5)/ 6) * log(det(R)) #this is the formula
   df &amp;lt;- (p * (p - 1)/ 2)
   crit &amp;lt;- qchisq(.95, df) #critical value
   p &amp;lt;- pchisq(chi2, df, lower.tail = F) #pvalue
   cat(&amp;quot;Bartlett&amp;#39;s test: X2(&amp;quot;,
    df,&amp;quot;) = &amp;quot;, chi2,&amp;quot;, p = &amp;quot;,
   round(p, 3), sep=&amp;quot;&amp;quot; )   
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above lines are a function that we just created. Now we have a homemade &lt;code&gt;bart&lt;/code&gt; function and we pass it the raw data we want to analyze.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bart(dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Bartlett&amp;#39;s test: X2(15) = 497, p = 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Results indicate that the p value is &amp;lt; .001 (not really 0!) and is statistically significant. PCA can be done. Note: I don’t often use this test but some folks tend to use this. For one, when I have data, I do end up checking, inspecting the correlation matrix or use items in a way where I know they will be correlated with each other to some extent. But, in any case, good to know this. Now, moving on…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;e &amp;lt;- eigen(R) #solving for the eigenvalues and eigenvectors from the correlation matrix
str(e)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 2
##  $ values : num [1:6] 4.042 0.873 0.42 0.299 0.232 ...
##  $ vectors: num [1:6, 1:6] -0.42 -0.307 -0.385 -0.446 -0.436 ...
##  - attr(*, &amp;quot;class&amp;quot;)= chr &amp;quot;eigen&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;NOTE: &lt;code&gt;e&lt;/code&gt; is a list of two important sets of values. The first set contains the eigenvalues and the second is the set of eigenvectors for the corresponding eigenvalues. We will store them separately and use them in our analyses. The eigenvalue equation is an important equation that is used regularly in MV stats– though fortunately, computers will solve those for us (as we have solved this before using a 2 x 2 matrix and it took a while).&lt;/p&gt;
&lt;p&gt;The eigenvalue is a variance for a linear combination of variables (i.e., weighted variables).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;L &amp;lt;- e$values #placing the eigenvalues in L
Vm &amp;lt;- matrix(0, nrow = p, ncol = p) #creating a p x p matrix with zeroes.
#Vm is an orthogonal matrix since all correlations between variable are 0.
diag(Vm) &amp;lt;- L #putting the eigenvalues in the diagonals
Vm #check-- matrix with eigenvalues on the diagonals&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]  [,2] [,3]  [,4]  [,5]  [,6]
## [1,] 4.04 0.000 0.00 0.000 0.000 0.000
## [2,] 0.00 0.873 0.00 0.000 0.000 0.000
## [3,] 0.00 0.000 0.42 0.000 0.000 0.000
## [4,] 0.00 0.000 0.00 0.299 0.000 0.000
## [5,] 0.00 0.000 0.00 0.000 0.232 0.000
## [6,] 0.00 0.000 0.00 0.000 0.000 0.134&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;e$vectors #these are the eigenvectors-- these are the standardized regression weights&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        [,1]    [,2]    [,3]    [,4]    [,5]    [,6]
## [1,] -0.420  0.2993 -0.0927  0.8002 -0.1228  0.2642
## [2,] -0.307  0.7597  0.4314 -0.3329  0.0299 -0.1754
## [3,] -0.385 -0.4678  0.6571 -0.0846  0.0660  0.4354
## [4,] -0.446 -0.3346  0.0653  0.1441 -0.1308 -0.8044
## [5,] -0.436 -0.0389 -0.4390 -0.4379 -0.6046  0.2420
## [6,] -0.439 -0.0290 -0.4201 -0.1709  0.7727  0.0647&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#a quicker way may be to just use Vm &amp;lt;- diag(L)
loadings &amp;lt;- e$vectors %*% sqrt(Vm) #these are the loadings
#or the correlation of the component variables with the original variables-- 
#sometimes referred to as the P matrix. And PP` is the original correlation matrix.
#SPSS refers to this as the component matrix&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To reproduce the original correlation matrix (just shown again below):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(dat) #original correlation matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          rhyme Begsnd   ABC    LS Spelling   COW
## rhyme    1.000  0.616 0.499 0.677    0.668 0.693
## Begsnd   0.616  1.000 0.285 0.347    0.469 0.469
## ABC      0.499  0.285 1.000 0.796    0.589 0.598
## LS       0.677  0.347 0.796 1.000    0.758 0.749
## Spelling 0.668  0.469 0.589 0.758    1.000 0.767
## COW      0.693  0.469 0.598 0.749    0.767 1.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the eigenvalues and the eigenvectors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;e$vectors %*% Vm %*% t(e$vectors) # V L V`&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]
## [1,] 1.000 0.616 0.499 0.677 0.668 0.693
## [2,] 0.616 1.000 0.285 0.347 0.469 0.469
## [3,] 0.499 0.285 1.000 0.796 0.589 0.598
## [4,] 0.677 0.347 0.796 1.000 0.758 0.749
## [5,] 0.668 0.469 0.589 0.758 1.000 0.767
## [6,] 0.693 0.469 0.598 0.749 0.767 1.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#NOTE: I use L above in the comment but L is really a diagonal matrix with the L 
#on the diagonal.
#This is the proportion of variance accounted for by each PC
L/length(L)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6736 0.1454 0.0700 0.0498 0.0387 0.0224&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the original correlation matrix can be reproduced as: the eigenvectors times the diagonal matrix of eigenvalues (has to be a matrix or will be nonconformable) times the tranpose of the matrix of eigenvectors.&lt;/p&gt;
&lt;p&gt;To compute component scores– PCA1 = a1z1 + a2z2 + … + a6z6– we need the &lt;em&gt;weights&lt;/em&gt; (the eigenvectors) and the &lt;em&gt;standardized&lt;/em&gt; values of the original data (z1, z2, etc.). You can have as many component scores as you have variables (but not all will be useful, need to decide how many to retain– if you end up with 6 components from your 6 variables, we haven’t reduced/compressed anything!). To compute the PCA scores (using matrices), zValues x eigenvectors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;zdat &amp;lt;- scale(dat) #this is just to standardize the original data, M = 0, SD =1
pca.scores &amp;lt;- zdat %*% e$vectors #scaled values x vectors
#this is just P = XA in the notes
colnames(pca.scores) &amp;lt;- paste0(&amp;#39;pca&amp;#39;, 1:6)
head(pca.scores) #just to show some component scores&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        pca1   pca2     pca3   pca4     pca5    pca6
## [1,]  1.119  2.226 -0.80240  0.849 -0.07816 -0.2028
## [2,] -1.345  0.536 -0.00557  0.327  0.21459 -0.2122
## [3,] -0.181  0.610  0.90468  0.477 -0.22323 -0.0487
## [4,]  2.227  1.663  0.07935 -0.374  0.00992 -0.0769
## [5,]  3.370  1.922 -0.22066 -0.990  0.22398 -0.4874
## [6,]  0.427 -0.597 -0.64273 -1.111 -1.20665  1.0341&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#NOTE: these scores are scaled such that they have 
#a mean of zero and the variance comes out to the eigenvalue for 
#that component
round(colMeans(pca.scores), 2) #each pca score has a mean of zero&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## pca1 pca2 pca3 pca4 pca5 pca6 
##    0    0    0    0    0    0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;apply(pca.scores, 2, var) #if you get the variance PER column...&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  pca1  pca2  pca3  pca4  pca5  pca6 
## 4.042 0.873 0.420 0.299 0.232 0.134&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;e$values #...it will be the same as the eigenvalues too&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.042 0.873 0.420 0.299 0.232 0.134&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The component scores will differ from what you have with SPSS which scales the components so that it has a mean of zero and an SD of 1. To get the same results (the +/- may just be switched but that does not matter):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(scale(pca.scores)[,1]) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  0.557 -0.669 -0.090  1.108  1.676  0.212&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first component accounts for the most variance (4.042/6). To show the percentage of variance accounted for by each variable– divide the eigenvalues by the number of variables since each scaled variable has a variance of 1. First component accounts for 67% of the variance, second 15%, etc.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;e$values / p &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6736 0.1454 0.0700 0.0498 0.0387 0.0224&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also note, a property of the principal component scores is that they are not correlated with each other– they are completely orthogonal. To see this, generate a correlation matrix based on the pca.scores dataset. So we can see why using PC scores also reduces multicollinearity when these components, if ever, are used in a regression.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(cor(pca.scores), 2) #that is not an error, I had it round to 2 decimal places &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      pca1 pca2 pca3 pca4 pca5 pca6
## pca1    1    0    0    0    0    0
## pca2    0    1    0    0    0    0
## pca3    0    0    1    0    0    0
## pca4    0    0    0    1    0    0
## pca5    0    0    0    0    1    0
## pca6    0    0    0    0    0    1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#to make it clearer. This is a correlation matrix. &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see the structure matrix– which is the correlation of the component scores with the original variables– we can get the correlation between the original values and the newly created component scores. We just correlate the first PC here since that’s the one that accounts for the most variance and the second one accounts for much less.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(dat[,1:6], pca.scores[,1]) #this is correlating the six original variables &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]
## rhyme    -0.845
## Begsnd   -0.617
## ABC      -0.774
## LS       -0.896
## Spelling -0.876
## COW      -0.882&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#with the first PC.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the same as (without having to compute the component scores first):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;comp.matrix &amp;lt;- e$vectors %*% sqrt(Vm) #sometimes referred to as P matrix
#or eigenvectors x sqrt(Vm): P %*% t(P) is equal to the R matrix.
comp.matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        [,1]    [,2]    [,3]    [,4]    [,5]    [,6]
## [1,] -0.845  0.2796 -0.0601  0.4376 -0.0592  0.0968
## [2,] -0.617  0.7097  0.2796 -0.1821  0.0144 -0.0643
## [3,] -0.774 -0.4370  0.4259 -0.0463  0.0318  0.1596
## [4,] -0.896 -0.3126  0.0423  0.0788 -0.0630 -0.2949
## [5,] -0.876 -0.0364 -0.2845 -0.2394 -0.2913  0.0887
## [6,] -0.882 -0.0271 -0.2722 -0.0935  0.3723  0.0237&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you square the correlations, this will give you how much variance the PC accounts for in the original variables. These are referred to as the &lt;strong&gt;communalities&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;comp.matrix[,1]^2 #comp 1 accounts can account for 71% of the variance &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.714 0.381 0.599 0.803 0.768 0.777&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#of the first variable&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get the equivalent of the component matrix (or the weights) can either do:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;solve(R, loadings)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]    [,2]   [,3]   [,4]    [,5]   [,6]
## rhyme    -0.209  0.3204 -0.143  1.463 -0.2549  0.721
## Begsnd   -0.153  0.8133  0.666 -0.609  0.0621 -0.478
## ABC      -0.191 -0.5008  1.014 -0.155  0.1370  1.188
## LS       -0.222 -0.3582  0.101  0.263 -0.2715 -2.194
## Spelling -0.217 -0.0417 -0.677 -0.801 -1.2546  0.660
## COW      -0.218 -0.0310 -0.648 -0.313  1.6034  0.177&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#which actually does:
(zweight &amp;lt;- solve(R) %*% loadings ) #solve(R) gets the inverse&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]    [,2]   [,3]   [,4]    [,5]   [,6]
## rhyme    -0.209  0.3204 -0.143  1.463 -0.2549  0.721
## Begsnd   -0.153  0.8133  0.666 -0.609  0.0621 -0.478
## ABC      -0.191 -0.5008  1.014 -0.155  0.1370  1.188
## LS       -0.222 -0.3582  0.101  0.263 -0.2715 -2.194
## Spelling -0.217 -0.0417 -0.677 -0.801 -1.2546  0.660
## COW      -0.218 -0.0310 -0.648 -0.313  1.6034  0.177&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;NOTE: these are also weights used in creating a PCA. The difference with what we did earlier is that the sd/var of each component is just scaled to 1. This is what SPSS uses.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;newcomp &amp;lt;- zdat %*% zweight
apply(newcomp, 2, var)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 1 1 1 1 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(pca.scores[,1], newcomp[,1])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(pca.scores[,1], newcomp[,1])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;NOTE: the end result is the same except that the component score have differing variances.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-a-function-for-running-a-principal-components-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2. Using a function for running a principal components analysis&lt;/h3&gt;
&lt;p&gt;You can compare our results above (the loadings and the eigenvectors) to what you would get if done in SPSS (or in this case, in &lt;code&gt;R&lt;/code&gt; which is done with the &lt;code&gt;psych&lt;/code&gt; package– install it if you have not already done so).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#install.packages(&amp;quot;psych&amp;quot;)
library(psych)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can run the Bartlett test as we did and you can also run a Kaiser, Meyer, Olkin (KMO) Measure of Sampling Adequacy (MSA, see Word doc class notes for interpretation).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cortest.bartlett(dat) #indicating that n equals the number of rows in our dataset. &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R was not square, finding R from data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $chisq
## [1] 497
## 
## $p.value
## [1] 2.06e-96
## 
## $df
## [1] 15&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#If not, it will default to n=100. 
#cortest.bartlett(R, n = nrow(dat)) 
#using the correlation matrix
KMO(dat) #raw data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Kaiser-Meyer-Olkin factor adequacy
## Call: KMO(r = dat)
## Overall MSA =  0.83
## MSA for each item = 
##    rhyme   Begsnd      ABC       LS Spelling      COW 
##     0.84     0.77     0.80     0.77     0.89     0.90&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#KMO(R) #using correlation matrix&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following lines perform the actual PCA.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca &amp;lt;- principal(dat, nfactor = p, rotate = &amp;quot;none&amp;quot;) #forcing to extract 
#p = 6 components-- use the rotate = &amp;#39;none&amp;#39;-- 
#if not, it will create a correlated component matrix.
#The default is one for the number of factors. 
pca&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Principal Components Analysis
## Call: principal(r = dat, nfactors = p, rotate = &amp;quot;none&amp;quot;)
## Standardized loadings (pattern matrix) based upon correlation matrix
##           PC1   PC2   PC3   PC4   PC5   PC6 h2       u2 com
## rhyme    0.84  0.28 -0.06 -0.44 -0.06  0.10  1  1.1e-16 1.8
## Begsnd   0.62  0.71  0.28  0.18  0.01 -0.06  1 -2.2e-16 2.5
## ABC      0.77 -0.44  0.43  0.05  0.03  0.16  1  5.6e-16 2.3
## LS       0.90 -0.31  0.04 -0.08 -0.06 -0.29  1 -2.2e-16 1.5
## Spelling 0.88 -0.04 -0.28  0.24 -0.29  0.09  1 -4.4e-16 1.6
## COW      0.88 -0.03 -0.27  0.09  0.37  0.02  1 -4.4e-16 1.6
## 
##                        PC1  PC2  PC3  PC4  PC5  PC6
## SS loadings           4.04 0.87 0.42 0.30 0.23 0.13
## Proportion Var        0.67 0.15 0.07 0.05 0.04 0.02
## Cumulative Var        0.67 0.82 0.89 0.94 0.98 1.00
## Proportion Explained  0.67 0.15 0.07 0.05 0.04 0.02
## Cumulative Proportion 0.67 0.82 0.89 0.94 0.98 1.00
## 
## Mean item complexity =  1.9
## Test of the hypothesis that 6 components are sufficient.
## 
## The root mean square of the residuals (RMSR) is  0 
##  with the empirical chi square  0  with prob &amp;lt;  NA 
## 
## Fit based upon off diagonal values = 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;NOTE: What is being shown is more commonly referred to as the &lt;em&gt;pattern matrix&lt;/em&gt;- which contain the zero-order correlations between the factor and the original variables and the loading (regression weights) when no rotation is used. You can compare these to the results generated using the eigenvectors x the square root of the matrix of eigenvalues (really a diagonal matrix with a the square root of the eigenvalues on the diagonal).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loadings &amp;lt;- e$vectors %*% sqrt(Vm) #these are the correlations as we have shown earlier
round(loadings, 2) #signs are just different&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]
## [1,] -0.84  0.28 -0.06  0.44 -0.06  0.10
## [2,] -0.62  0.71  0.28 -0.18  0.01 -0.06
## [3,] -0.77 -0.44  0.43 -0.05  0.03  0.16
## [4,] -0.90 -0.31  0.04  0.08 -0.06 -0.29
## [5,] -0.88 -0.04 -0.28 -0.24 -0.29  0.09
## [6,] -0.88 -0.03 -0.27 -0.09  0.37  0.02&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;NOTE: the correlations are the same. Except that eigenvectors 1 and 4 have the opposite signs in their loadings compared to the output from SPSS and R (using the principal function in base R). To get the exact same output, the syntax below can be applied to change the signs (if this bothers you).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sign &amp;lt;- vector(mode = &amp;quot;numeric&amp;quot;, length = p)
sign &amp;lt;- sign(colSums(loadings)) 
#generates a -1 if total is a negative number and keeps it at +1 if &amp;gt; 0. 
#This is then applied to the loadings to flip the signs. See below:
loadings2 &amp;lt;- loadings %*% diag(sign)
loadings2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       [,1]    [,2]    [,3]    [,4]    [,5]    [,6]
## [1,] 0.845  0.2796 -0.0601 -0.4376 -0.0592  0.0968
## [2,] 0.617  0.7097  0.2796  0.1821  0.0144 -0.0643
## [3,] 0.774 -0.4370  0.4259  0.0463  0.0318  0.1596
## [4,] 0.896 -0.3126  0.0423 -0.0788 -0.0630 -0.2949
## [5,] 0.876 -0.0364 -0.2845  0.2394 -0.2913  0.0887
## [6,] 0.882 -0.0271 -0.2722  0.0935  0.3723  0.0237&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The component weights can be displayed using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca$weights #these are the weights&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            PC1     PC2    PC3    PC4     PC5    PC6
## rhyme    0.209  0.3204 -0.143 -1.463 -0.2549  0.721
## Begsnd   0.153  0.8133  0.666  0.609  0.0621 -0.478
## ABC      0.191 -0.5008  1.014  0.155  0.1370  1.188
## LS       0.222 -0.3582  0.101 -0.263 -0.2715 -2.194
## Spelling 0.217 -0.0417 -0.677  0.801 -1.2546  0.660
## COW      0.218 -0.0310 -0.648  0.313  1.6034  0.177&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also use the eigenvalues to draw a scree plot like in SPSS (in case you want to refer to something like Kaiser’s rule later on– which many studies have shown to not be very exact– but it is simple):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#remember, L here refers to the eigenvalues we created earlier
plot(L, main = &amp;quot;Scree Plot&amp;quot;, ylab = &amp;quot;Eigenvalues&amp;quot;, xlab = &amp;quot;Component number&amp;quot;, 
    type = &amp;#39;b&amp;#39;)
abline(h = 1, lty = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This shows you what percent of overall variance each component accounts for, the first component accounts for 63% of the variance, the second, 15% –&amp;gt; have shown this already twice before.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;L/p &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6736 0.1454 0.0700 0.0498 0.0387 0.0224&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;number-of-components-to-retain&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;3. Number of components to retain&lt;/h3&gt;
&lt;p&gt;Several methods have been suggested to decide how many components/factors to retain. There are several methods, some of which are better than others. Kaiser’s rule suggests eigenvalues &amp;gt; 1 but that is not always a good way to decide (the thought being that if the L &amp;lt; 1 then the component accounts for less variance than one of the original variables). Others may suggest using the % of variance accounted for (as long as it is ‘meaningful’). One of the better recognized ways to empirically decide how many factors to retain is to run a parallel analysis (Horn, 1965).&lt;/p&gt;
&lt;p&gt;From the &lt;code&gt;hornpa&lt;/code&gt; function: The program generates a specified number of &lt;strong&gt;random&lt;/strong&gt; datasets based on the number of variables entered by the user. As the correlation matrices have an eigenvalue based on random noise, these eigenvalues are computed for the each dataset and collected. The mean and the specified percentile (95th is the default) are computed. The output table shows how large eigenvalues can be as a result of merely using randomly generated datasets. If the user’s own dataset has an actual eigenvalue greater than the generated eigenvalue (which is based on random noise), that lends support to retain that factor. In other words, if the i(th) eigenvalue from the actual data was larger than the percentile of the (i)th eigenvalue generated using randomly generated data, empirical support is provided to retain that factor. Install the &lt;code&gt;hornpa&lt;/code&gt; package if you have not already done so.&lt;/p&gt;
&lt;p&gt;We specify how many variables are in the original dataset (k = 6), how big our sample is (size = 120), how many repetitions to run (reps = 500), and we set an optional seed so we get the same results if we run it again (seed = 1234).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(hornpa)
hornpa(k = 6, size = 120, reps = 500, seed = 1234) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Parallel Analysis Results  
##  
## Method: pca 
## Number of variables: 6 
## Sample size: 120 
## Number of correlation matrices: 500 
## Seed: 1234 
## Percentile: 0.95 
##  
## Compare your observed eigenvalues from your original dataset to the 95 percentile in the table below generated using random data. If your eigenvalue is greater than the percentile indicated (not the mean), you have support to retain that factor/component. 
##  
##  Component  Mean  0.95
##          1 1.312 1.448
##          2 1.159 1.247
##          3 1.042 1.109
##          4 0.939 1.007
##          5 0.836 0.912
##          6 0.712 0.804&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;L #our original eigenvalues again&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.042 0.873 0.420 0.299 0.232 0.134&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since our first eigenvalue (4.04) is &amp;gt; 1.448, we have support to retain the first component. The second eigenvalue (0.87) is &amp;lt; 1.247– so we may opt to drop the second component and just retain the first one.&lt;/p&gt;
&lt;p&gt;Try to find a consensus among the different methods. For the above example, this satisfies the eigenvalue &amp;gt; 1 rule AND more importantly, is suggested by parallel analysis. Although we only account for 60%+ of the variance, that may be acceptable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generating-component-scores&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;4. Generating component scores&lt;/h3&gt;
&lt;p&gt;Remember, the weights are used just like in a regression.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fs &amp;lt;- factor.scores(x = dat, f = pca) #NOTE: these are your component scores-- 
#even if the function is called factor.scores. places the raw data in x and put the 
#output of your principal function for f.
str(fs) #contains a lot of stuff, if you just want the scores:&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 5
##  $ scores  : num [1:120, 1:6] -0.557 0.669 0.09 -1.108 -1.676 ...
##   ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. ..$ : NULL
##   .. ..$ : chr [1:6] &amp;quot;PC1&amp;quot; &amp;quot;PC2&amp;quot; &amp;quot;PC3&amp;quot; &amp;quot;PC4&amp;quot; ...
##  $ weights : num [1:6, 1:6] 0.209 0.153 0.191 0.222 0.217 ...
##   ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. ..$ : chr [1:6] &amp;quot;rhyme&amp;quot; &amp;quot;Begsnd&amp;quot; &amp;quot;ABC&amp;quot; &amp;quot;LS&amp;quot; ...
##   .. ..$ : chr [1:6] &amp;quot;PC1&amp;quot; &amp;quot;PC2&amp;quot; &amp;quot;PC3&amp;quot; &amp;quot;PC4&amp;quot; ...
##  $ r.scores: num [1:6, 1:6] 1.00 -5.52e-16 -6.94e-17 -1.11e-16 -8.33e-17 ...
##   ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. ..$ : chr [1:6] &amp;quot;PC1&amp;quot; &amp;quot;PC2&amp;quot; &amp;quot;PC3&amp;quot; &amp;quot;PC4&amp;quot; ...
##   .. ..$ : chr [1:6] &amp;quot;PC1&amp;quot; &amp;quot;PC2&amp;quot; &amp;quot;PC3&amp;quot; &amp;quot;PC4&amp;quot; ...
##  $ missing : num [1:120] 0 0 0 0 0 0 0 0 0 0 ...
##  $ R2      : num [1:6] 1 NA 1 NA 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fscores &amp;lt;- fs$scores[,1] #use the [,1] just to get the first column
&lt;p&gt;#what if I just unit weight the raw scores? manual just adds them together&lt;/p&gt;
&lt;p&gt;manual &amp;lt;- with(dat, rhyme + Begsnd + ABC + LS + Spelling + COW)
cor(manual, fscores) #almost perfect!&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.993&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(manual, fscores)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;unnamed-chunk-26-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case, it seems like the weighting was more complicated when we could just have added the raw scores together!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reliability-estimates&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;5. Reliability estimates&lt;/h3&gt;
&lt;p&gt;What about internal consistency or reliability estimates? Can use the &lt;code&gt;alpha&lt;/code&gt; function in the &lt;code&gt;psych&lt;/code&gt; package too.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;psych::alpha(dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Reliability analysis   
## Call: psych::alpha(x = dat)
## 
##   raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r
##       0.88       0.9    0.91       0.6   9 0.012   11 4.7     0.62
## 
##  lower alpha upper     95% confidence boundaries
## 0.86 0.88 0.91 
## 
##  Reliability if an item is dropped:
##          raw_alpha std.alpha G6(smc) average_r  S/N alpha se  var.r med.r
## rhyme         0.87      0.87    0.88      0.58  7.0    0.014 0.0342  0.59
## Begsnd        0.90      0.91    0.91      0.68 10.6    0.012 0.0089  0.68
## ABC           0.86      0.89    0.89      0.62  8.2    0.014 0.0209  0.67
## LS            0.83      0.87    0.86      0.57  6.5    0.019 0.0192  0.59
## Spelling      0.84      0.87    0.89      0.57  6.7    0.016 0.0287  0.61
## COW           0.84      0.87    0.89      0.57  6.6    0.017 0.0284  0.60
## 
##  Item statistics 
##            n raw.r std.r r.cor r.drop mean  sd
## rhyme    120  0.79  0.85  0.81   0.75  7.3 3.0
## Begsnd   120  0.55  0.65  0.56   0.47  7.9 2.7
## ABC      120  0.82  0.77  0.73   0.71 20.9 6.9
## LS       120  0.92  0.88  0.88   0.86 14.5 7.5
## Spelling 120  0.87  0.87  0.84   0.81  7.5 6.0
## COW      120  0.89  0.87  0.85   0.81 10.2 7.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;NOTE: I use the format &lt;code&gt;psych::alpha&lt;/code&gt; to specifically call the &lt;code&gt;alpha&lt;/code&gt; function in the &lt;code&gt;psych&lt;/code&gt; package. Sometimes, different packages will have the name for a function. If you use the &lt;code&gt;::&lt;/code&gt; you don’t even have to load in the library anymore and call the function directly. The alpha of our measure is quite good, &lt;span class=&#34;math inline&#34;&gt;\(\alpha = .88\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;You can also use the &lt;code&gt;ci.reliability&lt;/code&gt; function in the &lt;code&gt;MBESS&lt;/code&gt; package which can compute Omega (w/c I prefer and is often even better, &lt;span class=&#34;math inline&#34;&gt;\(\omega = .92\)&lt;/span&gt;.).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MBESS::ci.reliability(dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $est
## [1] 0.919
## 
## $se
## [1] 0.0104
## 
## $ci.lower
## [1] 0.898
## 
## $ci.upper
## [1] 0.939
## 
## $conf.level
## [1] 0.95
## 
## $type
## [1] &amp;quot;omega&amp;quot;
## 
## $interval.type
## [1] &amp;quot;robust maximum likelihood (wald ci)&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;/div&gt;
&lt;p&gt;Bartlett, M. S. (1951). The effect of standardization on a chi square approximation in factor analysis, &lt;em&gt;Biometrika, 38&lt;/em&gt;, 337-344.&lt;/p&gt;
&lt;p&gt;Horn, J. (1965). A rationale and test for the number of factors in factor analysis. &lt;em&gt;Psychometrika, 32&lt;/em&gt;, 179-185&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>🎉 Why does centering reduce multicollinearity?</title>
      <link>http://localhost:1313/post/centering/</link>
      <pubDate>Sun, 27 Oct 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/centering/</guid>
      <description>&lt;script src=&#34;http://localhost:1313/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;Centering often reduces the correlation between the individual variables (x1, x2) and the product term (x1 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; x2). In the example below, r(x1, x1x2) = .80. With the centered variables, r(x1c, x1x2c) = -.15.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NOTE:&lt;/em&gt; For examples of when centering may &lt;strong&gt;not&lt;/strong&gt; reduce multicollinearity but may make it worse, see &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/31488914/&#34;&gt;EPM article&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
x1 &amp;lt;- rnorm(100, 10, 1)
x2 &amp;lt;- rnorm(100, 15, 1) 
x1x2 &amp;lt;- x1*x2
x1c &amp;lt;- x1 - mean(x1)
x2c &amp;lt;- x2 - mean(x2)
x1x2c &amp;lt;- x1c * x2c
dat &amp;lt;- data.frame(x1, x2, x1x2, x1c, x2c, x1x2c)
round(cor(dat), 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          x1    x2  x1x2   x1c   x2c x1x2c
## x1     1.00 -0.05  0.80  1.00 -0.05 -0.15
## x2    -0.05  1.00  0.55 -0.05  1.00 -0.17
## x1x2   0.80  0.55  1.00  0.80  0.55 -0.17
## x1c    1.00 -0.05  0.80  1.00 -0.05 -0.15
## x2c   -0.05  1.00  0.55 -0.05  1.00 -0.17
## x1x2c -0.15 -0.17 -0.17 -0.15 -0.17  1.00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A question though may be raised why centering reduces collinearity?&lt;/p&gt;
&lt;p&gt;Consider the basic equation for a correlation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[r_{(X, Y)} = \frac{cov(X,Y)}{\sqrt{(var(X) \cdot var(Y))}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For the product score (X1X2) and X1:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[r_{(X1X2, X1) = \frac{cov(X1X2, X1)}{\sqrt{(var(X1X2) \cdot var(X1))}}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Focusing only on the &lt;em&gt;numerator&lt;/em&gt; and using covariance algebra, the covariance of a product score (X1X2) with another variable (X1) can be written as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[cov(AB, C) = \mathbb{E}(A) \cdot cov(B, C) + \mathbb{E}(B) \cdot cov(A, C)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[= \mathbb{E}(X1) \cdot cov(X2, X1) + \mathbb{E}(X2) \cdot cov(X1, X1)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[= \mathbb{E}(X1) \cdot cov(X2, X1) + \mathbb{E}(X2) \cdot var(X1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With &lt;strong&gt;mean-centered&lt;/strong&gt; variables:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[r_{((X1 - \bar{X}1)(X2 - \bar{X}2), (X1 - \bar{X}1))} = \frac{cov((X1 - \bar{X}1)(X2 - \bar{X}2), (X1 - \bar{X}1))}{\sqrt{var((X1 - \bar{X}1)(X2 - \bar{X}2)) \cdot var((X1 - \bar{X}1))}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Focusing only on the &lt;em&gt;numerator&lt;/em&gt; again:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[= \mathbb{E}(X1 - \bar{X}1) \cdot cov(X2 - \bar{X}2, X1 - \bar{X}1) + \mathbb{E}(X2 - \bar{X}2) \cdot cov(X1 - \bar{X}1, X1 - \bar{X}1)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[= \mathbb{E}(X1 - \bar{X}1)  \cdot cov(X2 - \bar{X}2, X1 - \bar{X}1) + \mathbb{E}(X2 - \bar{X}2)  \cdot var(X1 - \bar{X}1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The expected value though of a mean centered variable is zero. So if the numerator is zero, the whole equation reduces to zero (on average).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[= 0  \cdot cov(X2 - \bar{X}2, X1 - \bar{X}1) + 0 \cdot var(X1 - \bar{X}1)\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;using-a-short-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using a short simulation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Randomly generate 100 x1 and x2 variables&lt;/li&gt;
&lt;li&gt;Mean center the variables&lt;/li&gt;
&lt;li&gt;Compute corresponding interactions (x1x2 and x1x2c)&lt;/li&gt;
&lt;li&gt;Get the correlations of the variables and the product term (&lt;code&gt;r&lt;/code&gt; is for the raw variables, &lt;code&gt;cr&lt;/code&gt; is for the centered variables)&lt;/li&gt;
&lt;li&gt;Get the average of the terms over the replications&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(4567)
reps &amp;lt;- 1000
r1 &amp;lt;- r2 &amp;lt;- cr1 &amp;lt;- cr2 &amp;lt;- numeric(reps)
for (i in 1:reps){
  x1 &amp;lt;- rnorm(100, 10, 1) #mean of 10, SD = 1
  x2 &amp;lt;- rnorm(100, 15, 1) #mean of 15, SD = 1 
  x1x2 &amp;lt;- x1*x2
  x1c &amp;lt;- x1 - mean(x1)
  x2c &amp;lt;- x2 - mean(x2)
  x1x2c &amp;lt;- x1c * x2c
  cr1[i] &amp;lt;- cor(x1c, x1x2c)
  cr2[i] &amp;lt;- cor(x2c, x1x2c)
  r1[i] &amp;lt;- cor(x1, x1x2)
  r2[i] &amp;lt;- cor(x2, x1x2)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;r(x1,x2) should be zero because they were generated independently.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
res &amp;lt;- data.frame(r1, r2, cr1, cr2)
round(colMeans(res), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     r1     r2    cr1    cr2 
##  0.829  0.551 -0.001  0.008&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On average, the correlations of the centered variables are 0 or near 0. They are not always zero and plotting the distribution shows the range of correlations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(tidyr)
library(ggplot2)
mm &amp;lt;- gather(res, key = &amp;#39;vars&amp;#39;, value = &amp;#39;r&amp;#39;)
mm %&amp;gt;% ggplot(aes(x = r)) +
  geom_histogram(bins = 60) + facet_grid(~vars) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;– END&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Student and teacher perceptions on student-teacher relationship quality: A middle school perspective</title>
      <link>http://localhost:1313/publication/journal-article/test2/prewett-student-2019/</link>
      <pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test2/prewett-student-2019/</guid>
      <description>&lt;p&gt;Add the &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; for the publication here using Markdown formatting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>School teasing and bullying after the presidential election</title>
      <link>http://localhost:1313/publication/journal-article/test2/huang-school-2019/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test2/huang-school-2019/</guid>
      <description>&lt;p&gt;Add the &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; for the publication here using Markdown formatting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>School threat assessment versus suicide assessment: Statewide prevalence and case characteristics</title>
      <link>http://localhost:1313/publication/journal-article/test2/burnette-school-2019/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test2/burnette-school-2019/</guid>
      <description>&lt;p&gt;Add the &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; for the publication here using Markdown formatting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Teacher job satisfaction by principal support and teacher cooperation: Results from the Schools and Staffing Survey</title>
      <link>http://localhost:1313/publication/journal-article/test2/olsen-teacher-2019/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test2/olsen-teacher-2019/</guid>
      <description>&lt;p&gt;Add the &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; for the publication here using Markdown formatting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>🧠 Difference between correlation and covariance matrices</title>
      <link>http://localhost:1313/post/2018-06-16-difference-between-variance-covariance-and-correlation-matrix/</link>
      <pubDate>Fri, 26 Oct 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2018-06-16-difference-between-variance-covariance-and-correlation-matrix/</guid>
      <description>&lt;h2 id=&#34;variancecovariance&#34;&gt;Variance/Covariance&lt;/h2&gt;
&lt;p&gt;To start off, the sample variance formula is:&lt;/p&gt;
$$s^2 = \frac{\sum_{i=1}^{n}(x_i - \overline{x})^2} {n - 1 }$$&lt;p&gt;First of all, $x - \overline{x}$ is a deviation score (deviation from
what? deviation from the mean). Summing the deviations will just get us
zero so the deviations are squared and then added together. The
numerator of this formula is then called the &lt;strong&gt;sum of squared
deviations&lt;/strong&gt; which is literally what it is. This is not yet what we
refer to as the variance (&lt;em&gt;s&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;). We have to divide this by
&lt;em&gt;n&lt;/em&gt; − 1 which is the sample degrees of freedom.&lt;/p&gt;
&lt;p&gt;If you have two variables, x and y, those two variables can covary. The
formula is similar– instead of squaring the deviation scores, the
product of the deviation scores of the two variables are used.&lt;/p&gt;
$$cov(x, y) = \frac{\sum_{i=1}^{n}(x_i - \overline{x})(y_i - \overline{y})} {n - 1 }$$&lt;p&gt;The numerator is also called the sum of &lt;strong&gt;cross products&lt;/strong&gt; (which is
what it is). Then dividing this by &lt;em&gt;n&lt;/em&gt; − 1 is the &lt;strong&gt;covariance&lt;/strong&gt;. The
covariance of a variable with itself is also the &lt;strong&gt;variance&lt;/strong&gt; which
makes sense (instead of the cross product, you are multiplying the
deviance with itself or just squaring it).&lt;/p&gt;
$$cov(x, x) = s^2 = \frac{\sum_{i=1}^{n}(x_i - \overline{x})(x_i - \overline{x})} {n - 1 }$$&lt;p&gt;That is pretty useful to know. However, the covariance though is not
easy to interpret because it is dependent on the &lt;strong&gt;scale&lt;/strong&gt; of your
variables. For example, if you get the covariance of height and weight–
one is measured in inches (or cm) and the other in pounds (or kg).
Here’s an example (not height or weight):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;x &amp;lt;- c(12, 15, 20, 25, 30)
y &amp;lt;- c(2, 6, 8, 10, 12)
mean(x)

## [1] 20.4

mean(y)

## [1] 7.6

var(x)

## [1] 53.3

var(y)

## [1] 14.8

cov(x, y) #gets one covariance at a time. 

## [1] 27.2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can put these two variables together in a data frame and estimate the
covariance from there.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df &amp;lt;- data.frame(x, y)
cov(df) #same result

##      x    y
## x 53.3 27.2
## y 27.2 14.8
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;correlation&#34;&gt;Correlation&lt;/h2&gt;
&lt;p&gt;What then is the relationship with the correlation matrix? One way to
think about it is that the covariance matrix is a bit hard to interpret
(the covariances) because they are a mix of different units of measure.
A way we get around that is standardizing the measures by converting
them to z scores:&lt;/p&gt;
$$z-scores = \frac{(x_i - \overline{x})} {SD\_x  }$$&lt;p&gt;The scores then have a distribution with a M = 0 and SD = 1 (w/c also
means a variance of 1). NOTE: how we can access variables in the data
frame using the $ sign.&lt;/p&gt;
&lt;p&gt;We can convert by using:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;zx &amp;lt;- ( (df$x) - mean(df$x) ) / sd(df$x)
zy &amp;lt;- ( (df$y) - mean(df$y) ) / sd(df$y)

zx

## [1] -1.15057698 -0.73965663 -0.05478938  0.63007787
## [5]  1.31494512

zy

## [1] -1.4556507 -0.4159002  0.1039750  0.6238503
## [5]  1.1437255
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;NOTE: in R, a function to convert raw scores to z scores is the &lt;code&gt;scale&lt;/code&gt;
function.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;zx2 &amp;lt;- scale(df$x)
zy2 &amp;lt;- scale(df$y)
zx2

##             [,1]
## [1,] -1.15057698
## [2,] -0.73965663
## [3,] -0.05478938
## [4,]  0.63007787
## [5,]  1.31494512
## attr(,&amp;quot;scaled:center&amp;quot;)
## [1] 20.4
## attr(,&amp;quot;scaled:scale&amp;quot;)
## [1] 7.300685

zy2

##            [,1]
## [1,] -1.4556507
## [2,] -0.4159002
## [3,]  0.1039750
## [4,]  0.6238503
## [5,]  1.1437255
## attr(,&amp;quot;scaled:center&amp;quot;)
## [1] 7.6
## attr(,&amp;quot;scaled:scale&amp;quot;)
## [1] 3.847077
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to scale the whole dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;zdf &amp;lt;- scale(df)
zdf

##                x          y
## [1,] -1.15057698 -1.4556507
## [2,] -0.73965663 -0.4159002
## [3,] -0.05478938  0.1039750
## [4,]  0.63007787  0.6238503
## [5,]  1.31494512  1.1437255
## attr(,&amp;quot;scaled:center&amp;quot;)
##    x    y 
## 20.4  7.6 
## attr(,&amp;quot;scaled:scale&amp;quot;)
##        x        y 
## 7.300685 3.847077
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;NOTE: These variables now have a mean of 0 and sd of 1 (also a variance
of 1).&lt;/p&gt;
&lt;p&gt;A one unit change is a one standard deviation change. NOTE: this is how
you interpret standardized beta coefficients in regression. These new
measures are now ‘unitless’.&lt;/p&gt;
&lt;p&gt;If you get the covariance of the two standardized scores, that will be
the correlation (or r),&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cov(zx, zy)

## [1] 0.9684438

### You can compare if we just get compute the correlation using the raw scores
cov(zdf)

##           x         y
## x 1.0000000 0.9684438
## y 0.9684438 1.0000000

cor(df)

##           x         y
## x 1.0000000 0.9684438
## y 0.9684438 1.0000000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result is the same. We can convert a covariance matrix into a
correlation matrix.&lt;/p&gt;
$$cor(x, y) = \frac{cov(x, y)} {sd(x) \times sd(y) }$$&lt;p&gt;You can take the variances from the covariance matrix (the diagonal) and
then take the square root and those will be the standard deviations.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#check
cov(df)

##      x    y
## x 53.3 27.2
## y 27.2 14.8

sqrt(53.3) #see diagonal

## [1] 7.300685

sd(df$x)

## [1] 7.300685

sd(df$y)

## [1] 3.847077
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So to convert the covariance of 27.2, we divide it by the product of
sd(x) and sd(y).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;27.2 / (sd(df$x) * sd(df$y))

## [1] 0.9684438
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Think about it: Can you then convert a correlation matrix to a
covariance matrix if all you had is the correlation matrix?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>🎉 Class Example- Standard Errors Too Small</title>
      <link>http://localhost:1313/post/se/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/se/</guid>
      <description>&lt;p&gt;In our module on regression diagnostics, I mentioned 1) that at times (with clustered data) standard errors may be misestimated and may be too low, resulting in a greater chance of making a Type I error (i.e., claiming statistically significant results when they should not be). In our ANCOVA session, I also indicated that 2) covariates are helpful because they help to lower the (standard) error in the model and increase power. So, it sounds like we would like to have models with lower standard errors. However, there are cases when the standard error is estimated &lt;em&gt;lower&lt;/em&gt; than it should be (i.e., the standard error is &lt;em&gt;biased&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Here’s an example– we have 30 schools with 20 student participants in each school. Fifteen schools are assigned to the treatment group (tr = 1) and 15 schools are assigned to the control (tr = 0) condition. The treatment is expected to improve reading achievement (Y). We also have a pre-test for each child (Ypre) which we can use as a covariate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;### These next few lines create the simulated dataset
set.seed(12345)
sid &amp;lt;- rep(1:30, each = 20) #10 school ids
tr &amp;lt;- rep(0:1, each = 300) #treatment status
err2 &amp;lt;- rep(rnorm(30), each = 20) #L2 error
Ypre &amp;lt;- rnorm(600)
Y &amp;lt;- .5 * tr + err2 + .7 * Ypre + rnorm(600) #generate the outcome
tr &amp;lt;- factor(tr, labels = c(&amp;#39;control&amp;#39;, &amp;#39;treatment&amp;#39;)) #label after
dat &amp;lt;- data.frame(Y, sid, Ypre, tr) #create data frame&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A way to visualize this (each point is a child’s reading score):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(forcats)
library(ggplot2) #to visualize the scores
dat$sid &amp;lt;- factor(dat$sid)
ggplot(dat, aes(x = fct_reorder2(sid, Y, as.numeric(tr)), y = Y, col = tr)) + geom_boxplot() + geom_point(position = &amp;#39;jitter&amp;#39;) + labs(y = &amp;quot;Standardized reading scores&amp;quot;, x = &amp;#39;School ID&amp;#39;) + theme_bw() + theme(legend.position=&amp;quot;bottom&amp;quot;, legend.title = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;scores.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Comparing outcomes in the treatment vs. control suggests that the treatment was effective with students on average in the treatment group scoring higher (0.59) vs students in the control schools (0.15). So the difference in scores is around 0.44 points.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aggregate(Y ~ tr, dat, FUN = mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          tr         Y
## 1   control 0.1535202
## 2 treatment 0.5886845&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod1 &amp;lt;- lm(Y ~ tr + Ypre, data = dat)
summary(mod1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Y ~ tr + Ypre, data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.7011 -0.8952  0.0623  0.8584  4.0943 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  0.06997    0.07433   0.941    0.347    
## trtreatment  0.48542    0.10487   4.629 4.52e-06 ***
## Ypre         0.78547    0.05260  14.934  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.284 on 597 degrees of freedom
## Multiple R-squared:  0.287,  Adjusted R-squared:  0.2846 
## F-statistic: 120.1 on 2 and 597 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;The OLS regression results show that the difference between treatment and control groups are statistically significant, t(597) = 4.63, p &amp;lt; .001.&lt;/strong&gt; Note though that the treatment is a group/cluster level variable. All students in the same school are in the same treatment or control group. The overall n is 600 (600 students) but then we actually only have 30 schools. Remember, larger ns result in lower standard errors. The ns also affect the critical value.&lt;/p&gt;
&lt;p&gt;One way to test this though would be to get the mean scores per school and test for differences between the treatment and control schools (since the school was the unit of analysis). However, instead of 600 observations, we end up with only 30 schools.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;agg.data &amp;lt;- aggregate(Y ~ tr + sid, FUN = mean)
#view agg.data to see
head(agg.data, n = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        tr sid         Y
## 1 control   1 1.1084184
## 2 control   2 0.6630194
## 3 control   3 0.2631278&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tail(agg.data, n = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           tr sid         Y
## 28 treatment  28 1.0279860
## 29 treatment  29 1.2251011
## 30 treatment  30 0.4352093&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod2 &amp;lt;- lm(Y ~ tr, data = agg.data)
summary(mod2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Y ~ tr, data = agg.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.9137 -0.4653  0.1002  0.5981  1.8768 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)
## (Intercept)   0.1535     0.2333   0.658    0.516
## trtreatment   0.4352     0.3300   1.319    0.198
## 
## Residual standard error: 0.9037 on 28 degrees of freedom
## Multiple R-squared:  0.05847,    Adjusted R-squared:  0.02485 
## F-statistic: 1.739 on 1 and 28 DF,  p-value: 0.198&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our results show– using aggregated data– that the treatment schools scored .44 points higher vs control schools. &lt;strong&gt;However, results are not statistically significant (using &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; = .05), t(28) = 1.32, p = .20.&lt;/strong&gt; A more common way to estimate these effects is using multilevel modeling (or hierarchical linear modeling / HLM).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(nlme) #this is for running a MLM
mod2 &amp;lt;- lme(Y ~ tr + Ypre, random = ~1|sid) #this is the multilevel model
summary(mod2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed-effects model fit by REML
##  Data: NULL 
##        AIC      BIC    logLik
##   1790.635 1812.595 -890.3177
## 
## Random effects:
##  Formula: ~1 | sid
##         (Intercept)  Residual
## StdDev:   0.8396444 0.9935838
## 
## Fixed effects: Y ~ tr + Ypre 
##                 Value Std.Error  DF   t-value p-value
## (Intercept) 0.0723148 0.2242997 569  0.322402  0.7473
## trtreatment 0.4840115 0.3171574  28  1.526093  0.1382
## Ypre        0.7634510 0.0414719 569 18.408893  0.0000
##  Correlation: 
##             (Intr) trtrtm
## trtreatment -0.707       
## Ypre        -0.020  0.008
## 
## Standardized Within-Group Residuals:
##         Min          Q1         Med          Q3         Max 
## -2.74065672 -0.67229384  0.02075635  0.65358281  3.00500913 
## 
## Number of Observations: 600
## Number of Groups: 30&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Results are similar to our &lt;em&gt;t&lt;/em&gt;-test. Compare the standard errors of the models. The naive model (ignoring the clustering) has a standard error of 0.10. The MLM has a standard error of 0.32. Results are also not statistically significant.&lt;/p&gt;
&lt;div id=&#34;yet-another-way&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Yet another way…&lt;/h2&gt;
&lt;p&gt;Another way to estimate this is to use cluster robust standard errors (CRSEs). CRSEs adjust the standard errors of the OLS regression model. The CRSEs are 0.33, similar to the MLM model. &lt;strong&gt;NOTE: CRSEs are only &lt;code&gt;good&lt;/code&gt; if the number of clusters is at least 25.&lt;/strong&gt; With fewer clusters, the standard errors will still be too small.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(jtools) #need both the cluster and robust options
summ(mod1, cluster = &amp;#39;sid&amp;#39;, robust = T, digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## MODEL INFO:
## Observations: 600
## Dependent Variable: Y
## Type: OLS linear regression 
## 
## MODEL FIT:
## F(2,597) = 120.125, p = 0.000
## R² = 0.287
## Adj. R² = 0.285 
## 
## Standard errors: Cluster-robust, type = HC3
##              Est.  S.E. t val.     p    
## (Intercept) 0.070 0.225  0.311 0.756    
## trtreatment 0.485 0.329  1.476 0.140    
## Ypre        0.785 0.059 13.395 0.000 ***&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Code below shows how to estimate the same results but use more lines
#library(sandwich)
#library(lmtest)
#vc &amp;lt;- vcovCL(mod1, cluster = dat$sid)
#coeftest(mod1, vc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use &lt;code&gt;jtools&lt;/code&gt; for this. Note: we can’t have missing data in the data.frame to make this work properly (so use &lt;code&gt;na.omit&lt;/code&gt; to remove missing data).&lt;/p&gt;
&lt;p&gt;For more, see article: Huang, F. (2016). Alternatives to multilevel modeling.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>🎉 Understanding instrumental variables</title>
      <link>http://localhost:1313/post/iv/</link>
      <pubDate>Wed, 27 Jun 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/iv/</guid>
      <description>&lt;div id=&#34;instrumental-variables-itt-and-tot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Instrumental variables, ITT, and TOT&lt;/h2&gt;
&lt;p&gt;Based on:&lt;/p&gt;
&lt;p&gt;Angrist, J. D. (2006). Instrumental variables methods in experimental criminological research: what, why and how. &lt;em&gt;Journal of Experimental Criminology&lt;/em&gt;, 2, 23-44. &lt;a href=&#34;https://doi.org/10.1007/s11292-005-5126-x&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1007/s11292-005-5126-x&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Evaluation of the Minneapolis Domestic Violence Experiment (MDVE): Evaluated the police response to domestic violence reports. Experiment conducted in 1981-82 by Lawrence Sherman and Richard Berk. Police may be reluctant to get involved for various reasons (e.g., might be viewed as a private matter, may not want to get involved).&lt;/p&gt;
&lt;p&gt;Design applied only to simple misdemenors where both suspect and victim were present when the police arrived. Only included cases where the police were empowered to make a decision (but not required to make an arrest- but must have probable cause).&lt;/p&gt;
&lt;p&gt;Each officer carried a pad of color coded report forms. Each time an officer encountered a situation that fit the experimental criteria, they were to take action as indicated by the color coded form (which were assigned by lottery assignment or random). To monitor the consistency of lottery assignment, research staff rode on patrols for a sample of evenings. (read the article below for further details):&lt;/p&gt;
&lt;p&gt;See: &lt;a href=&#34;https://en.wikipedia.org/wiki/Minneapolis_Domestic_Violence_Experiment&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Minneapolis_Domestic_Violence_Experiment&lt;/a&gt;– link to the original article is there.&lt;/p&gt;
&lt;p&gt;After qualifying a certain case, police officers were provided three randomized approaches that they could use:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Send abuser away for 8 hours&lt;/li&gt;
&lt;li&gt;Advice and mediation of disputes&lt;/li&gt;
&lt;li&gt;Make an arrest&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For purposes of the experiment, we refer to 1 and 2 as the coddling condition. The main outcome of interest was the rate of reoffending (which condition would have higher rates of reoffending: coddling or arrests?).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- rio::import(&amp;#39;http://faculty.missouri.edu/huangf/data/eval/coddled.sav&amp;#39;)
#x$t_random &amp;lt;- factor(x$t_random, labels = c(&amp;#39;pink&amp;#39;,&amp;#39;yellow&amp;#39;,&amp;#39;blue&amp;#39;))
#x$t_random &amp;lt;- factor(x$t_random, labels = c(&amp;#39;arrest&amp;#39;,&amp;#39;advise&amp;#39;,&amp;#39;separate&amp;#39;))
#head(x)
names(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;t_random&amp;quot;    &amp;quot;z_coddled&amp;quot;   &amp;quot;d_coddled&amp;quot;   &amp;quot;y82&amp;quot;         &amp;quot;q1&amp;quot;         
##  [6] &amp;quot;q2&amp;quot;          &amp;quot;q3&amp;quot;          &amp;quot;mixed&amp;quot;       &amp;quot;s_influence&amp;quot; &amp;quot;anyweapon&amp;quot;  
## [11] &amp;quot;nonwhite&amp;quot;    &amp;quot;fail_z&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inspect the variables: z_coddled represents the assigned treatment. d_coddled shows if the treatment was actually delivered.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;descr::crosstab(x$z_coddled, x$d_coddled, prop.r = T, plot = F)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Cell Contents 
## |-------------------------|
## |                   Count | 
## |             Row Percent | 
## |-------------------------|
## 
## ====================================
##                x$d_coddled
## x$z_coddled        0       1   Total
## ------------------------------------
## 0                91       1      92 
##                98.9%    1.1%   29.3%
## ------------------------------------
## 1                45     177     222 
##                20.3%   79.7%   70.7%
## ------------------------------------
## Total           136     178     314 
## ====================================&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;79.7 - 1.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 78.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on this, 80% complied with the treatment delivery of coddling when assigned to coddle. On the other hand, 1 out of 92 coddled when assigned to not coddle. More precisely, 78.6% followed treatment assignment. This can also be estimated using OLS.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;assign1 &amp;lt;- (lm(d_coddled ~  z_coddled, data = x))
assign2 &amp;lt;- (lm(d_coddled ~  z_coddled + anyweapon + s_influence + y82 + q1 + q2 + q3 + nonwhite + mixed, data = x))
summary(assign1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = d_coddled ~ z_coddled, data = x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.79730 -0.01087  0.20270  0.20270  0.98913 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  0.01087    0.03584   0.303    0.762    
## z_coddled    0.78643    0.04262  18.451   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.3438 on 312 degrees of freedom
## Multiple R-squared:  0.5218, Adjusted R-squared:  0.5203 
## F-statistic: 340.4 on 1 and 312 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(assign2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = d_coddled ~ z_coddled + anyweapon + s_influence + 
##     y82 + q1 + q2 + q3 + nonwhite + mixed, data = x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.92651 -0.03539  0.11573  0.21259  0.90118 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  0.047312   0.068947   0.686   0.4931    
## z_coddled    0.773049   0.042849  18.041   &amp;lt;2e-16 ***
## anyweapon   -0.064339   0.044525  -1.445   0.1495    
## s_influence -0.087523   0.040233  -2.175   0.0304 *  
## y82         -0.036886   0.048734  -0.757   0.4497    
## q1           0.053236   0.073185   0.727   0.4675    
## q2           0.063912   0.058087   1.100   0.2721    
## q3          -0.002788   0.063121  -0.044   0.9648    
## nonwhite     0.024477   0.039752   0.616   0.5385    
## mixed        0.042235   0.045305   0.932   0.3520    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.3422 on 304 degrees of freedom
## Multiple R-squared:  0.5382, Adjusted R-squared:  0.5245 
## F-statistic: 39.37 on 9 and 304 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#jtools::export_summs(assign1, assign2)
&lt;p&gt;mean(x$d_coddled) #mean of coddling delivered&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.566879&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#match this with p. 33&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With controls, the compliance rate was 77.3%.&lt;/p&gt;
&lt;p&gt;Let’s investigate the outcome, if the suspect was rearrested. On p. 28, this refers to recidivism or “the occurence of post-treatment domestic assault by the same suspect.”&lt;/p&gt;
&lt;p&gt;Using simple descriptives:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;descr::crosstab(x$fail_z, x$z_coddled, prop.c = T, plot = F)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Cell Contents 
## |-------------------------|
## |                   Count | 
## |          Column Percent | 
## |-------------------------|
## 
## =================================
##             x$z_coddled
## x$fail_z        0       1   Total
## ---------------------------------
## 0             83     175     258 
##             90.2%   78.8%        
## ---------------------------------
## 1              9      47      56 
##              9.8%   21.2%        
## ---------------------------------
## Total         92     222     314 
##             29.3%   70.7%        
## =================================&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using a linear model. Get the same results. The second model allows for the inclusion of covariates. (including a year dummy and quarter dummies).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;itt1 &amp;lt;- lm(fail_z ~  z_coddled, data = x)
itt2 &amp;lt;- lm(fail_z ~  z_coddled + anyweapon + s_influence + y82 + q1 + q2 + q3 + nonwhite + mixed  , data = x)
#jtools::export_summs(itt1, itt2)
stargazer::stargazer(itt1, itt2, star.cutoffs = c(.05,.01,.001), type = &amp;#39;text&amp;#39;, no.space = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ================================================================
##                                 Dependent variable:             
##                     --------------------------------------------
##                                        fail_z                   
##                             (1)                    (2)          
## ----------------------------------------------------------------
## z_coddled                  0.114*                0.108**        
##                           (0.047)                (0.041)        
## anyweapon                                        -0.004         
##                                                  (0.042)        
## s_influence                                       0.052         
##                                                  (0.038)        
## y82                                             -0.363***       
##                                                  (0.046)        
## q1                                              0.322***        
##                                                  (0.069)        
## q2                                              0.442***        
##                                                  (0.055)        
## q3                                               0.186**        
##                                                  (0.060)        
## nonwhite                                         -0.017         
##                                                  (0.038)        
## mixed                                             0.047         
##                                                  (0.043)        
## Constant                   0.098*                -0.093         
##                           (0.040)                (0.065)        
## ----------------------------------------------------------------
## Observations                314                    314          
## R2                         0.018                  0.307         
## Adjusted R2                0.015                  0.287         
## Residual Std. Error   0.380 (df = 312)      0.324 (df = 304)    
## F Statistic         5.827* (df = 1; 312) 14.970*** (df = 9; 304)
## ================================================================
## Note:                              *p&amp;lt;0.05; **p&amp;lt;0.01; ***p&amp;lt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#summary(itt1)
#summary(itt2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ITT rate is generally more conservative as this factors in noncompliance (10.8% more arrest if coddled). Often reported because this is in actuality what you get on average if a policy is implemented on a large scale basis. Not everyone complies maybe. Coddling is associated with an increase of rearrests by around 11%. This is the Intent to Treat (ITT) effect.&lt;/p&gt;
&lt;p&gt;If you just look at the actual treatment delivery (d_coddled), the effect is smaller than it should be. The effect of coddling here is ~ 8.7%.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t1 &amp;lt;- lm(fail_z ~  d_coddled , data = x)
t2 &amp;lt;- lm(fail_z ~  d_coddled + anyweapon + s_influence + y82 + q1 + q2 + q3 + nonwhite + mixed , data = x)
stargazer::stargazer(t1, t2, star.cutoffs = c(.05,.01,.001), type = &amp;#39;text&amp;#39;, no.space = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ================================================================
##                                 Dependent variable:             
##                     --------------------------------------------
##                                        fail_z                   
##                             (1)                    (2)          
## ----------------------------------------------------------------
## d_coddled                  0.107*                0.087*         
##                           (0.043)                (0.038)        
## anyweapon                                        -0.002         
##                                                  (0.042)        
## s_influence                                       0.057         
##                                                  (0.038)        
## y82                                             -0.360***       
##                                                  (0.046)        
## q1                                              0.316***        
##                                                  (0.069)        
## q2                                              0.437***        
##                                                  (0.055)        
## q3                                               0.185**        
##                                                  (0.060)        
## nonwhite                                         -0.022         
##                                                  (0.038)        
## mixed                                             0.045         
##                                                  (0.043)        
## Constant                  0.118***               -0.065         
##                           (0.033)                (0.062)        
## ----------------------------------------------------------------
## Observations                314                    314          
## R2                         0.019                  0.303         
## Adjusted R2                0.016                  0.282         
## Residual Std. Error   0.380 (df = 312)      0.325 (df = 304)    
## F Statistic         6.111* (df = 1; 312) 14.683*** (df = 9; 304)
## ================================================================
## Note:                              *p&amp;lt;0.05; **p&amp;lt;0.01; ***p&amp;lt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To ‘recover’ the actual effect, divide the ITT estimate (10.8%) with the compliance rate (77.3%).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;.108/.773&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1397154&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The effect is actually a bit higher (14.0%). However, no one really calculates it this way– just shows how this is done. The other way is to run two regressions. In the first regression (or the first stage as it is called), predict actual compliance using treatment assignment (here we use the &lt;code&gt;fitted&lt;/code&gt; function). We already did this in the &lt;code&gt;assign2&lt;/code&gt; model earlier (to get the compliance rate). Next, we estimate the outcome using the predicted values based on &lt;code&gt;assign2&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stage1 &amp;lt;- lm(fail_z ~ fitted(assign1), data = x)
stage2 &amp;lt;- lm(fail_z ~ fitted(assign2) + anyweapon + s_influence + y82 + q1 + q2 + q3 + nonwhite + mixed, data = x)
stargazer::stargazer(stage1, stage2, star.cutoffs = c(.05,.01,.001), type = &amp;#39;text&amp;#39;, no.space = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ================================================================
##                                 Dependent variable:             
##                     --------------------------------------------
##                                        fail_z                   
##                             (1)                    (2)          
## ----------------------------------------------------------------
## fitted(assign1)            0.145*                               
##                           (0.060)                               
## fitted(assign2)                                  0.140**        
##                                                  (0.052)        
## anyweapon                                         0.005         
##                                                  (0.043)        
## s_influence                                       0.064         
##                                                  (0.039)        
## y82                                             -0.358***       
##                                                  (0.046)        
## q1                                              0.314***        
##                                                  (0.069)        
## q2                                              0.433***        
##                                                  (0.055)        
## q3                                               0.186**        
##                                                  (0.060)        
## nonwhite                                         -0.021         
##                                                  (0.038)        
## mixed                                             0.041         
##                                                  (0.043)        
## Constant                   0.096*                -0.100         
##                           (0.040)                (0.067)        
## ----------------------------------------------------------------
## Observations                314                    314          
## R2                         0.018                  0.307         
## Adjusted R2                0.015                  0.287         
## Residual Std. Error   0.380 (df = 312)      0.324 (df = 304)    
## F Statistic         5.827* (df = 1; 312) 14.970*** (df = 9; 304)
## ================================================================
## Note:                              *p&amp;lt;0.05; **p&amp;lt;0.01; ***p&amp;lt;0.001&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The point estimate is correct though the standard error (.052) may be off. In general, this is computed using canned functions in statistics programs. In this case though, the standard errors were similar.&lt;/p&gt;
&lt;p&gt;The actual IV reg using two stage least squares estimation (2SLS). Can be done using the &lt;code&gt;sem&lt;/code&gt; or &lt;code&gt;AER&lt;/code&gt; package. Shown both ways below (one with no controls, other with controls).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iv1 &amp;lt;- sem::tsls(fail_z ~  d_coddled, ~ z_coddled, data = x)
iv2 &amp;lt;- sem::tsls(fail_z ~  d_coddled + anyweapon + s_influence + y82 + q1 + q2 + q3 + nonwhite + mixed, ~ anyweapon + s_influence + y82 + q1 + q2 + q3 + nonwhite + mixed + z_coddled , data = x)
summary(iv1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  2SLS Estimates
## 
## Model Formula: fail_z ~ d_coddled
## 
## Instruments: ~z_coddled
## 
## Residuals:
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -0.24107 -0.24107 -0.09625  0.00000 -0.09625  0.90375 
## 
##               Estimate Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept) 0.09625202 0.04024964 2.39138 0.017378 *
## d_coddled   0.14481385 0.06003622 2.41211 0.016437 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.3807832 on 312 degrees of freedom&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(iv2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  2SLS Estimates
## 
## Model Formula: fail_z ~ d_coddled + anyweapon + s_influence + y82 + q1 + q2 + 
##     q3 + nonwhite + mixed
## 
## Instruments: ~anyweapon + s_influence + y82 + q1 + q2 + q3 + nonwhite + mixed + 
##     z_coddled
## 
## Residuals:
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -0.55831 -0.20427 -0.04468  0.00000  0.11556  0.97566 
## 
##                 Estimate   Std. Error  t value   Pr(&amp;gt;|t|)    
## (Intercept) -0.099921186  0.066920037 -1.49314  0.1364369    
## d_coddled    0.139898052  0.052777514  2.65071  0.0084529 ** 
## anyweapon    0.004955654  0.042832553  0.11570  0.9079680    
## s_influence  0.064255255  0.038909632  1.65140  0.0996901 .  
## y82         -0.358006623  0.046428421 -7.71094 1.7941e-13 ***
## q1           0.314227128  0.069686777  4.50914 9.3049e-06 ***
## q2           0.433264526  0.055436372  7.81553 9.0150e-14 ***
## q3           0.186213372  0.060104751  3.09815  0.0021293 ** 
## nonwhite    -0.020594117  0.037793842 -0.54491  0.5862172    
## mixed        0.041405215  0.043250600  0.95733  0.3391600    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.3258485 on 304 degrees of freedom&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iv1a &amp;lt;- AER::ivreg(fail_z ~ d_coddled | z_coddled, data = x)
iv2a &amp;lt;- AER::ivreg(fail_z ~ d_coddled + anyweapon + s_influence + y82 + q1 + q2 + q3 + nonwhite + mixed | anyweapon + s_influence + y82 + q1 + q2 + q3 + nonwhite + mixed + z_coddled , data = x)
&lt;p&gt;summary(iv1a)&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## AER::ivreg(formula = fail_z ~ d_coddled | z_coddled, data = x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.24107 -0.24107 -0.09625 -0.09625  0.90375 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept)  0.09625    0.04025   2.391   0.0174 *
## d_coddled    0.14481    0.06004   2.412   0.0164 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.3808 on 312 degrees of freedom
## Multiple R-Squared: 0.01682, Adjusted R-squared: 0.01367 
## Wald test: 5.818 on 1 and 312 DF,  p-value: 0.01644&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(iv2a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## AER::ivreg(formula = fail_z ~ d_coddled + anyweapon + s_influence + 
##     y82 + q1 + q2 + q3 + nonwhite + mixed | anyweapon + s_influence + 
##     y82 + q1 + q2 + q3 + nonwhite + mixed + z_coddled, data = x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.55831 -0.20427 -0.04468  0.11556  0.97566 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -0.099921   0.066920  -1.493  0.13644    
## d_coddled    0.139898   0.052778   2.651  0.00845 ** 
## anyweapon    0.004956   0.042833   0.116  0.90797    
## s_influence  0.064255   0.038910   1.651  0.09969 .  
## y82         -0.358007   0.046428  -7.711 1.79e-13 ***
## q1           0.314227   0.069687   4.509 9.30e-06 ***
## q2           0.433265   0.055436   7.816 9.01e-14 ***
## q3           0.186213   0.060105   3.098  0.00213 ** 
## nonwhite    -0.020594   0.037794  -0.545  0.58622    
## mixed        0.041405   0.043251   0.957  0.33916    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.3258 on 304 degrees of freedom
## Multiple R-Squared: 0.2985,  Adjusted R-squared: 0.2777 
## Wald test: 14.79 on 9 and 304 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ivpack)
iv.nocov &amp;lt;- ivreg(fail_z ~ d_coddled | z_coddled, data = x)
summary(iv.nocov)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## ivreg(formula = fail_z ~ d_coddled | z_coddled, data = x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.24107 -0.24107 -0.09625 -0.09625  0.90375 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept)  0.09625    0.04025   2.391   0.0174 *
## d_coddled    0.14481    0.06004   2.412   0.0164 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.3808 on 312 degrees of freedom
## Multiple R-Squared: 0.01682, Adjusted R-squared: 0.01367 
## Wald test: 5.818 on 1 and 312 DF,  p-value: 0.01644&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;robust.se(iv.nocov)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Robust Standard Errors&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## t test of coefficients:
## 
##             Estimate Std. Error t value Pr(&amp;gt;|t|)   
## (Intercept) 0.096252   0.031498  3.0558 0.002438 **
## d_coddled   0.144814   0.052694  2.7482 0.006341 **
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iv.wcov &amp;lt;- ivreg(fail_z ~ d_coddled + anyweapon + s_influence + y82 + q1 + q2 + q3 + nonwhite + mixed | anyweapon + s_influence + y82 + q1 + q2 + q3 + nonwhite + mixed + z_coddled , data = x)
robust.se(iv.wcov)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Robust Standard Errors&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## t test of coefficients:
## 
##               Estimate Std. Error t value  Pr(&amp;gt;|t|)    
## (Intercept) -0.0999212  0.0461484 -2.1652 0.0311488 *  
## d_coddled    0.1398981  0.0481670  2.9044 0.0039489 ** 
## anyweapon    0.0049557  0.0420239  0.1179 0.9062053    
## s_influence  0.0642553  0.0369475  1.7391 0.0830302 .  
## y82         -0.3580066  0.0421370 -8.4962 8.879e-16 ***
## q1           0.3142271  0.0537986  5.8408 1.335e-08 ***
## q2           0.4332645  0.0504189  8.5933 4.510e-16 ***
## q3           0.1862134  0.0475390  3.9171 0.0001107 ***
## nonwhite    -0.0205941  0.0367519 -0.5604 0.5756499    
## mixed        0.0414052  0.0415748  0.9959 0.3200808    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;BEWARE&lt;/em&gt;: Even though the model may look like a strucural equation model (SEM), &lt;em&gt;do not&lt;/em&gt; run it like one. Answers will not be the same.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lavaan)
sem1 &amp;lt;- &amp;#39;
fail_z ~ a*d_coddled
d_coddled ~ b*z_coddled
ab := a*b #indirect effect
&amp;#39;
&lt;p&gt;sem1 &amp;lt;- sem(data = x, model = sem1)
summary(sem1)&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## lavaan (0.6-1.1186) converged normally after  21 iterations
## 
##   Number of observations                           314
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                       0.835
##   Degrees of freedom                                 1
##   P-value (Chi-square)                           0.361
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   fail_z ~                                            
##     d_coddled  (a)    0.107    0.043    2.480    0.013
##   d_coddled ~                                         
##     z_coddled  (b)    0.786    0.042   18.510    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .fail_z            0.144    0.011   12.530    0.000
##    .d_coddled         0.117    0.009   12.530    0.000
## 
## Defined Parameters:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     ab                0.084    0.034    2.458    0.014&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Student Threat Assessment as a Standard School Safety Practice: Results From a Statewide Implementation Study.</title>
      <link>http://localhost:1313/publication/journal-article/test2/cornell-student-2017/</link>
      <pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test2/cornell-student-2017/</guid>
      <description>&lt;p&gt;Add the &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; for the publication here using Markdown formatting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Student attitudes and behaviors as explanations for the Black-White suspension gap</title>
      <link>http://localhost:1313/publication/journal-article/test2/huang-student-2017/</link>
      <pubDate>Wed, 01 Feb 2017 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test2/huang-student-2017/</guid>
      <description>&lt;p&gt;Add the &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; for the publication here using Markdown formatting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Racial/ethnic differences in perceptions of school climate and its association with student engagement and peer aggression</title>
      <link>http://localhost:1313/publication/journal-article/konold-racialethnic-2017/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/konold-racialethnic-2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Question order affects the measurement of bullying victimization among middle school students</title>
      <link>http://localhost:1313/publication/journal-article/huang-question-2016/</link>
      <pubDate>Sat, 01 Oct 2016 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/huang-question-2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>School counselor use of peer nominations to identify victims of bullying</title>
      <link>http://localhost:1313/publication/journal-article/test2/cornell-school-2015/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test2/cornell-school-2015/</guid>
      <description>&lt;p&gt;Add the &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; for the publication here using Markdown formatting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Successive teacher expectation effects across the early school years</title>
      <link>http://localhost:1313/publication/journal-article/test2/rubie-davies-successive-2014/</link>
      <pubDate>Thu, 01 May 2014 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test2/rubie-davies-successive-2014/</guid>
      <description>&lt;p&gt;Add the &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; for the publication here using Markdown formatting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The differential effects of preschool: Evidence from Virginia</title>
      <link>http://localhost:1313/publication/journal-article/test2/huang-differential-2012/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publication/journal-article/test2/huang-differential-2012/</guid>
      <description>&lt;p&gt;Add the &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; for the publication here using Markdown formatting.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
